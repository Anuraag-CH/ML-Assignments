{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import shutil\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "matplotlib.use(u'nbAgg')\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pickle\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn import preprocessing\n",
    "import pandas as pd\n",
    "from multiprocessing import Process# this is used for multithreading\n",
    "import multiprocessing\n",
    "import codecs# this is used for file operations \n",
    "import random as r\n",
    "import array\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from tqdm import tqdm\n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import imageio\n",
    "asm_df=pd.read_csv('asmoutputfile.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10868, 52)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asm_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>HEADER:</th>\n",
       "      <th>.text:</th>\n",
       "      <th>.Pav:</th>\n",
       "      <th>.idata:</th>\n",
       "      <th>.data:</th>\n",
       "      <th>.bss:</th>\n",
       "      <th>.rdata:</th>\n",
       "      <th>.edata:</th>\n",
       "      <th>.rsrc:</th>\n",
       "      <th>...</th>\n",
       "      <th>:dword</th>\n",
       "      <th>edx</th>\n",
       "      <th>esi</th>\n",
       "      <th>eax</th>\n",
       "      <th>ebx</th>\n",
       "      <th>ecx</th>\n",
       "      <th>edi</th>\n",
       "      <th>ebp</th>\n",
       "      <th>esp</th>\n",
       "      <th>eip</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01kcPWA9K2BOxQeS5Rju</td>\n",
       "      <td>19</td>\n",
       "      <td>744</td>\n",
       "      <td>0</td>\n",
       "      <td>127</td>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>323</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>137</td>\n",
       "      <td>18</td>\n",
       "      <td>66</td>\n",
       "      <td>15</td>\n",
       "      <td>43</td>\n",
       "      <td>83</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>48</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1E93CpP60RHFNiT5Qfvn</td>\n",
       "      <td>17</td>\n",
       "      <td>838</td>\n",
       "      <td>0</td>\n",
       "      <td>103</td>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>130</td>\n",
       "      <td>18</td>\n",
       "      <td>29</td>\n",
       "      <td>48</td>\n",
       "      <td>82</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3ekVow2ajZHbTnBcsDfX</td>\n",
       "      <td>17</td>\n",
       "      <td>427</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "      <td>145</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>84</td>\n",
       "      <td>13</td>\n",
       "      <td>42</td>\n",
       "      <td>10</td>\n",
       "      <td>67</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3X2nY7iQaPBIWDrAZqJe</td>\n",
       "      <td>17</td>\n",
       "      <td>227</td>\n",
       "      <td>0</td>\n",
       "      <td>43</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>25</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>46OZzdsSKDCFV8h7XWxf</td>\n",
       "      <td>17</td>\n",
       "      <td>402</td>\n",
       "      <td>0</td>\n",
       "      <td>59</td>\n",
       "      <td>170</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>18</td>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "      <td>18</td>\n",
       "      <td>29</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 52 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     ID  HEADER:  .text:  .Pav:  .idata:  .data:  .bss:  \\\n",
       "0  01kcPWA9K2BOxQeS5Rju       19     744      0      127      57      0   \n",
       "1  1E93CpP60RHFNiT5Qfvn       17     838      0      103      49      0   \n",
       "2  3ekVow2ajZHbTnBcsDfX       17     427      0       50      43      0   \n",
       "3  3X2nY7iQaPBIWDrAZqJe       17     227      0       43      19      0   \n",
       "4  46OZzdsSKDCFV8h7XWxf       17     402      0       59     170      0   \n",
       "\n",
       "   .rdata:  .edata:  .rsrc:  ...  :dword  edx  esi  eax  ebx  ecx  edi  ebp  \\\n",
       "0      323        0       3  ...     137   18   66   15   43   83    0   17   \n",
       "1        0        0       3  ...     130   18   29   48   82   12    0   14   \n",
       "2      145        0       3  ...      84   13   42   10   67   14    0   11   \n",
       "3        0        0       3  ...      25    6    8   14    7    2    0    8   \n",
       "4        0        0       3  ...      18   12    9   18   29    5    0   11   \n",
       "\n",
       "   esp  eip  \n",
       "0   48   29  \n",
       "1    0   20  \n",
       "2    0    9  \n",
       "3    0    6  \n",
       "4    0   11  \n",
       "\n",
       "[5 rows x 52 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asm_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10868/10868 [3:52:19<00:00,  1.28s/it]  \n"
     ]
    }
   ],
   "source": [
    "#https://towardsdatascience.com/malware-classification-using-machine-learning-7c648fb1da79 and dchad\n",
    "files=os.listdir(\"asmFiles\")\n",
    "for file in tqdm(files):\n",
    "    filename = file.split('.')[0]\n",
    "    f = codecs.open(\"asmFiles/\" +file, 'rb')\n",
    "    length= os.path.getsize(\"asmFiles/\" +file)\n",
    "    width = int(length ** 0.5)\n",
    "    rem = int(length/ width)\n",
    "    arr = array.array('B')\n",
    "    arr.frombytes(f.read())\n",
    "    f.close()\n",
    "    reshaped = np.reshape(arr[:width * width], (width, width))\n",
    "    reshaped = np.uint8(reshaped)\n",
    "    imageio.imsave('asm_image/' + filename + '.png',reshaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/sai977/microsoft-malware-detection/blob/master/MicrosoftMalwareDetection.ipynb\n",
    "import cv2\n",
    "imagefeatures = np.zeros((10868, 200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, asmfile in enumerate (os.listdir(\"asmFiles\")):\n",
    "    img = cv2.imread(\"asm_image/\" + asmfile.split('.')[0] + '.png')\n",
    "    img_arr = img.flatten()[:200]\n",
    "    imagefeatures[i, :] += img_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(imagefeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_features_name = []\n",
    "for i in range(200):\n",
    "    img_features_name.append('pix' + str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgdf = pd.DataFrame((imagefeatures), columns = img_features_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pix0</th>\n",
       "      <th>pix1</th>\n",
       "      <th>pix2</th>\n",
       "      <th>pix3</th>\n",
       "      <th>pix4</th>\n",
       "      <th>pix5</th>\n",
       "      <th>pix6</th>\n",
       "      <th>pix7</th>\n",
       "      <th>pix8</th>\n",
       "      <th>pix9</th>\n",
       "      <th>...</th>\n",
       "      <th>pix190</th>\n",
       "      <th>pix191</th>\n",
       "      <th>pix192</th>\n",
       "      <th>pix193</th>\n",
       "      <th>pix194</th>\n",
       "      <th>pix195</th>\n",
       "      <th>pix196</th>\n",
       "      <th>pix197</th>\n",
       "      <th>pix198</th>\n",
       "      <th>pix199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>72.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>...</td>\n",
       "      <td>45.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>45.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>46.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>...</td>\n",
       "      <td>45.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>45.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>72.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>...</td>\n",
       "      <td>45.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>45.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>72.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>...</td>\n",
       "      <td>45.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>45.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>72.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>...</td>\n",
       "      <td>45.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>45.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   pix0  pix1  pix2   pix3   pix4   pix5   pix6   pix7   pix8   pix9  ...  \\\n",
       "0  72.0  72.0  72.0   69.0   69.0   69.0   65.0   65.0   65.0   68.0  ...   \n",
       "1  46.0  46.0  46.0  116.0  116.0  116.0  101.0  101.0  101.0  120.0  ...   \n",
       "2  72.0  72.0  72.0   69.0   69.0   69.0   65.0   65.0   65.0   68.0  ...   \n",
       "3  72.0  72.0  72.0   69.0   69.0   69.0   65.0   65.0   65.0   68.0  ...   \n",
       "4  72.0  72.0  72.0   69.0   69.0   69.0   65.0   65.0   65.0   68.0  ...   \n",
       "\n",
       "   pix190  pix191  pix192  pix193  pix194  pix195  pix196  pix197  pix198  \\\n",
       "0    45.0    45.0    45.0    45.0    45.0    45.0    45.0    45.0    45.0   \n",
       "1    45.0    45.0    45.0    45.0    45.0    45.0    45.0    45.0    45.0   \n",
       "2    45.0    45.0    45.0    45.0    45.0    45.0    45.0    45.0    45.0   \n",
       "3    45.0    45.0    45.0    45.0    45.0    45.0    45.0    45.0    45.0   \n",
       "4    45.0    45.0    45.0    45.0    45.0    45.0    45.0    45.0    45.0   \n",
       "\n",
       "   pix199  \n",
       "0    45.0  \n",
       "1    45.0  \n",
       "2    45.0  \n",
       "3    45.0  \n",
       "4    45.0  \n",
       "\n",
       "[5 rows x 200 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgdf=pd.read_csv('image_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=pd.read_csv('trainLabels.csv')\n",
    "y=y['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_asm, X_test_asm, y_train_asm, y_test_asm = train_test_split(imgdf,y ,stratify=y,test_size=0.20)\n",
    "X_train_asm, X_cv_asm, y_train_asm, y_cv_asm = train_test_split(X_train_asm, y_train_asm,stratify=y_train_asm,test_size=0.20)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.0577401 ,  0.48869151,  0.48869151, ...,  0.01199175,\n",
       "         0.01199175,  0.01199175],\n",
       "       [ 0.77509524,  0.48869151,  0.48869151, ...,  0.01199175,\n",
       "         0.01199175,  0.01199175],\n",
       "       [ 0.7228014 ,  0.48869151,  0.48869151, ...,  0.01199175,\n",
       "         0.01199175,  0.01199175],\n",
       "       ...,\n",
       "       [-1.62181207, -1.78472789, -1.78472789, ...,  0.01199175,\n",
       "         0.01199175,  0.01199175],\n",
       "       [ 0.93580314, -1.78472789, -1.78472789, ...,  0.01199175,\n",
       "         0.01199175,  0.01199175],\n",
       "       [ 1.47500365,  0.48869151,  0.48869151, ...,  0.01199175,\n",
       "         0.01199175,  0.01199175]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train_asm)\n",
    "scaler.transform(X_train_asm)\n",
    "scaler.transform(X_cv_asm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_loss for k =  1 is 1.9007431478587495\n",
      "log_loss for k =  3 is 1.90021685474741\n",
      "log_loss for k =  5 is 1.8998562163237904\n",
      "log_loss for k =  7 is 1.9006330524381227\n",
      "log_loss for k =  9 is 1.9005240107141173\n",
      "log_loss for k =  11 is 1.8997854850504061\n",
      "log_loss for k =  13 is 1.899940808887257\n",
      "log_loss for k =  15 is 1.900386432750284\n",
      "log_loss for k =  17 is 1.900817750512503\n",
      "log_loss for k =  19 is 1.8995922501820257\n",
      "log loss for train data 1.8986815155936156\n",
      "log loss for cv data 1.8995922501820257\n",
      "log loss for test data 1.9004669967069017\n"
     ]
    }
   ],
   "source": [
    "# find more about KNeighborsClassifier() here http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\n",
    "# -------------------------\n",
    "\n",
    "alpha = [x for x in range(1, 21,2)]\n",
    "cv_log_error_array=[]\n",
    "for i in alpha:\n",
    "    k_cfl=KNeighborsClassifier(n_neighbors=i)\n",
    "    k_cfl.fit(X_train_asm,y_train_asm)\n",
    "    sig_clf = CalibratedClassifierCV(k_cfl, method=\"sigmoid\")\n",
    "    sig_clf.fit(X_train_asm, y_train_asm)\n",
    "    predict_y = sig_clf.predict_proba(X_cv_asm)\n",
    "    cv_log_error_array.append(log_loss(y_cv_asm, predict_y, labels=k_cfl.classes_, eps=1e-15))\n",
    "    \n",
    "for i in range(len(cv_log_error_array)):\n",
    "    print ('log_loss for k = ',alpha[i],'is',cv_log_error_array[i])\n",
    "\n",
    "best_alpha = np.argmin(cv_log_error_array)\n",
    "    \n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(alpha, cv_log_error_array,c='g')\n",
    "for i, txt in enumerate(np.round(cv_log_error_array,3)):\n",
    "    ax.annotate((alpha[i],np.round(txt,3)), (alpha[i],cv_log_error_array[i]))\n",
    "plt.grid()\n",
    "plt.title(\"Cross Validation Error for each alpha\")\n",
    "plt.xlabel(\"Alpha i's\")\n",
    "plt.ylabel(\"Error measure\")\n",
    "plt.show()\n",
    "\n",
    "k_cfl=KNeighborsClassifier(n_neighbors=alpha[best_alpha])\n",
    "k_cfl.fit(X_train_asm,y_train_asm)\n",
    "sig_clf = CalibratedClassifierCV(k_cfl, method=\"sigmoid\")\n",
    "sig_clf.fit(X_train_asm, y_train_asm)\n",
    "pred_y=sig_clf.predict(X_test_asm)\n",
    "\n",
    "\n",
    "predict_y = sig_clf.predict_proba(X_train_asm)\n",
    "print ('log loss for train data',log_loss(y_train_asm, predict_y))\n",
    "predict_y = sig_clf.predict_proba(X_cv_asm)\n",
    "print ('log loss for cv data',log_loss(y_cv_asm, predict_y))\n",
    "predict_y = sig_clf.predict_proba(X_test_asm)\n",
    "print ('log loss for test data',log_loss(y_test_asm, predict_y))\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_loss for c =  1e-05 is 1.9009276538582975\n",
      "log_loss for c =  0.0001 is 1.9007559853431149\n",
      "log_loss for c =  0.001 is 1.900194523795631\n",
      "log_loss for c =  0.01 is 1.9002274021385404\n",
      "log_loss for c =  0.1 is 1.900333895661907\n",
      "log_loss for c =  1 is 1.900272427923307\n",
      "log_loss for c =  10 is 1.9002277263669891\n",
      "log_loss for c =  100 is 1.9002473550720096\n",
      "log_loss for c =  1000 is 1.9002115015225962\n",
      "log loss for train data 1.8994641717475333\n",
      "log loss for cv data 1.900194523795631\n",
      "log loss for test data 1.8999456559801227\n"
     ]
    }
   ],
   "source": [
    "# read more about SGDClassifier() at http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html\n",
    "# ------------------------------\n",
    "# default parameters\n",
    "# SGDClassifier(loss=’hinge’, penalty=’l2’, alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=None, tol=None, \n",
    "# shuffle=True, verbose=0, epsilon=0.1, n_jobs=1, random_state=None, learning_rate=’optimal’, eta0=0.0, power_t=0.5, \n",
    "# class_weight=None, warm_start=False, average=False, n_iter=None)\n",
    "\n",
    "# some of methods\n",
    "# fit(X, y[, coef_init, intercept_init, …])\tFit linear model with Stochastic Gradient Descent.\n",
    "# predict(X)\tPredict class labels for samples in X.\n",
    "\n",
    "#-------------------------------\n",
    "# video link: https://www.appliedaicourse.com/course/applied-ai-course-online/lessons/geometric-intuition-1/\n",
    "#------------------------------\n",
    "\n",
    "\n",
    "alpha = [10 ** x for x in range(-5, 4)]\n",
    "cv_log_error_array=[]\n",
    "for i in alpha:\n",
    "    logisticR=LogisticRegression(penalty='l2',C=i,class_weight='balanced')\n",
    "    logisticR.fit(X_train_asm,y_train_asm)\n",
    "    sig_clf = CalibratedClassifierCV(logisticR, method=\"sigmoid\")\n",
    "    sig_clf.fit(X_train_asm, y_train_asm)\n",
    "    predict_y = sig_clf.predict_proba(X_cv_asm)\n",
    "    cv_log_error_array.append(log_loss(y_cv_asm, predict_y, labels=logisticR.classes_, eps=1e-15))\n",
    "    \n",
    "for i in range(len(cv_log_error_array)):\n",
    "    print ('log_loss for c = ',alpha[i],'is',cv_log_error_array[i])\n",
    "\n",
    "best_alpha = np.argmin(cv_log_error_array)\n",
    "    \n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(alpha, cv_log_error_array,c='g')\n",
    "for i, txt in enumerate(np.round(cv_log_error_array,3)):\n",
    "    ax.annotate((alpha[i],np.round(txt,3)), (alpha[i],cv_log_error_array[i]))\n",
    "plt.grid()\n",
    "plt.title(\"Cross Validation Error for each alpha\")\n",
    "plt.xlabel(\"Alpha i's\")\n",
    "plt.ylabel(\"Error measure\")\n",
    "plt.show()\n",
    "\n",
    "logisticR=LogisticRegression(penalty='l2',C=alpha[best_alpha],class_weight='balanced')\n",
    "logisticR.fit(X_train_asm,y_train_asm)\n",
    "sig_clf = CalibratedClassifierCV(logisticR, method=\"sigmoid\")\n",
    "sig_clf.fit(X_train_asm, y_train_asm)\n",
    "\n",
    "predict_y = sig_clf.predict_proba(X_train_asm)\n",
    "print ('log loss for train data',(log_loss(y_train_asm, predict_y, labels=logisticR.classes_, eps=1e-15)))\n",
    "predict_y = sig_clf.predict_proba(X_cv_asm)\n",
    "print ('log loss for cv data',(log_loss(y_cv_asm, predict_y, labels=logisticR.classes_, eps=1e-15)))\n",
    "predict_y = sig_clf.predict_proba(X_test_asm)\n",
    "print ('log loss for test data',(log_loss(y_test_asm, predict_y, labels=logisticR.classes_, eps=1e-15)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_loss for c =  10 is 1.901305907386374\n",
      "log_loss for c =  50 is 1.9015514185982654\n",
      "log_loss for c =  100 is 1.9017623008144102\n",
      "log_loss for c =  500 is 1.9017019201698366\n",
      "log_loss for c =  1000 is 1.9018027584219617\n",
      "log_loss for c =  2000 is 1.901847232429469\n",
      "log_loss for c =  3000 is 1.901851304214067\n",
      "log loss for train data 1.906384707650573\n",
      "log loss for cv data 1.901305907386374\n",
      "log loss for test data 1.9011828047817003\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------\n",
    "# default parameters \n",
    "# sklearn.ensemble.RandomForestClassifier(n_estimators=10, criterion=’gini’, max_depth=None, min_samples_split=2, \n",
    "# min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=’auto’, max_leaf_nodes=None, min_impurity_decrease=0.0, \n",
    "# min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=1, random_state=None, verbose=0, warm_start=False, \n",
    "# class_weight=None)\n",
    "\n",
    "# Some of methods of RandomForestClassifier()\n",
    "# fit(X, y, [sample_weight])\tFit the SVM model according to the given training data.\n",
    "# predict(X)\tPerform classification on samples in X.\n",
    "# predict_proba (X)\tPerform classification on samples in X.\n",
    "\n",
    "# some of attributes of  RandomForestClassifier()\n",
    "# feature_importances_ : array of shape = [n_features]\n",
    "# The feature importances (the higher, the more important the feature).\n",
    "\n",
    "# --------------------------------\n",
    "# video link: https://www.appliedaicourse.com/course/applied-ai-course-online/lessons/random-forest-and-their-construction-2/\n",
    "# --------------------------------\n",
    "\n",
    "alpha=[10,50,100,500,1000,2000,3000]\n",
    "cv_log_error_array=[]\n",
    "for i in alpha:\n",
    "    r_cfl=RandomForestClassifier(n_estimators=i,random_state=42,n_jobs=-1)\n",
    "    r_cfl.fit(X_train_asm,y_train_asm)\n",
    "    sig_clf = CalibratedClassifierCV(r_cfl, method=\"sigmoid\")\n",
    "    sig_clf.fit(X_train_asm, y_train_asm)\n",
    "    predict_y = sig_clf.predict_proba(X_cv_asm)\n",
    "    cv_log_error_array.append(log_loss(y_cv_asm, predict_y, labels=r_cfl.classes_, eps=1e-15))\n",
    "\n",
    "for i in range(len(cv_log_error_array)):\n",
    "    print ('log_loss for c = ',alpha[i],'is',cv_log_error_array[i])\n",
    "\n",
    "\n",
    "best_alpha = np.argmin(cv_log_error_array)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(alpha, cv_log_error_array,c='g')\n",
    "for i, txt in enumerate(np.round(cv_log_error_array,3)):\n",
    "    ax.annotate((alpha[i],np.round(txt,3)), (alpha[i],cv_log_error_array[i]))\n",
    "plt.grid()\n",
    "plt.title(\"Cross Validation Error for each alpha\")\n",
    "plt.xlabel(\"Alpha i's\")\n",
    "plt.ylabel(\"Error measure\")\n",
    "plt.show()\n",
    "\n",
    "r_cfl=RandomForestClassifier(n_estimators=alpha[best_alpha],random_state=42,n_jobs=-1)\n",
    "r_cfl.fit(X_train_asm,y_train_asm)\n",
    "sig_clf = CalibratedClassifierCV(r_cfl, method=\"sigmoid\")\n",
    "sig_clf.fit(X_train_asm, y_train_asm)\n",
    "predict_y = sig_clf.predict_proba(X_train_asm)\n",
    "print ('log loss for train data',(log_loss(y_train_asm, predict_y, labels=sig_clf.classes_, eps=1e-15)))\n",
    "predict_y = sig_clf.predict_proba(X_cv_asm)\n",
    "print ('log loss for cv data',(log_loss(y_cv_asm, predict_y, labels=sig_clf.classes_, eps=1e-15)))\n",
    "predict_y = sig_clf.predict_proba(X_test_asm)\n",
    "print ('log loss for test data',(log_loss(y_test_asm, predict_y, labels=sig_clf.classes_, eps=1e-15)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_loss for c =  10 is 0.00892940072388406\n",
      "log_loss for c =  50 is 0.008928101391465378\n",
      "log_loss for c =  100 is 0.008927848785653378\n",
      "log_loss for c =  500 is 0.0089289430933055\n",
      "log_loss for c =  1000 is 0.008928778863411096\n",
      "log_loss for c =  2000 is 0.008928904895220172\n",
      "log_loss for c =  3000 is 0.008928290533490849\n",
      "For values of best alpha =  100 The train log loss is: 0.008358048109044992\n",
      "For values of best alpha =  100 The cross validation log loss is: 0.008927848785653378\n",
      "For values of best alpha =  100 The test log loss is: 0.00821029874953462\n"
     ]
    }
   ],
   "source": [
    "# Training a hyper-parameter tuned Xg-Boost regressor on our train data\n",
    "\n",
    "# find more about XGBClassifier function here http://xgboost.readthedocs.io/en/latest/python/python_api.html?#xgboost.XGBClassifier\n",
    "# -------------------------\n",
    "# default paramters\n",
    "# class xgboost.XGBClassifier(max_depth=3, learning_rate=0.1, n_estimators=100, silent=True, \n",
    "# objective='binary:logistic', booster='gbtree', n_jobs=1, nthread=None, gamma=0, min_child_weight=1, \n",
    "# max_delta_step=0, subsample=1, colsample_bytree=1, colsample_bylevel=1, reg_alpha=0, reg_lambda=1, \n",
    "# scale_pos_weight=1, base_score=0.5, random_state=0, seed=None, missing=None, **kwargs)\n",
    "\n",
    "# some of methods of RandomForestRegressor()\n",
    "# fit(X, y, sample_weight=None, eval_set=None, eval_metric=None, early_stopping_rounds=None, verbose=True, xgb_model=None)\n",
    "# get_params([deep])\tGet parameters for this estimator.\n",
    "# predict(data, output_margin=False, ntree_limit=0) : Predict with data. NOTE: This function is not thread safe.\n",
    "# get_score(importance_type='weight') -> get the feature importance\n",
    "# -----------------------\n",
    "# video link2: https://www.appliedaicourse.com/course/applied-ai-course-online/lessons/what-are-ensembles/\n",
    "# -----------------------\n",
    "\n",
    "alpha=[10,50,100,500,1000,2000,3000]\n",
    "cv_log_error_array=[]\n",
    "for i in alpha:\n",
    "    x_cfl=XGBClassifier(n_estimators=i,nthread=-1)\n",
    "    x_cfl.fit(X_train_asm,y_train_asm)\n",
    "    sig_clf = CalibratedClassifierCV(x_cfl, method=\"sigmoid\")\n",
    "    sig_clf.fit(X_train_asm, y_train_asm)\n",
    "    predict_y = sig_clf.predict_proba(X_cv_asm)\n",
    "    cv_log_error_array.append(log_loss(y_cv_asm, predict_y, labels=x_cfl.classes_, eps=1e-15))\n",
    "\n",
    "for i in range(len(cv_log_error_array)):\n",
    "    print ('log_loss for c = ',alpha[i],'is',cv_log_error_array[i])\n",
    "\n",
    "\n",
    "best_alpha = np.argmin(cv_log_error_array)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(alpha, cv_log_error_array,c='g')\n",
    "for i, txt in enumerate(np.round(cv_log_error_array,3)):\n",
    "    ax.annotate((alpha[i],np.round(txt,3)), (alpha[i],cv_log_error_array[i]))\n",
    "plt.grid()\n",
    "plt.title(\"Cross Validation Error for each alpha\")\n",
    "plt.xlabel(\"Alpha i's\")\n",
    "plt.ylabel(\"Error measure\")\n",
    "plt.show()\n",
    "\n",
    "x_cfl=XGBClassifier(n_estimators=alpha[best_alpha],nthread=-1)\n",
    "x_cfl.fit(X_train_asm,y_train_asm)\n",
    "sig_clf = CalibratedClassifierCV(x_cfl, method=\"sigmoid\")\n",
    "sig_clf.fit(X_train_asm, y_train_asm)\n",
    "    \n",
    "predict_y = sig_clf.predict_proba(X_train_asm)\n",
    "\n",
    "print ('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train_asm, predict_y))\n",
    "predict_y = sig_clf.predict_proba(X_cv_asm)\n",
    "print('For values of best alpha = ', alpha[best_alpha], \"The cross validation log loss is:\",log_loss(y_cv_asm, predict_y))\n",
    "predict_y = sig_clf.predict_proba(X_test_asm)\n",
    "print('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test_asm, predict_y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:   46.4s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:  4.5min\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  5.7min\n",
      "[Parallel(n_jobs=-1)]: Done  41 out of  50 | elapsed:  6.6min remaining:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done  47 out of  50 | elapsed: 10.7min remaining:   40.8s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed: 10.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'colsample_bytree': 0.5, 'max_depth': 3, 'subsample': 0.3, 'learning_rate': 0.15, 'n_estimators': 1000}\n"
     ]
    }
   ],
   "source": [
    "x_cfl=XGBClassifier()\n",
    "\n",
    "prams={\n",
    "    'learning_rate':[0.01,0.03,0.05,0.1,0.15,0.2],\n",
    "     'n_estimators':[100,200,500,1000,2000],\n",
    "     'max_depth':[3,5,10],\n",
    "    'colsample_bytree':[0.1,0.3,0.5,1],\n",
    "    'subsample':[0.1,0.3,0.5,1]\n",
    "}\n",
    "random_cfl=RandomizedSearchCV(x_cfl,param_distributions=prams,verbose=10,n_jobs=-1,)\n",
    "random_cfl.fit(X_train_asm,y_train_asm)\n",
    "print (random_cfl.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.008262141416707685\n",
      "cv loss 0.011123518620043514\n",
      "test loss 0.008197434380798377\n"
     ]
    }
   ],
   "source": [
    "# Training a hyper-parameter tuned Xg-Boost regressor on our train data\n",
    "\n",
    "# find more about XGBClassifier function here http://xgboost.readthedocs.io/en/latest/python/python_api.html?#xgboost.XGBClassifier\n",
    "# -------------------------\n",
    "# default paramters\n",
    "# class xgboost.XGBClassifier(max_depth=3, learning_rate=0.1, n_estimators=100, silent=True, \n",
    "# objective='binary:logistic', booster='gbtree', n_jobs=1, nthread=None, gamma=0, min_child_weight=1, \n",
    "# max_delta_step=0, subsample=1, colsample_bytree=1, colsample_bylevel=1, reg_alpha=0, reg_lambda=1, \n",
    "# scale_pos_weight=1, base_score=0.5, random_state=0, seed=None, missing=None, **kwargs)\n",
    "\n",
    "# some of methods of RandomForestRegressor()\n",
    "# fit(X, y, sample_weight=None, eval_set=None, eval_metric=None, early_stopping_rounds=None, verbose=True, xgb_model=None)\n",
    "# get_params([deep])\tGet parameters for this estimator.\n",
    "# predict(data, output_margin=False, ntree_limit=0) : Predict with data. NOTE: This function is not thread safe.\n",
    "# get_score(importance_type='weight') -> get the feature importance\n",
    "# -----------------------\n",
    "# video link2: https://www.appliedaicourse.com/course/applied-ai-course-online/lessons/what-are-ensembles/\n",
    "# -----------------------\n",
    "\n",
    "x_cfl=XGBClassifier(n_estimators=200,subsample=0.5,learning_rate=0.15,colsample_bytree=0.5,max_depth=3)\n",
    "x_cfl.fit(X_train_asm,y_train_asm)\n",
    "c_cfl=CalibratedClassifierCV(x_cfl,method='sigmoid')\n",
    "c_cfl.fit(X_train_asm,y_train_asm)\n",
    "\n",
    "predict_y = c_cfl.predict_proba(X_train_asm)\n",
    "print ('train loss',log_loss(y_train_asm, predict_y))\n",
    "predict_y = c_cfl.predict_proba(X_cv_asm)\n",
    "print ('cv loss',log_loss(y_cv_asm, predict_y))\n",
    "predict_y = c_cfl.predict_proba(X_test_asm)\n",
    "print ('test loss',log_loss(y_test_asm, predict_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.008256678409115094\n",
      "cv loss 0.011090946258662731\n",
      "test loss 0.008196739825909375\n"
     ]
    }
   ],
   "source": [
    "# Training a hyper-parameter tuned Xg-Boost regressor on our train data\n",
    "\n",
    "# find more about XGBClassifier function here http://xgboost.readthedocs.io/en/latest/python/python_api.html?#xgboost.XGBClassifier\n",
    "# -------------------------\n",
    "# default paramters\n",
    "# class xgboost.XGBClassifier(max_depth=3, learning_rate=0.1, n_estimators=100, silent=True, \n",
    "# objective='binary:logistic', booster='gbtree', n_jobs=1, nthread=None, gamma=0, min_child_weight=1, \n",
    "# max_delta_step=0, subsample=1, colsample_bytree=1, colsample_bylevel=1, reg_alpha=0, reg_lambda=1, \n",
    "# scale_pos_weight=1, base_score=0.5, random_state=0, seed=None, missing=None, **kwargs)\n",
    "\n",
    "# some of methods of RandomForestRegressor()\n",
    "# fit(X, y, sample_weight=None, eval_set=None, eval_metric=None, early_stopping_rounds=None, verbose=True, xgb_model=None)\n",
    "# get_params([deep])\tGet parameters for this estimator.\n",
    "# predict(data, output_margin=False, ntree_limit=0) : Predict with data. NOTE: This function is not thread safe.\n",
    "# get_score(importance_type='weight') -> get the feature importance\n",
    "# -----------------------\n",
    "# video link2: https://www.appliedaicourse.com/course/applied-ai-course-online/lessons/what-are-ensembles/\n",
    "# -----------------------\n",
    "\n",
    "x_cfl=XGBClassifier(n_estimators=1000,subsample=0.5,learning_rate=0.15,colsample_bytree=0.5,max_depth=3)\n",
    "x_cfl.fit(X_train_asm,y_train_asm)\n",
    "c_cfl=CalibratedClassifierCV(x_cfl,method='sigmoid')\n",
    "c_cfl.fit(X_train_asm,y_train_asm)\n",
    "\n",
    "predict_y = c_cfl.predict_proba(X_train_asm)\n",
    "print ('train loss',log_loss(y_train_asm, predict_y))\n",
    "predict_y = c_cfl.predict_proba(X_cv_asm)\n",
    "print ('cv loss',log_loss(y_cv_asm, predict_y))\n",
    "predict_y = c_cfl.predict_proba(X_test_asm)\n",
    "print ('test loss',log_loss(y_test_asm, predict_y))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Xgboost with asm image features gives us the best log loss 0.008"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
