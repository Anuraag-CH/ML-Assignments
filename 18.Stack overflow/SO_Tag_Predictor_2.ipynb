{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ihs1Hb3RU93S"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "#from wordcloud import WordCloud\n",
    "import re\n",
    "import os\n",
    "from sqlalchemy import create_engine # database connection\n",
    "import datetime as dt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score,precision_score,recall_score\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from skmultilearn.adapt import mlknn\n",
    "from skmultilearn.problem_transform import ClassifierChain\n",
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "from skmultilearn.problem_transform import LabelPowerset\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rAZDkLNPU93Y"
   },
   "source": [
    "# Stack Overflow: Tag Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TpVk_usWU93a"
   },
   "source": [
    "<h1>1. Business Problem </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5-1WlUboU93c"
   },
   "source": [
    "<h2> 1.1 Description </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "37SdsKLdU93e"
   },
   "source": [
    "<p style='font-size:18px'><b> Description </b></p>\n",
    "<p>\n",
    "Stack Overflow is the largest, most trusted online community for developers to learn, share their programming knowledge, and build their careers.<br />\n",
    "<br />\n",
    "Stack Overflow is something which every programmer use one way or another. Each month, over 50 million developers come to Stack Overflow to learn, share their knowledge, and build their careers. It features questions and answers on a wide range of topics in computer programming. The website serves as a platform for users to ask and answer questions, and, through membership and active participation, to vote questions and answers up or down and edit questions and answers in a fashion similar to a wiki or Digg. As of April 2014 Stack Overflow has over 4,000,000 registered users, and it exceeded 10,000,000 questions in late August 2015. Based on the type of tags assigned to questions, the top eight most discussed topics on the site are: Java, JavaScript, C#, PHP, Android, jQuery, Python and HTML.<br />\n",
    "<br />\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9brAMjUsU93f"
   },
   "source": [
    "<p style='font-size:18px'><b> Problem Statemtent </b></p>\n",
    "Suggest the tags based on the content that was there in the question posted on Stackoverflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "URJINSY2U93h"
   },
   "source": [
    "<p style='font-size:18px'><b> Source:  </b> https://www.kaggle.com/c/facebook-recruiting-iii-keyword-extraction/</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ggC_3T9XU93j"
   },
   "source": [
    "<h2> 1.2 Source / useful links </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Nr7T9iknU93l"
   },
   "source": [
    "Data Source : https://www.kaggle.com/c/facebook-recruiting-iii-keyword-extraction/data <br>\n",
    "Youtube : https://youtu.be/nNDqbUhtIRg <br>\n",
    "Research paper : https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tagging-1.pdf <br>\n",
    "Research paper : https://dl.acm.org/citation.cfm?id=2660970&dl=ACM&coll=DL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hbXm8lAuU93p"
   },
   "source": [
    "<h2> 1.3 Real World / Business Objectives and Constraints </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "88TwKPItU93q"
   },
   "source": [
    "1. Predict as many tags as possible with high precision and recall.\n",
    "2. Incorrect tags could impact customer experience on StackOverflow.\n",
    "3. No strict latency constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lE0wX1roU93s"
   },
   "source": [
    "<h1>2. Machine Learning problem </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ynAszp_uU93u"
   },
   "source": [
    "<h2> 2.1 Data </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "elwijxMGU93w"
   },
   "source": [
    "<h3> 2.1.1 Data Overview </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mdFiIj7_U93x"
   },
   "source": [
    "Refer: https://www.kaggle.com/c/facebook-recruiting-iii-keyword-extraction/data\n",
    "<br>\n",
    "All of the data is in 2 files: Train and Test.<br />\n",
    "<pre>\n",
    "<b>Train.csv</b> contains 4 columns: Id,Title,Body,Tags.<br />\n",
    "<b>Test.csv</b> contains the same columns but without the Tags, which you are to predict.<br />\n",
    "<b>Size of Train.csv</b> - 6.75GB<br />\n",
    "<b>Size of Test.csv</b> - 2GB<br />\n",
    "<b>Number of rows in Train.csv</b> = 6034195<br />\n",
    "</pre>\n",
    "The questions are randomized and contains a mix of verbose text sites as well as sites related to math and programming. The number of questions from each site may vary, and no filtering has been performed on the questions (such as closed questions).<br />\n",
    "<br />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ji0A66hWU93z"
   },
   "source": [
    "__Data Field Explaination__\n",
    "\n",
    "Dataset contains 6,034,195 rows. The columns in the table are:<br />\n",
    "<pre>\n",
    "<b>Id</b> - Unique identifier for each question<br />\n",
    "<b>Title</b> - The question's title<br />\n",
    "<b>Body</b> - The body of the question<br />\n",
    "<b>Tags</b> - The tags associated with the question in a space-seperated format (all lowercase, should not contain tabs '\\t' or ampersands '&')<br />\n",
    "</pre>\n",
    "\n",
    "<br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WNDiy42GU931"
   },
   "source": [
    "<h3>2.1.2 Example Data point </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "D5IcxRbYU932"
   },
   "source": [
    "<pre>\n",
    "<b>Title</b>:  Implementing Boundary Value Analysis of Software Testing in a C++ program?\n",
    "<b>Body </b>: <pre><code>\n",
    "        #include&lt;\n",
    "        iostream&gt;\\n\n",
    "        #include&lt;\n",
    "        stdlib.h&gt;\\n\\n\n",
    "        using namespace std;\\n\\n\n",
    "        int main()\\n\n",
    "        {\\n\n",
    "                 int n,a[n],x,c,u[n],m[n],e[n][4];\\n         \n",
    "                 cout&lt;&lt;\"Enter the number of variables\";\\n         cin&gt;&gt;n;\\n\\n         \n",
    "                 cout&lt;&lt;\"Enter the Lower, and Upper Limits of the variables\";\\n         \n",
    "                 for(int y=1; y&lt;n+1; y++)\\n         \n",
    "                 {\\n                 \n",
    "                    cin&gt;&gt;m[y];\\n                 \n",
    "                    cin&gt;&gt;u[y];\\n         \n",
    "                 }\\n         \n",
    "                 for(x=1; x&lt;n+1; x++)\\n         \n",
    "                 {\\n                 \n",
    "                    a[x] = (m[x] + u[x])/2;\\n         \n",
    "                 }\\n         \n",
    "                 c=(n*4)-4;\\n         \n",
    "                 for(int a1=1; a1&lt;n+1; a1++)\\n         \n",
    "                 {\\n\\n             \n",
    "                    e[a1][0] = m[a1];\\n             \n",
    "                    e[a1][1] = m[a1]+1;\\n             \n",
    "                    e[a1][2] = u[a1]-1;\\n             \n",
    "                    e[a1][3] = u[a1];\\n         \n",
    "                 }\\n         \n",
    "                 for(int i=1; i&lt;n+1; i++)\\n         \n",
    "                 {\\n            \n",
    "                    for(int l=1; l&lt;=i; l++)\\n            \n",
    "                    {\\n                 \n",
    "                        if(l!=1)\\n                 \n",
    "                        {\\n                    \n",
    "                            cout&lt;&lt;a[l]&lt;&lt;\"\\\\t\";\\n                 \n",
    "                        }\\n            \n",
    "                    }\\n            \n",
    "                    for(int j=0; j&lt;4; j++)\\n            \n",
    "                    {\\n                \n",
    "                        cout&lt;&lt;e[i][j];\\n                \n",
    "                        for(int k=0; k&lt;n-(i+1); k++)\\n                \n",
    "                        {\\n                    \n",
    "                            cout&lt;&lt;a[k]&lt;&lt;\"\\\\t\";\\n               \n",
    "                        }\\n                \n",
    "                        cout&lt;&lt;\"\\\\n\";\\n            \n",
    "                    }\\n        \n",
    "                 }    \\n\\n        \n",
    "                 system(\"PAUSE\");\\n        \n",
    "                 return 0;    \\n\n",
    "        }\\n\n",
    "        </code></pre>\\n\\n\n",
    "        <p>The answer should come in the form of a table like</p>\\n\\n\n",
    "        <pre><code>       \n",
    "        1            50              50\\n       \n",
    "        2            50              50\\n       \n",
    "        99           50              50\\n       \n",
    "        100          50              50\\n       \n",
    "        50           1               50\\n       \n",
    "        50           2               50\\n       \n",
    "        50           99              50\\n       \n",
    "        50           100             50\\n       \n",
    "        50           50              1\\n       \n",
    "        50           50              2\\n       \n",
    "        50           50              99\\n       \n",
    "        50           50              100\\n\n",
    "        </code></pre>\\n\\n\n",
    "        <p>if the no of inputs is 3 and their ranges are\\n\n",
    "        1,100\\n\n",
    "        1,100\\n\n",
    "        1,100\\n\n",
    "        (could be varied too)</p>\\n\\n\n",
    "        <p>The output is not coming,can anyone correct the code or tell me what\\'s wrong?</p>\\n'\n",
    "<b>Tags </b>: 'c++ c'\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MomJjFPnU934"
   },
   "source": [
    "<h2>2.2 Mapping the real-world problem to a Machine Learning Problem </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sJ5lyxIUU936"
   },
   "source": [
    "<h3> 2.2.1 Type of Machine Learning Problem </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5Jb9u-38U938"
   },
   "source": [
    "<p> It is a multi-label classification problem  <br>\n",
    "<b>Multi-label Classification</b>: Multilabel classification assigns to each sample a set of target labels. This can be thought as predicting properties of a data-point that are not mutually exclusive, such as topics that are relevant for a document. A question on Stackoverflow might be about any of C, Pointers, FileIO and/or memory-management at the same time or none of these. <br>\n",
    "__Credit__: http://scikit-learn.org/stable/modules/multiclass.html\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QIOwycKkU93_"
   },
   "source": [
    "<h3>2.2.2 Performance metric </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DDQXZ4k5U94A"
   },
   "source": [
    "<b>Micro-Averaged F1-Score (Mean F Score) </b>: \n",
    "The F1 score can be interpreted as a weighted average of the precision and recall, where an F1 score reaches its best value at 1 and worst score at 0. The relative contribution of precision and recall to the F1 score are equal. The formula for the F1 score is:\n",
    "\n",
    "<i>F1 = 2 * (precision * recall) / (precision + recall)</i><br>\n",
    "\n",
    "In the multi-class and multi-label case, this is the weighted average of the F1 score of each class. <br>\n",
    "\n",
    "<b>'Micro f1 score': </b><br>\n",
    "Calculate metrics globally by counting the total true positives, false negatives and false positives. This is a better metric when we have class imbalance.\n",
    "<br>\n",
    "\n",
    "<b>'Macro f1 score': </b><br>\n",
    "Calculate metrics for each label, and find their unweighted mean. This does not take label imbalance into account.\n",
    "<br>\n",
    "\n",
    "https://www.kaggle.com/wiki/MeanFScore <br>\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html <br>\n",
    "<br>\n",
    "<b> Hamming loss </b>: The Hamming loss is the fraction of labels that are incorrectly predicted. <br>\n",
    "https://www.kaggle.com/wiki/HammingLoss <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xqsVRjSmU94C"
   },
   "source": [
    "<h1> 3. Exploratory Data Analysis </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "azda-BmRU94H"
   },
   "source": [
    "<h2> 3.1 Data Loading and Cleaning </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "62GDC_VjU94J"
   },
   "source": [
    "<h3>3.1.1 Using Pandas with SQLite to Load the data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "289tS71cU94L"
   },
   "outputs": [],
   "source": [
    "#Creating db file from csv\n",
    "#Learn SQL: https://www.w3schools.com/sql/default.asp\n",
    "if not os.path.isfile('train.db'):\n",
    "    start = datetime.now()\n",
    "    disk_engine = create_engine('sqlite:///train.db')\n",
    "    start = dt.datetime.now()\n",
    "    chunksize = 180000\n",
    "    j = 0\n",
    "    index_start = 1\n",
    "    for df in pd.read_csv('Train.csv', names=['Id', 'Title', 'Body', 'Tags'], chunksize=chunksize, iterator=True, encoding='utf-8', ):\n",
    "        df.index += index_start\n",
    "        j+=1\n",
    "        print('{} rows'.format(j*chunksize))\n",
    "        df.to_sql('data', disk_engine, if_exists='append')\n",
    "        index_start = df.index[-1] + 1\n",
    "    print(\"Time taken to run this cell :\", datetime.now() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d5yUhXVNU94Q"
   },
   "source": [
    "<h3> 3.1.2 Counting the number of rows </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3ORCclXYU94R",
    "outputId": "9625d8e0-bf34-413e-8246-932fa1cb21b7"
   },
   "outputs": [],
   "source": [
    "if os.path.isfile('train.db'):\n",
    "    start = datetime.now()\n",
    "    con = sqlite3.connect('train.db')\n",
    "    num_rows = pd.read_sql_query(\"\"\"SELECT count(*) FROM data\"\"\", con)\n",
    "    #Always remember to close the database\n",
    "    print(\"Number of rows in the database :\",\"\\n\",num_rows['count(*)'].values[0])\n",
    "    con.close()\n",
    "    print(\"Time taken to count the number of rows :\", datetime.now() - start)\n",
    "else:\n",
    "    print(\"Please download the train.db file from drive or run the above cell to genarate train.db file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xso2eOEvU94Z"
   },
   "source": [
    "<h3>3.1.3 Checking for duplicates </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iBHCcr3DU94b",
    "outputId": "2340d414-8570-4d7e-fda6-a69c39f13780"
   },
   "outputs": [],
   "source": [
    "#Learn SQl: https://www.w3schools.com/sql/default.asp\n",
    "if os.path.isfile('train.db'):\n",
    "    start = datetime.now()\n",
    "    con = sqlite3.connect('train.db')\n",
    "    df_no_dup = pd.read_sql_query('SELECT Title, Body, Tags, COUNT(*) as cnt_dup FROM data GROUP BY Title, Body, Tags', con)\n",
    "    con.close()\n",
    "    print(\"Time taken to run this cell :\", datetime.now() - start)\n",
    "else:\n",
    "    print(\"Please download the train.db file from drive or run the first to genarate train.db file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gap4NRPWU94h",
    "outputId": "e0af72d7-4faf-4232-849b-d27bc1963221"
   },
   "outputs": [],
   "source": [
    "df_no_dup.head()\n",
    "# we can observe that there are duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JzFO4EeDU94n",
    "outputId": "198980b2-6480-4a49-ab0a-8074199f5edb"
   },
   "outputs": [],
   "source": [
    "print(\"number of duplicate questions :\", num_rows['count(*)'].values[0]- df_no_dup.shape[0], \"(\",(1-((df_no_dup.shape[0])/(num_rows['count(*)'].values[0])))*100,\"% )\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gd2VdpN6U94t",
    "outputId": "05af061e-31e0-4a93-d41a-d738c5b9e890"
   },
   "outputs": [],
   "source": [
    "# number of times each question appeared in our database\n",
    "df_no_dup.cnt_dup.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EogeNAhCU94z",
    "outputId": "661010f0-0d38-4a33-d20f-e94ef51344ab"
   },
   "outputs": [],
   "source": [
    "start = datetime.now()\n",
    "df_no_dup[\"tag_count\"] = df_no_dup[\"Tags\"].str.count(' ') + 1\n",
    "# adding a new feature number of tags per question\n",
    "print(\"Time taken to run this cell :\", datetime.now() - start)\n",
    "df_no_dup.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iItMHo6MU948",
    "outputId": "824508df-c0c0-4d33-a667-83af84360bfc"
   },
   "outputs": [],
   "source": [
    "# distribution of number of tags per question\n",
    "df_no_dup.tag_count.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2xMgCGUKU95C"
   },
   "outputs": [],
   "source": [
    "#Creating a new database with no duplicates\n",
    "if not os.path.isfile('train_no_dup.db'):\n",
    "    disk_dup = create_engine(\"sqlite:///train_no_dup.db\")\n",
    "    no_dup = pd.DataFrame(df_no_dup, columns=['Title', 'Body', 'Tags'])\n",
    "    no_dup.to_sql('no_dup_train',disk_dup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6Ou53MzeU95H",
    "outputId": "0b643de8-3481-45d0-ff2e-8400f7f5f12b"
   },
   "outputs": [],
   "source": [
    "#This method seems more appropriate to work with this much data.\n",
    "#creating the connection with database file.\n",
    "if os.path.isfile('train_no_dup.db'):\n",
    "    start = datetime.now()\n",
    "    con = sqlite3.connect('train_no_dup.db')\n",
    "    tag_data = pd.read_sql_query(\"\"\"SELECT Tags FROM no_dup_train\"\"\", con)\n",
    "    #Always remember to close the database\n",
    "    con.close()\n",
    "\n",
    "    # Let's now drop unwanted column.\n",
    "    tag_data.drop(tag_data.index[0], inplace=True)\n",
    "    #Printing first 5 columns from our data frame\n",
    "    tag_data.head()\n",
    "    print(\"Time taken to run this cell :\", datetime.now() - start)\n",
    "else:\n",
    "    print(\"Please download the train.db file from drive or run the above cells to genarate train.db file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hwZVL3doU95O"
   },
   "source": [
    "<h2> 3.2 Analysis of Tags </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zYs5peW8U95P"
   },
   "source": [
    "<h3> 3.2.1 Total number of unique tags </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ROdC95M_U95Q"
   },
   "outputs": [],
   "source": [
    "# Importing & Initializing the \"CountVectorizer\" object, which \n",
    "#is scikit-learn's bag of words tool.\n",
    "\n",
    "#by default 'split()' will tokenize each tag using space.\n",
    "vectorizer = CountVectorizer(tokenizer = lambda x: x.split())\n",
    "# fit_transform() does two functions: First, it fits the model\n",
    "# and learns the vocabulary; second, it transforms our training data\n",
    "# into feature vectors. The input to fit_transform should be a list of strings.\n",
    "tag_dtm = vectorizer.fit_transform(tag_data['Tags'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Oz5N0GH0U95V",
    "outputId": "a65cd48f-6df2-44f5-e77c-99b38bb7ae2a"
   },
   "outputs": [],
   "source": [
    "print(\"Number of data points :\", tag_dtm.shape[0])\n",
    "print(\"Number of unique tags :\", tag_dtm.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Otn6CUuQU95b",
    "outputId": "91ff8093-ebae-4218-a786-a117a9eadc0e"
   },
   "outputs": [],
   "source": [
    "#'get_feature_name()' gives us the vocabulary.\n",
    "tags = vectorizer.get_feature_names()\n",
    "#Lets look at the tags we have.\n",
    "print(\"Some of the tags we have :\", tags[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NQa3ETSeU95g"
   },
   "source": [
    "<h3> 3.2.3 Number of times a tag appeared </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "beThyuyqU95h"
   },
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/15115765/how-to-access-sparse-matrix-elements\n",
    "#Lets now store the document term matrix in a dictionary.\n",
    "freqs = tag_dtm.sum(axis=0).A1\n",
    "result = dict(zip(tags, freqs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6ALwSSx9U95m",
    "outputId": "452a449b-5605-4c32-e571-961eee563161"
   },
   "outputs": [],
   "source": [
    "#Saving this dictionary to csv files.\n",
    "if not os.path.isfile('tag_counts_dict_dtm.csv'):\n",
    "    with open('tag_counts_dict_dtm.csv', 'w') as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        for key, value in result.items():\n",
    "            writer.writerow([key, value])\n",
    "tag_df = pd.read_csv(\"tag_counts_dict_dtm.csv\", names=['Tags', 'Counts'])\n",
    "tag_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mzmS8yiNU95t"
   },
   "outputs": [],
   "source": [
    "tag_df_sorted = tag_df.sort_values(['Counts'], ascending=False)\n",
    "tag_counts = tag_df_sorted['Counts'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6mcj55FIU95z",
    "outputId": "d6c9d567-a9a4-48e6-9d44-6f099f198339"
   },
   "outputs": [],
   "source": [
    "plt.plot(tag_counts)\n",
    "plt.title(\"Distribution of number of times tag appeared questions\")\n",
    "plt.grid()\n",
    "plt.xlabel(\"Tag number\")\n",
    "plt.ylabel(\"Number of times tag appeared\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UzTqln6XU955",
    "outputId": "07f08ef0-d26b-4797-eb3c-6586011e2588"
   },
   "outputs": [],
   "source": [
    "plt.plot(tag_counts[0:10000])\n",
    "plt.title('first 10k tags: Distribution of number of times tag appeared questions')\n",
    "plt.grid()\n",
    "plt.xlabel(\"Tag number\")\n",
    "plt.ylabel(\"Number of times tag appeared\")\n",
    "plt.show()\n",
    "print(len(tag_counts[0:10000:25]), tag_counts[0:10000:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ntm8E_K9U95-",
    "outputId": "750edd11-e52b-46bd-caba-1835994a0e05"
   },
   "outputs": [],
   "source": [
    "plt.plot(tag_counts[0:1000])\n",
    "plt.title('first 1k tags: Distribution of number of times tag appeared questions')\n",
    "plt.grid()\n",
    "plt.xlabel(\"Tag number\")\n",
    "plt.ylabel(\"Number of times tag appeared\")\n",
    "plt.show()\n",
    "print(len(tag_counts[0:1000:5]), tag_counts[0:1000:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-SRUeKuWU96I",
    "outputId": "f20a242c-2f7d-4e7d-f3f2-291d2ea38caa"
   },
   "outputs": [],
   "source": [
    "plt.plot(tag_counts[0:500])\n",
    "plt.title('first 500 tags: Distribution of number of times tag appeared questions')\n",
    "plt.grid()\n",
    "plt.xlabel(\"Tag number\")\n",
    "plt.ylabel(\"Number of times tag appeared\")\n",
    "plt.show()\n",
    "print(len(tag_counts[0:500:5]), tag_counts[0:500:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CCOE0Nu4U96N",
    "outputId": "1f8df41e-b0dc-4d7f-a3b2-0a0c472ef448"
   },
   "outputs": [],
   "source": [
    "plt.plot(tag_counts[0:100], c='b')\n",
    "plt.scatter(x=list(range(0,100,5)), y=tag_counts[0:100:5], c='orange', label=\"quantiles with 0.05 intervals\")\n",
    "# quantiles with 0.25 difference\n",
    "plt.scatter(x=list(range(0,100,25)), y=tag_counts[0:100:25], c='m', label = \"quantiles with 0.25 intervals\")\n",
    "\n",
    "for x,y in zip(list(range(0,100,25)), tag_counts[0:100:25]):\n",
    "    plt.annotate(s=\"({} , {})\".format(x,y), xy=(x,y), xytext=(x-0.05, y+500))\n",
    "\n",
    "plt.title('first 100 tags: Distribution of number of times tag appeared questions')\n",
    "plt.grid()\n",
    "plt.xlabel(\"Tag number\")\n",
    "plt.ylabel(\"Number of times tag appeared\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(len(tag_counts[0:100:5]), tag_counts[0:100:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MnwXytypU96R",
    "outputId": "3427f2fd-4b27-435e-d1a8-f1bcf4515d9c"
   },
   "outputs": [],
   "source": [
    "# Store tags greater than 10K in one list\n",
    "lst_tags_gt_10k = tag_df[tag_df.Counts>10000].Tags\n",
    "#Print the length of the list\n",
    "print ('{} Tags are used more than 10000 times'.format(len(lst_tags_gt_10k)))\n",
    "# Store tags greater than 100K in one list\n",
    "lst_tags_gt_100k = tag_df[tag_df.Counts>100000].Tags\n",
    "#Print the length of the list.\n",
    "print ('{} Tags are used more than 100000 times'.format(len(lst_tags_gt_100k)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oOXPF7q1U96W"
   },
   "source": [
    "<b>Observations:</b><br />\n",
    "1. There are total 153 tags which are used more than 10000 times.\n",
    "2. 14 tags are used more than 100000 times.\n",
    "3. Most frequent tag (i.e. c#) is used 331505 times.\n",
    "4. Since some tags occur much more frequenctly than others, Micro-averaged F1-score is the appropriate metric for this probelm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zb-Tg_sgU96a"
   },
   "source": [
    "<h3> 3.2.4 Tags Per Question </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bsPStbjGU96g",
    "outputId": "2e57d47f-1ae9-4776-ab17-d4315b9d0c14"
   },
   "outputs": [],
   "source": [
    "#Storing the count of tag in each question in list 'tag_count'\n",
    "tag_quest_count = tag_dtm.sum(axis=1).tolist()\n",
    "#Converting list of lists into single list, we will get [[3], [4], [2], [2], [3]] and we are converting this to [3, 4, 2, 2, 3]\n",
    "tag_quest_count=[int(j) for i in tag_quest_count for j in i]\n",
    "print ('We have total {} datapoints.'.format(len(tag_quest_count)))\n",
    "\n",
    "print(tag_quest_count[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DLbm7crfU96n",
    "outputId": "f71ff030-9937-4081-9d4a-fb2c62386e11"
   },
   "outputs": [],
   "source": [
    "print( \"Maximum number of tags per question: %d\"%max(tag_quest_count))\n",
    "print( \"Minimum number of tags per question: %d\"%min(tag_quest_count))\n",
    "print( \"Avg. number of tags per question: %f\"% ((sum(tag_quest_count)*1.0)/len(tag_quest_count)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mb1vdd8KU96x",
    "outputId": "4579b7c7-ff27-4fb3-f3b9-29401610a3a0"
   },
   "outputs": [],
   "source": [
    "sns.countplot(tag_quest_count, palette='gist_rainbow')\n",
    "plt.title(\"Number of tags in the questions \")\n",
    "plt.xlabel(\"Number of Tags\")\n",
    "plt.ylabel(\"Number of questions\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0RsUcQkNU963"
   },
   "source": [
    "<b>Observations:</b><br />\n",
    "1. Maximum number of tags per question: 5\n",
    "2. Minimum number of tags per question: 1\n",
    "3. Avg. number of tags per question: 2.899\n",
    "4. Most of the questions are having 2 or 3 tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7-M5-A-MU963"
   },
   "source": [
    "<h3>3.2.5 Most Frequent Tags </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Brokf0gSU965",
    "outputId": "b9794ec6-8e67-49ad-f641-069638dbd951",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Ploting word cloud\n",
    "start = datetime.now()\n",
    "\n",
    "# Lets first convert the 'result' dictionary to 'list of tuples'\n",
    "tup = dict(result.items())\n",
    "#Initializing WordCloud using frequencies of tags.\n",
    "wordcloud = WordCloud(    background_color='black',\n",
    "                          width=1600,\n",
    "                          height=800,\n",
    "                    ).generate_from_frequencies(tup)\n",
    "\n",
    "fig = plt.figure(figsize=(30,20))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.tight_layout(pad=0)\n",
    "fig.savefig(\"tag.png\")\n",
    "plt.show()\n",
    "print(\"Time taken to run this cell :\", datetime.now() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_n7RaKj2U96-"
   },
   "source": [
    "<b>Observations:</b><br />\n",
    "A look at the word cloud shows that \"c#\", \"java\", \"php\", \"asp.net\", \"javascript\", \"c++\" are some of the most frequent tags."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hpc8IWjqU96_"
   },
   "source": [
    "<h3> 3.2.6 The top 20 tags </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ov3WmIEHU97A",
    "outputId": "8e4867c3-81da-445d-a7b3-fc8d4c283be0"
   },
   "outputs": [],
   "source": [
    "i=np.arange(30)\n",
    "tag_df_sorted.head(30).plot(kind='bar')\n",
    "plt.title('Frequency of top 20 tags')\n",
    "plt.xticks(i, tag_df_sorted['Tags'])\n",
    "plt.xlabel('Tags')\n",
    "plt.ylabel('Counts')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rlmizz7LU97D"
   },
   "source": [
    "<b>Observations:</b><br />\n",
    "1. Majority of the most frequent tags are programming language.\n",
    "2. C# is the top most frequent programming language.\n",
    "3. Android, IOS, Linux and windows are among the top most frequent operating systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I-Z7F0_mU97F"
   },
   "source": [
    "<h3> 3.3 Cleaning and preprocessing of Questions </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cWzF-nN6U97G"
   },
   "source": [
    "<h3> 3.3.1 Preprocessing </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MeR1aoQaU97H"
   },
   "source": [
    "<ol> \n",
    "    <li> Sample 1M data points </li>\n",
    "    <li> Separate out code-snippets from Body </li>\n",
    "    <li> Remove Spcial characters from Question title and description (not in code)</li>\n",
    "    <li> Remove stop words (Except 'C') </li>\n",
    "    <li> Remove HTML Tags </li>\n",
    "    <li> Convert all the characters into small letters </li>\n",
    "    <li> Use SnowballStemmer to stem the words </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qr2xhpsAU97I"
   },
   "outputs": [],
   "source": [
    "def striphtml(data):\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, ' ', str(data))\n",
    "    return cleantext\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LCDUa4KxU97L",
    "outputId": "8270b10f-cf17-4025-9440-5f2b2b62579c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables in the databse:\n",
      "QuestionsProcessed\n"
     ]
    }
   ],
   "source": [
    "#http://www.sqlitetutorial.net/sqlite-python/create-tables/\n",
    "def create_connection(db_file):\n",
    "    \"\"\" create a database connection to the SQLite database\n",
    "        specified by db_file\n",
    "    :param db_file: database file\n",
    "    :return: Connection object or None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_file)\n",
    "        return conn\n",
    "    except Error as e:\n",
    "        print(e)\n",
    " \n",
    "    return None\n",
    "\n",
    "def create_table(conn, create_table_sql):\n",
    "    \"\"\" create a table from the create_table_sql statement\n",
    "    :param conn: Connection object\n",
    "    :param create_table_sql: a CREATE TABLE statement\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    try:\n",
    "        c = conn.cursor()\n",
    "        c.execute(create_table_sql)\n",
    "    except Error as e:\n",
    "        print(e)\n",
    "        \n",
    "def checkTableExists(dbcon):\n",
    "    cursr = dbcon.cursor()\n",
    "    str = \"select name from sqlite_master where type='table'\"\n",
    "    table_names = cursr.execute(str)\n",
    "    print(\"Tables in the databse:\")\n",
    "    tables =table_names.fetchall() \n",
    "    print(tables[0][0])\n",
    "    return(len(tables))\n",
    "\n",
    "def create_database_table(database, query):\n",
    "    conn = create_connection(database)\n",
    "    if conn is not None:\n",
    "        create_table(conn, query)\n",
    "        checkTableExists(conn)\n",
    "    else:\n",
    "        print(\"Error! cannot create the database connection.\")\n",
    "    conn.close()\n",
    "\n",
    "sql_create_table = \"\"\"CREATE TABLE IF NOT EXISTS QuestionsProcessed (question text NOT NULL, code text, tags text, words_pre integer, words_post integer, is_code integer);\"\"\"\n",
    "create_database_table(\"Processed.db\", sql_create_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qz092AdFU97P",
    "outputId": "b00a3eec-60d8-4c08-83c8-54debd0a4123"
   },
   "outputs": [],
   "source": [
    "# http://www.sqlitetutorial.net/sqlite-delete/\n",
    "# https://stackoverflow.com/questions/2279706/select-random-row-from-a-sqlite-table\n",
    "start = datetime.now()\n",
    "read_db = 'train_no_dup.db'\n",
    "write_db = 'Processed.db'\n",
    "if os.path.isfile(read_db):\n",
    "    conn_r = create_connection(read_db)\n",
    "    if conn_r is not None:\n",
    "        reader =conn_r.cursor()\n",
    "        reader.execute(\"SELECT Title, Body, Tags From no_dup_train ORDER BY RANDOM() LIMIT 1000000;\")\n",
    "\n",
    "if os.path.isfile(write_db):\n",
    "    conn_w = create_connection(write_db)\n",
    "    if conn_w is not None:\n",
    "        tables = checkTableExists(conn_w)\n",
    "        writer =conn_w.cursor()\n",
    "        if tables != 0:\n",
    "            writer.execute(\"DELETE FROM QuestionsProcessed WHERE 1\")\n",
    "            print(\"Cleared All the rows\")\n",
    "print(\"Time taken to run this cell :\", datetime.now() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FELLR5FBU97U"
   },
   "source": [
    "__ we create a new data base to store the sampled and preprocessed questions __"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "clKVIuAcU97W",
    "outputId": "eab75c6e-f76e-4cf2-eb4d-c589a9908be5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#http://www.bernzilla.com/2008/05/13/selecting-a-random-row-from-an-sqlite-table/\n",
    "\n",
    "start = datetime.now()\n",
    "preprocessed_data_list=[]\n",
    "reader.fetchone()\n",
    "questions_with_code=0\n",
    "len_pre=0\n",
    "len_post=0\n",
    "questions_proccesed = 0\n",
    "for row in reader:\n",
    "\n",
    "    is_code = 0\n",
    "\n",
    "    title, question, tags = row[0], row[1], row[2]\n",
    "\n",
    "    if '<code>' in question:\n",
    "        questions_with_code+=1\n",
    "        is_code = 1\n",
    "    x = len(question)+len(title)\n",
    "    len_pre+=x\n",
    "\n",
    "    code = str(re.findall(r'<code>(.*?)</code>', question, flags=re.DOTALL))\n",
    "\n",
    "    question=re.sub('<code>(.*?)</code>', '', question, flags=re.MULTILINE|re.DOTALL)\n",
    "    question=striphtml(question.encode('utf-8'))\n",
    "\n",
    "    title=title.encode('utf-8')\n",
    "\n",
    "    question=str(title)+\" \"+str(question)\n",
    "    question=re.sub(r'[^A-Za-z]+',' ',question)\n",
    "    words=word_tokenize(str(question.lower()))\n",
    "\n",
    "    #Removing all single letter and and stopwords from question exceptt for the letter 'c'\n",
    "    question=' '.join(str(stemmer.stem(j)) for j in words if j not in stop_words and (len(j)!=1 or j=='c'))\n",
    "\n",
    "    len_post+=len(question)\n",
    "    tup = (question,code,tags,x,len(question),is_code)\n",
    "    questions_proccesed += 1\n",
    "    writer.execute(\"insert into QuestionsProcessed(question,code,tags,words_pre,words_post,is_code) values (?,?,?,?,?,?)\",tup)\n",
    "    if (questions_proccesed%100000==0):\n",
    "        print(\"number of questions completed=\",questions_proccesed)\n",
    "\n",
    "no_dup_avg_len_pre=(len_pre*1.0)/questions_proccesed\n",
    "no_dup_avg_len_post=(len_post*1.0)/questions_proccesed\n",
    "\n",
    "print( \"Avg. length of questions(Title+Body) before processing: %d\"%no_dup_avg_len_pre)\n",
    "print( \"Avg. length of questions(Title+Body) after processing: %d\"%no_dup_avg_len_post)\n",
    "print (\"Percent of questions containing code: %d\"%((questions_with_code*100.0)/questions_proccesed))\n",
    "\n",
    "print(\"Time taken to run this cell :\", datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fMihWt4uU97b"
   },
   "outputs": [],
   "source": [
    "# dont forget to close the connections, or else you will end up with locks\n",
    "conn_r.commit()\n",
    "conn_w.commit()\n",
    "conn_r.close()\n",
    "conn_w.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y8VzP3hsU97e",
    "outputId": "5efb2876-7d07-438f-aeac-edeecc694f5b"
   },
   "outputs": [],
   "source": [
    "if os.path.isfile(write_db):\n",
    "    conn_r = create_connection(write_db)\n",
    "    if conn_r is not None:\n",
    "        reader =conn_r.cursor()\n",
    "        reader.execute(\"SELECT question From QuestionsProcessed LIMIT 10\")\n",
    "        print(\"Questions after preprocessed\")\n",
    "        print('='*100)\n",
    "        reader.fetchone()\n",
    "        for row in reader:\n",
    "            print(row)\n",
    "            print('-'*100)\n",
    "conn_r.commit()\n",
    "conn_r.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jSJXxeS0U97i"
   },
   "outputs": [],
   "source": [
    "#Taking 1 Million entries to a dataframe.\n",
    "write_db = 'Processed.db'\n",
    "if os.path.isfile(write_db):\n",
    "    conn_r = create_connection(write_db)\n",
    "    if conn_r is not None:\n",
    "        preprocessed_data = pd.read_sql_query(\"\"\"SELECT question, Tags FROM QuestionsProcessed\"\"\", conn_r)\n",
    "conn_r.commit()\n",
    "conn_r.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ANsenX1EU97l",
    "outputId": "6170e6c6-86e0-48e9-d1c1-190c0cfc15be"
   },
   "outputs": [],
   "source": [
    "preprocessed_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I6vsCoLOU97r",
    "outputId": "494f221b-f6ed-4aeb-a557-aeb6d209cf38"
   },
   "outputs": [],
   "source": [
    "print(\"number of data points in sample :\", preprocessed_data.shape[0])\n",
    "print(\"number of dimensions :\", preprocessed_data.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qB0bL2drU97w"
   },
   "source": [
    "<h1>4. Machine Learning Models </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BZ3-VPbqU97w"
   },
   "source": [
    "<h2> 4.1 Converting tags for multilabel problems </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K88oaTD9U97y"
   },
   "source": [
    "<table>\n",
    "<tr>\n",
    "<th>X</th><th>y1</th><th>y2</th><th>y3</th><th>y4</th>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>x1</td><td>0</td><td>1</td><td>1</td><td>0</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>x1</td><td>1</td><td>0</td><td>0</td><td>0</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>x1</td><td>0</td><td>1</td><td>0</td><td>0</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ttr2m-qiU97z"
   },
   "outputs": [],
   "source": [
    "# binary='true' will give a binary vectorizer\n",
    "vectorizer = CountVectorizer(tokenizer = lambda x: x.split(), binary='true')\n",
    "multilabel_y = vectorizer.fit_transform(preprocessed_data['tags'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_VCwyfHxU972"
   },
   "source": [
    "__ We will sample the number of tags instead considering all of them (due to limitation of computing power) __"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-FtgktWvU973"
   },
   "outputs": [],
   "source": [
    "def tags_to_choose(n):\n",
    "    t = multilabel_y.sum(axis=0).tolist()[0]\n",
    "    sorted_tags_i = sorted(range(len(t)), key=lambda i: t[i], reverse=True)\n",
    "    multilabel_yn=multilabel_y[:,sorted_tags_i[:n]]\n",
    "    return multilabel_yn\n",
    "\n",
    "def questions_explained_fn(n):\n",
    "    multilabel_yn = tags_to_choose(n)\n",
    "    x= multilabel_yn.sum(axis=1)\n",
    "    return (np.count_nonzero(x==0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XxbL2OyqU978"
   },
   "outputs": [],
   "source": [
    "questions_explained = []\n",
    "total_tags=multilabel_y.shape[1]\n",
    "total_qs=preprocessed_data.shape[0]\n",
    "for i in range(500, total_tags, 100):\n",
    "    questions_explained.append(np.round(((total_qs-questions_explained_fn(i))/total_qs)*100,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sbX6_QWtU98B",
    "outputId": "7add8021-d595-4e9e-c506-611520b21a9c"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(questions_explained)\n",
    "xlabel = list(500+np.array(range(-50,450,50))*50)\n",
    "ax.set_xticklabels(xlabel)\n",
    "plt.xlabel(\"Number of tags\")\n",
    "plt.ylabel(\"Number Questions coverd partially\")\n",
    "plt.grid()\n",
    "plt.show()\n",
    "# you can choose any number of tags based on your computing power, minimun is 50(it covers 90% of the tags)\n",
    "print(\"with \",5500,\"tags we are covering \",questions_explained[50],\"% of questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IgJKNRG0U98F",
    "outputId": "b620a1a2-a9c8-43be-b325-a8ff5ec6c0e0"
   },
   "outputs": [],
   "source": [
    "multilabel_yx = tags_to_choose(5500)\n",
    "print(\"number of questions that are not covered :\", questions_explained_fn(5500),\"out of \", total_qs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BTuhd5XMU98L",
    "outputId": "c60da5fb-99c9-4c8b-f417-633ef97eb598"
   },
   "outputs": [],
   "source": [
    "print(\"Number of tags in sample :\", multilabel_y.shape[1])\n",
    "print(\"number of tags taken :\", multilabel_yx.shape[1],\"(\",(multilabel_yx.shape[1]/multilabel_y.shape[1])*100,\"%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IxH1ggjxU98R"
   },
   "source": [
    "__ We consider top 15% tags which covers  99% of the questions __"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "stMfn7tMU98U"
   },
   "source": [
    "<h2>4.2 Split the data into test and train (80:20) </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YqeDZrwWU98U"
   },
   "outputs": [],
   "source": [
    "total_size=preprocessed_data.shape[0]\n",
    "train_size=int(0.80*total_size)\n",
    "\n",
    "x_train=preprocessed_data.head(train_size)\n",
    "x_test=preprocessed_data.tail(total_size - train_size)\n",
    "\n",
    "y_train = multilabel_yx[0:train_size,:]\n",
    "y_test = multilabel_yx[train_size:total_size,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TgNXo4eJU98X",
    "outputId": "21bef306-6c57-47af-e70a-e501c4225d4d"
   },
   "outputs": [],
   "source": [
    "print(\"Number of data points in train data :\", y_train.shape)\n",
    "print(\"Number of data points in test data :\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "664323eyU98a"
   },
   "source": [
    "<h2>4.3 Featurizing data </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EnV3O0WFU98b",
    "outputId": "ef87c28c-4224-451e-c762-0baa2086bd70"
   },
   "outputs": [],
   "source": [
    "start = datetime.now()\n",
    "vectorizer = TfidfVectorizer(min_df=0.00009, max_features=200000, smooth_idf=True, norm=\"l2\", \\\n",
    "                             tokenizer = lambda x: x.split(), sublinear_tf=False, ngram_range=(1,3))\n",
    "x_train_multilabel = vectorizer.fit_transform(x_train['question'])\n",
    "x_test_multilabel = vectorizer.transform(x_test['question'])\n",
    "print(\"Time taken to run this cell :\", datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CB01RkDzU98f",
    "outputId": "cbeda5ce-bbc3-4d6a-8324-53962dd50d56"
   },
   "outputs": [],
   "source": [
    "print(\"Dimensions of train data X:\",x_train_multilabel.shape, \"Y :\",y_train.shape)\n",
    "print(\"Dimensions of test data X:\",x_test_multilabel.shape,\"Y:\",y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L-JQh1bHU98j",
    "outputId": "63863f36-79ad-4726-de7c-c5d728f8902e"
   },
   "outputs": [],
   "source": [
    "# https://www.analyticsvidhya.com/blog/2017/08/introduction-to-multi-label-classification/\n",
    "#https://stats.stackexchange.com/questions/117796/scikit-multi-label-classification\n",
    "# classifier = LabelPowerset(GaussianNB())\n",
    "\"\"\"\n",
    "from skmultilearn.adapt import MLkNN\n",
    "classifier = MLkNN(k=21)\n",
    "\n",
    "# train\n",
    "classifier.fit(x_train_multilabel, y_train)\n",
    "\n",
    "# predict\n",
    "predictions = classifier.predict(x_test_multilabel)\n",
    "print(accuracy_score(y_test,predictions))\n",
    "print(metrics.f1_score(y_test, predictions, average = 'macro'))\n",
    "print(metrics.f1_score(y_test, predictions, average = 'micro'))\n",
    "print(metrics.hamming_loss(y_test,predictions))\n",
    "\n",
    "\"\"\"\n",
    "# we are getting memory error because the multilearn package \n",
    "# is trying to convert the data into dense matrix\n",
    "# ---------------------------------------------------------------------------\n",
    "#MemoryError                               Traceback (most recent call last)\n",
    "#<ipython-input-170-f0e7c7f3e0be> in <module>()\n",
    "#----> classifier.fit(x_train_multilabel, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qh-EUIsAU98l"
   },
   "source": [
    "<h2> 4.4 Applying Logistic Regression with OneVsRest Classifier </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2Cp-1EWpU98m",
    "outputId": "6ecf8c51-1828-438f-dd78-4bf262a5efaa",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# this will be taking so much time try not to run it, download the lr_with_equal_weight.pkl file and use to predict\n",
    "# This takes about 6-7 hours to run.\n",
    "classifier = OneVsRestClassifier(SGDClassifier(loss='log', alpha=0.00001, penalty='l1'), n_jobs=-1)\n",
    "classifier.fit(x_train_multilabel, y_train)\n",
    "predictions = classifier.predict(x_test_multilabel)\n",
    "\n",
    "print(\"accuracy :\",metrics.accuracy_score(y_test,predictions))\n",
    "print(\"macro f1 score :\",metrics.f1_score(y_test, predictions, average = 'macro'))\n",
    "print(\"micro f1 scoore :\",metrics.f1_score(y_test, predictions, average = 'micro'))\n",
    "print(\"hamming loss :\",metrics.hamming_loss(y_test,predictions))\n",
    "print(\"Precision recall report :\\n\",metrics.classification_report(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hCR2BtdxU98s"
   },
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "joblib.dump(classifier, 'lr_with_equal_weight.pkl') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LvjtTBZ6U98y"
   },
   "source": [
    "<h2> 4.5 Modeling with less data points (0.5M data points) and more weight to title and 500 tags only. </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n0QKMrEwU98y",
    "outputId": "3f1295d3-1aac-4cf0-f7a7-fab13cab6d6d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables in the databse:\n",
      "QuestionsProcessed\n"
     ]
    }
   ],
   "source": [
    "sql_create_table = \"\"\"CREATE TABLE IF NOT EXISTS QuestionsProcessed (question text NOT NULL, code text, tags text, words_pre integer, words_post integer, is_code integer);\"\"\"\n",
    "create_database_table(\"Titlemoreweight.db\", sql_create_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XHLunpYsU982",
    "outputId": "e79456dd-e137-4e91-8171-8379f4c0c286"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables in the databse:\n",
      "QuestionsProcessed\n",
      "Cleared All the rows\n"
     ]
    }
   ],
   "source": [
    "# http://www.sqlitetutorial.net/sqlite-delete/\n",
    "# https://stackoverflow.com/questions/2279706/select-random-row-from-a-sqlite-table\n",
    "\n",
    "read_db = 'train_no_dup.db'\n",
    "write_db = 'Titlemoreweight.db'\n",
    "train_datasize = 400000\n",
    "if os.path.isfile(read_db):\n",
    "    conn_r = create_connection(read_db)\n",
    "    if conn_r is not None:\n",
    "        reader =conn_r.cursor()\n",
    "        # for selecting first 0.5M rows\n",
    "        reader.execute(\"SELECT Title, Body, Tags From no_dup_train LIMIT 500001;\")\n",
    "        # for selecting random points\n",
    "        #reader.execute(\"SELECT Title, Body, Tags From no_dup_train ORDER BY RANDOM() LIMIT 500001;\")\n",
    "\n",
    "if os.path.isfile(write_db):\n",
    "    conn_w = create_connection(write_db)\n",
    "    if conn_w is not None:\n",
    "        tables = checkTableExists(conn_w)\n",
    "        writer =conn_w.cursor()\n",
    "        if tables != 0:\n",
    "            writer.execute(\"DELETE FROM QuestionsProcessed WHERE 1\")\n",
    "            print(\"Cleared All the rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jvi2298wU986"
   },
   "source": [
    "<h3> 4.5.1 Preprocessing of questions </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JNhcD1LYU987"
   },
   "source": [
    "<ol> \n",
    "    <li> Separate Code from Body </li>\n",
    "    <li> Remove Spcial characters from Question title and description (not in code)</li>\n",
    "    <li> <b> Give more weightage to title : Add title three times to the question </b> </li>\n",
    "   \n",
    "    <li> Remove stop words (Except 'C') </li>\n",
    "    <li> Remove HTML Tags </li>\n",
    "    <li> Convert all the characters into small letters </li>\n",
    "    <li> Use SnowballStemmer to stem the words </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ifSmL0M-U98-",
    "outputId": "78019f2f-3a06-4b6d-a761-c8cae342ac42"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of questions completed= 100000\n",
      "number of questions completed= 200000\n",
      "number of questions completed= 300000\n",
      "number of questions completed= 400000\n",
      "Avg. length of questions(Title+Body) before processing: 1239\n",
      "Avg. length of questions(Title+Body) after processing: 424\n",
      "Percent of questions containing code: 57\n",
      "Time taken to run this cell : 0:18:51.052368\n"
     ]
    }
   ],
   "source": [
    "#http://www.bernzilla.com/2008/05/13/selecting-a-random-row-from-an-sqlite-table/\n",
    "start = datetime.now()\n",
    "preprocessed_data_list=[]\n",
    "reader.fetchone()\n",
    "questions_with_code=0\n",
    "len_pre=0\n",
    "len_post=0\n",
    "questions_proccesed = 0\n",
    "for row in reader:\n",
    "    \n",
    "    is_code = 0\n",
    "    \n",
    "    title, question, tags = row[0], row[1], str(row[2])\n",
    "    \n",
    "    if '<code>' in question:\n",
    "        questions_with_code+=1\n",
    "        is_code = 1\n",
    "    x = len(question)+len(title)\n",
    "    len_pre+=x\n",
    "    \n",
    "    code = str(re.findall(r'<code>(.*?)</code>', question, flags=re.DOTALL))\n",
    "    \n",
    "    question=re.sub('<code>(.*?)</code>', '', question, flags=re.MULTILINE|re.DOTALL)\n",
    "    question=striphtml(question.encode('utf-8'))\n",
    "    \n",
    "    title=title.encode('utf-8')\n",
    "    \n",
    "    # adding title three time to the data to increase its weight\n",
    "    # add tags string to the training data\n",
    "    \n",
    "    question=str(title)+\" \"+str(title)+\" \"+str(title)+\" \"+question\n",
    "    \n",
    "#     if questions_proccesed<=train_datasize:\n",
    "#         question=str(title)+\" \"+str(title)+\" \"+str(title)+\" \"+question+\" \"+str(tags)\n",
    "#     else:\n",
    "#         question=str(title)+\" \"+str(title)+\" \"+str(title)+\" \"+question\n",
    "\n",
    "    question=re.sub(r'[^A-Za-z0-9#+.\\-]+',' ',question)\n",
    "    words=word_tokenize(str(question.lower()))\n",
    "    \n",
    "    #Removing all single letter and and stopwords from question exceptt for the letter 'c'\n",
    "    question=' '.join(str(stemmer.stem(j)) for j in words if j not in stop_words and (len(j)!=1 or j=='c'))\n",
    "    \n",
    "    len_post+=len(question)\n",
    "    tup = (question,code,tags,x,len(question),is_code)\n",
    "    questions_proccesed += 1\n",
    "    writer.execute(\"insert into QuestionsProcessed(question,code,tags,words_pre,words_post,is_code) values (?,?,?,?,?,?)\",tup)\n",
    "    if (questions_proccesed%100000==0):\n",
    "        print(\"number of questions completed=\",questions_proccesed)\n",
    "\n",
    "no_dup_avg_len_pre=(len_pre*1.0)/questions_proccesed\n",
    "no_dup_avg_len_post=(len_post*1.0)/questions_proccesed\n",
    "\n",
    "print( \"Avg. length of questions(Title+Body) before processing: %d\"%no_dup_avg_len_pre)\n",
    "print( \"Avg. length of questions(Title+Body) after processing: %d\"%no_dup_avg_len_post)\n",
    "print (\"Percent of questions containing code: %d\"%((questions_with_code*100.0)/questions_proccesed))\n",
    "\n",
    "print(\"Time taken to run this cell :\", datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x54WQvZAU99B"
   },
   "outputs": [],
   "source": [
    "# never forget to close the conections or else we will end up with database locks\n",
    "conn_r.commit()\n",
    "conn_w.commit()\n",
    "conn_r.close()\n",
    "conn_w.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gpN1ZM2bU99F"
   },
   "source": [
    "__ Sample quesitons after preprocessing of data __"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ytEecnCtU99H",
    "outputId": "f3c4992d-41e4-4e4a-9ae9-c40bb79dc43a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Questions after preprocessed\n",
      "====================================================================================================\n",
      "('java.sql.sqlexcept microsoft odbc driver manag invalid descriptor index java.sql.sqlexcept microsoft odbc driver manag invalid descriptor index java.sql.sqlexcept microsoft odbc driver manag invalid descriptor index use follow code display caus solv',)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "('better way updat feed fb php sdk better way updat feed fb php sdk better way updat feed fb php sdk novic facebook api read mani tutori still confused.i find post feed api method like correct second way use curl someth like way better',)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "('btnadd click event open two window record ad btnadd click event open two window record ad btnadd click event open two window record ad open window search.aspx use code hav add button search.aspx nwhen insert record btnadd click event open anoth window nafter insert record close window',)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "('sql inject issu prevent correct form submiss php sql inject issu prevent correct form submiss php sql inject issu prevent correct form submiss php check everyth think make sure input field safe type sql inject good news safe bad news one tag mess form submiss place even touch life figur exact html use templat file forgiv okay entir php script get execut see data post none forum field post problem use someth titl field none data get post current use print post see submit noth work flawless statement though also mention script work flawless local machin use host come across problem state list input test mess',)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "('countabl subaddit lebesgu measur countabl subaddit lebesgu measur countabl subaddit lebesgu measur let lbrace rbrace sequenc set sigma -algebra mathcal want show left bigcup right leq sum left right countabl addit measur defin set sigma algebra mathcal think use monoton properti somewher proof start appreci littl help nthank ad han answer make follow addit construct given han answer clear bigcup bigcup cap emptyset neq left bigcup right left bigcup right sum left right also construct subset monoton left right leq left right final would sum leq sum result follow',)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "('hql equival sql queri hql equival sql queri hql equival sql queri hql queri replac name class properti name error occur hql error',)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "('undefin symbol architectur i386 objc class skpsmtpmessag referenc error undefin symbol architectur i386 objc class skpsmtpmessag referenc error undefin symbol architectur i386 objc class skpsmtpmessag referenc error import framework send email applic background import framework i.e skpsmtpmessag somebodi suggest get error collect2 ld return exit status import framework correct sorc taken framework follow mfmailcomposeviewcontrol question lock field updat answer drag drop folder project click copi nthat',)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "('java.lang.nosuchmethoderror javax.servlet.servletcontext.geteffectivesessiontrackingmod ljava util set java.lang.nosuchmethoderror javax.servlet.servletcontext.geteffectivesessiontrackingmod ljava util set java.lang.nosuchmethoderror javax.servlet.servletcontext.geteffectivesessiontrackingmod ljava util set want servlet process input standalon java program deploy servlet jboss put servlet.class file web-inf class web.xml gave servlet url map .do java client program open connect servlet use url object use localhost 8080 .do get folow error error org.apache.catalina.connector.coyoteadapt except error occur contain request process java.lang.nosuchmethoderror javax.servlet.servletcontext.geteffectivesessiontrackingmod ljava util set org.apache.catalina.connector.coyoteadapter.postparserequest coyoteadapter.java 567 org.apache.catalina.connector.coyoteadapter.servic coyoteadapter.java 359 org.apache.coyote.http11.http11processor.process http11processor.java 877 org.apache.coyote.http11.http11protocol http11connectionhandler.process http11protocol.java 654 org.apache.tomcat.util.net.jioendpoint worker.run jioendpoint.java 951 web.xml file content',)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "('obtain updat locat use gps servic obtain updat locat use gps servic obtain updat locat use gps servic app two button start track stop track strart track button click gps start listen locat stop listen use besid toast everi new updat locat want thing use background servic alway updat locat even activ closed.a toast appear everi new updat location.pleas hint link would appreci',)\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "if os.path.isfile(write_db):\n",
    "    conn_r = create_connection(write_db)\n",
    "    if conn_r is not None:\n",
    "        reader =conn_r.cursor()\n",
    "        reader.execute(\"SELECT question From QuestionsProcessed LIMIT 10\")\n",
    "        print(\"Questions after preprocessed\")\n",
    "        print('='*100)\n",
    "        reader.fetchone()\n",
    "        for row in reader:\n",
    "            print(row)\n",
    "            print('-'*100)\n",
    "conn_r.commit()\n",
    "conn_r.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IspyyegoU99N"
   },
   "source": [
    "__ Saving Preprocessed data to a Database __"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F_x-ETQJU99P"
   },
   "outputs": [],
   "source": [
    "#Taking 0.5 Million entries to a dataframe.\n",
    "write_db = 'Titlemoreweight.db'\n",
    "if os.path.isfile(write_db):\n",
    "    conn_r = create_connection(write_db)\n",
    "    if conn_r is not None:\n",
    "        preprocessed_data = pd.read_sql_query(\"\"\"SELECT question, Tags FROM QuestionsProcessed\"\"\", conn_r)\n",
    "conn_r.commit()\n",
    "conn_r.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bc7hwHjBU99U",
    "outputId": "7a90802a-0630-44da-853d-987e6ece8c4b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>java.lang.noclassdeffounderror javax servlet j...</td>\n",
       "      <td>jsp jstl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>java.sql.sqlexcept microsoft odbc driver manag...</td>\n",
       "      <td>java jdbc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>better way updat feed fb php sdk better way up...</td>\n",
       "      <td>facebook api facebook-php-sdk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>btnadd click event open two window record ad b...</td>\n",
       "      <td>javascript asp.net web</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sql inject issu prevent correct form submiss p...</td>\n",
       "      <td>php forms</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  java.lang.noclassdeffounderror javax servlet j...   \n",
       "1  java.sql.sqlexcept microsoft odbc driver manag...   \n",
       "2  better way updat feed fb php sdk better way up...   \n",
       "3  btnadd click event open two window record ad b...   \n",
       "4  sql inject issu prevent correct form submiss p...   \n",
       "\n",
       "                            tags  \n",
       "0                       jsp jstl  \n",
       "1                      java jdbc  \n",
       "2  facebook api facebook-php-sdk  \n",
       "3         javascript asp.net web  \n",
       "4                      php forms  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xk9V0azqU99X",
    "outputId": "c767de50-461e-4454-c634-5487aa543b82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of data points in sample : 499998\n",
      "number of dimensions : 2\n"
     ]
    }
   ],
   "source": [
    "print(\"number of data points in sample :\", preprocessed_data.shape[0])\n",
    "print(\"number of dimensions :\", preprocessed_data.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oUpccCSkU99Z"
   },
   "source": [
    "__ Converting string Tags to multilable output variables __ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SWg_g1lNU99a"
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(tokenizer = lambda x: x.split(), binary='true')\n",
    "multilabel_y = vectorizer.fit_transform(preprocessed_data['tags'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pbtD0Hx8U99c"
   },
   "source": [
    "__ Selecting 500 Tags __"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h_nMDxAIU99d"
   },
   "outputs": [],
   "source": [
    "questions_explained = []\n",
    "total_tags=multilabel_y.shape[1]\n",
    "total_qs=preprocessed_data.shape[0]\n",
    "for i in range(500, total_tags, 100):\n",
    "    questions_explained.append(np.round(((total_qs-questions_explained_fn(i))/total_qs)*100,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fggMk2IJU99f",
    "outputId": "7c443492-b0c4-492d-afc2-16c59b8b954e"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with  5500 tags we are covering  99.157 % of questions\n",
      "with  500 tags we are covering  90.956 % of questions\n"
     ]
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(questions_explained)\n",
    "xlabel = list(500+np.array(range(-50,450,50))*50)\n",
    "ax.set_xticklabels(xlabel)\n",
    "plt.xlabel(\"Number of tags\")\n",
    "plt.ylabel(\"Number Questions coverd partially\")\n",
    "plt.grid()\n",
    "plt.show()\n",
    "# you can choose any number of tags based on your computing power, minimun is 500(it covers 90% of the tags)\n",
    "print(\"with \",5500,\"tags we are covering \",questions_explained[50],\"% of questions\")\n",
    "print(\"with \",500,\"tags we are covering \",questions_explained[0],\"% of questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VuJzfmNrU99i",
    "outputId": "2c34ec8f-ee38-451d-f477-89f880a98b7d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of questions that are not covered : 45221 out of  499998\n"
     ]
    }
   ],
   "source": [
    "# we will be taking 500 tags\n",
    "multilabel_yx = tags_to_choose(500)\n",
    "print(\"number of questions that are not covered :\", questions_explained_fn(500),\"out of \", total_qs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WsduwXTeU99k"
   },
   "outputs": [],
   "source": [
    "x_train=preprocessed_data.head(train_datasize)\n",
    "x_test=preprocessed_data.tail(preprocessed_data.shape[0] - 400000)\n",
    "\n",
    "y_train = multilabel_yx[0:train_datasize,:]\n",
    "y_test = multilabel_yx[train_datasize:preprocessed_data.shape[0],:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iZZDSH_VU99m",
    "outputId": "15b74fe3-29ac-40d2-c393-82260f78ce19"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data points in train data : (400000, 500)\n",
      "Number of data points in test data : (99998, 500)\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of data points in train data :\", y_train.shape)\n",
    "print(\"Number of data points in test data :\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gDJ2PvnzU99o"
   },
   "source": [
    "<h3> 4.5.2 Featurizing data with TfIdf vectorizer </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "530e8tW9U99o",
    "outputId": "a73463fb-694d-44bf-dc16-97c9b161f0a0"
   },
   "outputs": [],
   "source": [
    "start = datetime.now()\n",
    "vectorizer = TfidfVectorizer(min_df=0.00009, max_features=200000, smooth_idf=True, norm=\"l2\", \\\n",
    "                             tokenizer = lambda x: x.split(), sublinear_tf=False, ngram_range=(1,3))\n",
    "x_train_multilabel = vectorizer.fit_transform(x_train['question'])\n",
    "x_test_multilabel = vectorizer.transform(x_test['question'])\n",
    "print(\"Time taken to run this cell :\", datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to run this cell : 0:01:42.011543\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "start = datetime.now()\n",
    "vectorizer = CountVectorizer(min_df=0.00009, max_features=20000,tokenizer = lambda x: x.split(),ngram_range=(1,2))\n",
    "x_train_multilabel = vectorizer.fit_transform(x_train['question'])\n",
    "x_test_multilabel = vectorizer.transform(x_test['question'])\n",
    "print(\"Time taken to run this cell :\", datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r9iDfzXIU99t",
    "outputId": "23c4dfde-72c4-40c3-9339-29e56fc18b59"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of train data X: (400000, 20000) Y : (400000, 500)\n",
      "Dimensions of test data X: (99998, 20000) Y: (99998, 500)\n"
     ]
    }
   ],
   "source": [
    "print(\"Dimensions of train data X:\",x_train_multilabel.shape, \"Y :\",y_train.shape)\n",
    "print(\"Dimensions of test data X:\",x_test_multilabel.shape,\"Y:\",y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_multilabel=x_train_multilabel[0:160000]\n",
    "y_train=y_train[0:160000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_multilabel=x_test_multilabel[0:40000]\n",
    "y_test=y_test[0:40000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_multilabel=pd.DataFrame.sparse.from_spmatrix(x_train_multilabel)\n",
    "y_train=pd.DataFrame.sparse.from_spmatrix(y_train)\n",
    "\n",
    "x_test_multilabel=pd.DataFrame.sparse.from_spmatrix(x_test_multilabel)\n",
    "y_test=pd.DataFrame.sparse.from_spmatrix(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of train data X: (160000, 20000) Y : (160000, 500)\n",
      "Dimensions of test data X: (40000, 20000) Y: (40000, 500)\n"
     ]
    }
   ],
   "source": [
    "print(\"Dimensions of train data X:\",x_train_multilabel.shape, \"Y :\",y_train.shape)\n",
    "print(\"Dimensions of test data X:\",x_test_multilabel.shape,\"Y:\",y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "print(type(x_train_multilabel))\n",
    "print(type(y_train))\n",
    "print(type(x_test_multilabel))\n",
    "print(type(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Kmmnoy4XU99v"
   },
   "source": [
    "<h3> 4.5.3 Applying Logistic Regression with OneVsRest Classifier </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GnHoxl5DU99w",
    "outputId": "fea313da-ed92-469d-d34b-149d3fc5e01e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.069875\n",
      "Hamming loss  0.00880855\n",
      "Micro-average quality numbers\n",
      "Precision: 0.1881, Recall: 0.4859, F1-measure: 0.2712\n",
      "Macro-average quality numbers\n",
      "Precision: 0.1347, Recall: 0.4231, F1-measure: 0.1956\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.80      0.73      1936\n",
      "           1       0.37      0.40      0.38      3047\n",
      "           2       0.47      0.52      0.49      2805\n",
      "           3       0.49      0.66      0.56      1644\n",
      "           4       0.51      0.57      0.53      2441\n",
      "           5       0.35      0.46      0.39      1171\n",
      "           6       0.51      0.62      0.56      1942\n",
      "           7       0.46      0.64      0.53      1343\n",
      "           8       0.23      0.28      0.25      1427\n",
      "           9       0.42      0.64      0.51       587\n",
      "          10       0.24      0.33      0.28       888\n",
      "          11       0.40      0.54      0.46      1197\n",
      "          12       0.31      0.42      0.36       964\n",
      "          13       0.26      0.46      0.33       696\n",
      "          14       0.52      0.68      0.59      1102\n",
      "          15       0.28      0.41      0.33       832\n",
      "          16       0.18      0.36      0.24       393\n",
      "          17       0.43      0.70      0.53       790\n",
      "          18       0.23      0.41      0.30       836\n",
      "          19       0.17      0.45      0.24       337\n",
      "          20       0.31      0.53      0.39       522\n",
      "          21       0.15      0.24      0.19       865\n",
      "          22       0.22      0.39      0.28       614\n",
      "          23       0.39      0.70      0.50       313\n",
      "          24       0.17      0.36      0.23       194\n",
      "          25       0.14      0.51      0.23       195\n",
      "          26       0.25      0.50      0.33       344\n",
      "          27       0.29      0.50      0.37       525\n",
      "          28       0.32      0.57      0.41       432\n",
      "          29       0.10      0.34      0.15       151\n",
      "          30       0.12      0.32      0.18       201\n",
      "          31       0.10      0.23      0.14       307\n",
      "          32       0.16      0.47      0.24       130\n",
      "          33       0.26      0.49      0.34       634\n",
      "          34       0.17      0.38      0.23       320\n",
      "          35       0.33      0.67      0.44       297\n",
      "          36       0.14      0.43      0.21       195\n",
      "          37       0.32      0.72      0.44        69\n",
      "          38       0.27      0.57      0.37       332\n",
      "          39       0.19      0.49      0.27       276\n",
      "          40       0.23      0.51      0.31       421\n",
      "          41       0.04      0.27      0.07        77\n",
      "          42       0.20      0.70      0.31        73\n",
      "          43       0.28      0.61      0.38       178\n",
      "          44       0.27      0.57      0.37       367\n",
      "          45       0.20      0.51      0.29       319\n",
      "          46       0.24      0.58      0.34       261\n",
      "          47       0.09      0.35      0.15       121\n",
      "          48       0.16      0.48      0.24       172\n",
      "          49       0.42      0.76      0.54       315\n",
      "          50       0.07      0.23      0.11       122\n",
      "          51       0.06      0.26      0.10       187\n",
      "          52       0.06      0.19      0.09        63\n",
      "          53       0.21      0.46      0.29       271\n",
      "          54       0.15      0.34      0.21       334\n",
      "          55       0.03      0.16      0.04        58\n",
      "          56       0.15      0.39      0.22       313\n",
      "          57       0.12      0.29      0.17       325\n",
      "          58       0.22      0.47      0.30       270\n",
      "          59       0.43      0.82      0.56       267\n",
      "          60       0.07      0.22      0.11       158\n",
      "          61       0.13      0.51      0.21        67\n",
      "          62       0.46      0.85      0.60       396\n",
      "          63       0.34      0.73      0.46       158\n",
      "          64       0.10      0.43      0.16        46\n",
      "          65       0.06      0.25      0.09       121\n",
      "          66       0.11      0.30      0.16       221\n",
      "          67       0.21      0.67      0.32       105\n",
      "          68       0.33      0.66      0.44       222\n",
      "          69       0.17      0.62      0.27       129\n",
      "          70       0.08      0.32      0.13       135\n",
      "          71       0.11      0.38      0.18       125\n",
      "          72       0.24      0.62      0.34       172\n",
      "          73       0.14      0.88      0.25        26\n",
      "          74       0.10      0.55      0.16        42\n",
      "          75       0.40      0.82      0.54        50\n",
      "          76       0.06      0.17      0.09       158\n",
      "          77       0.08      0.45      0.13        74\n",
      "          78       0.17      0.55      0.26       147\n",
      "          79       0.14      0.37      0.21       187\n",
      "          80       0.12      0.45      0.19       128\n",
      "          81       0.08      0.26      0.13       173\n",
      "          82       0.06      0.22      0.09       103\n",
      "          83       0.20      0.58      0.30       168\n",
      "          84       0.13      0.45      0.20        76\n",
      "          85       0.15      0.35      0.21       212\n",
      "          86       0.09      0.27      0.13       184\n",
      "          87       0.08      0.38      0.13        74\n",
      "          88       0.07      0.50      0.13        38\n",
      "          89       0.06      0.27      0.10        59\n",
      "          90       0.19      0.39      0.26       194\n",
      "          91       0.13      0.48      0.20       164\n",
      "          92       0.26      0.66      0.37       194\n",
      "          93       0.17      0.64      0.27       120\n",
      "          94       0.04      0.16      0.06        49\n",
      "          95       0.08      0.33      0.13        86\n",
      "          96       0.13      0.29      0.18       277\n",
      "          97       0.18      0.53      0.27       116\n",
      "          98       0.20      0.73      0.32        59\n",
      "          99       0.10      0.36      0.16        95\n",
      "         100       0.09      0.26      0.14       178\n",
      "         101       0.46      0.76      0.58       136\n",
      "         102       0.56      0.81      0.67       295\n",
      "         103       0.07      0.29      0.11        70\n",
      "         104       0.05      0.17      0.08       113\n",
      "         105       0.22      0.59      0.32       145\n",
      "         106       0.09      0.29      0.14       118\n",
      "         107       0.24      0.64      0.35       146\n",
      "         108       0.18      0.47      0.26       152\n",
      "         109       0.16      0.55      0.25       114\n",
      "         110       0.04      0.16      0.07        91\n",
      "         111       0.04      0.31      0.07        54\n",
      "         112       0.11      0.57      0.18        60\n",
      "         113       0.10      0.43      0.16        72\n",
      "         114       0.06      0.30      0.10        82\n",
      "         115       0.14      0.47      0.22       159\n",
      "         116       0.23      0.60      0.33       154\n",
      "         117       0.13      0.37      0.20       142\n",
      "         118       0.08      0.38      0.13        50\n",
      "         119       0.15      0.45      0.23       168\n",
      "         120       0.03      0.13      0.05        92\n",
      "         121       0.30      0.68      0.41       229\n",
      "         122       0.06      0.28      0.10        60\n",
      "         123       0.02      0.08      0.03        95\n",
      "         124       0.36      0.79      0.50        96\n",
      "         125       0.07      0.38      0.11        56\n",
      "         126       0.25      0.68      0.36       157\n",
      "         127       0.02      0.20      0.04        50\n",
      "         128       0.06      0.25      0.09        95\n",
      "         129       0.13      0.36      0.19        42\n",
      "         130       0.07      0.43      0.12        44\n",
      "         131       0.05      0.21      0.08        76\n",
      "         132       0.15      0.45      0.23       137\n",
      "         133       0.41      0.76      0.54       232\n",
      "         134       0.31      0.84      0.46        49\n",
      "         135       0.05      0.28      0.08        40\n",
      "         136       0.05      0.46      0.10        59\n",
      "         137       0.16      0.40      0.22       126\n",
      "         138       0.07      0.28      0.11       116\n",
      "         139       0.11      0.40      0.17        73\n",
      "         140       0.22      0.63      0.33       114\n",
      "         141       0.06      0.21      0.09        84\n",
      "         142       0.19      0.56      0.28       119\n",
      "         143       0.58      0.85      0.69       194\n",
      "         144       0.08      0.47      0.14        66\n",
      "         145       0.15      0.55      0.24       100\n",
      "         146       0.17      0.47      0.25       167\n",
      "         147       0.09      0.47      0.15        36\n",
      "         148       0.07      0.28      0.11        78\n",
      "         149       0.27      0.65      0.38       187\n",
      "         150       0.07      0.28      0.11       156\n",
      "         151       0.09      0.78      0.15        18\n",
      "         152       0.31      0.71      0.43       150\n",
      "         153       0.09      0.42      0.15        38\n",
      "         154       0.09      0.30      0.14       118\n",
      "         155       0.05      0.19      0.08        95\n",
      "         156       0.24      0.82      0.37        66\n",
      "         157       0.09      0.27      0.14        86\n",
      "         158       0.18      0.67      0.29        33\n",
      "         159       0.06      0.35      0.10        49\n",
      "         160       0.06      0.28      0.11        83\n",
      "         161       0.17      0.66      0.27        76\n",
      "         162       0.05      0.23      0.09        91\n",
      "         163       0.18      0.49      0.26       121\n",
      "         164       0.16      0.51      0.24        95\n",
      "         165       0.14      0.44      0.22       122\n",
      "         166       0.12      0.55      0.19        53\n",
      "         167       0.11      0.42      0.17        97\n",
      "         168       0.13      0.41      0.19       135\n",
      "         169       0.12      0.33      0.18       105\n",
      "         170       0.02      0.17      0.03        29\n",
      "         171       0.12      0.49      0.19        43\n",
      "         172       0.09      0.51      0.16        83\n",
      "         173       0.16      0.45      0.23       130\n",
      "         174       0.05      0.26      0.09        66\n",
      "         175       0.39      0.93      0.55        56\n",
      "         176       0.39      0.77      0.52       159\n",
      "         177       0.26      0.67      0.38       130\n",
      "         178       0.20      0.67      0.31       115\n",
      "         179       0.03      0.23      0.06        60\n",
      "         180       0.08      0.61      0.13        51\n",
      "         181       0.03      0.28      0.05        46\n",
      "         182       0.04      0.24      0.07        51\n",
      "         183       0.18      0.54      0.27       165\n",
      "         184       0.05      0.23      0.09        97\n",
      "         185       0.03      0.19      0.06        32\n",
      "         186       0.07      0.27      0.11        79\n",
      "         187       0.20      0.72      0.32        87\n",
      "         188       0.08      0.34      0.12        80\n",
      "         189       0.16      0.57      0.25        51\n",
      "         190       0.23      0.65      0.34        80\n",
      "         191       0.05      0.27      0.08        83\n",
      "         192       0.18      0.61      0.27       103\n",
      "         193       0.05      0.40      0.09        20\n",
      "         194       0.19      0.62      0.29        89\n",
      "         195       0.04      0.17      0.06        77\n",
      "         196       0.03      0.16      0.06        91\n",
      "         197       0.12      0.31      0.17       112\n",
      "         198       0.11      0.29      0.16       118\n",
      "         199       0.04      0.25      0.07        53\n",
      "         200       0.34      0.79      0.48        86\n",
      "         201       0.13      0.43      0.20       129\n",
      "         202       0.35      0.67      0.46       158\n",
      "         203       0.09      0.24      0.13       111\n",
      "         204       0.09      0.32      0.14        93\n",
      "         205       0.04      0.26      0.08        34\n",
      "         206       0.01      0.04      0.01        50\n",
      "         207       0.10      0.45      0.17        31\n",
      "         208       0.22      0.63      0.32       138\n",
      "         209       0.09      0.30      0.14       106\n",
      "         210       0.15      0.44      0.22       126\n",
      "         211       0.06      0.41      0.10        51\n",
      "         212       0.05      0.28      0.08        32\n",
      "         213       0.05      0.43      0.09        14\n",
      "         214       0.10      0.60      0.17        40\n",
      "         215       0.10      0.51      0.16        57\n",
      "         216       0.20      0.42      0.27       186\n",
      "         217       0.04      0.35      0.07        20\n",
      "         218       0.43      0.89      0.58       100\n",
      "         219       0.19      0.51      0.27        96\n",
      "         220       0.35      0.75      0.48       109\n",
      "         221       0.36      0.79      0.50       129\n",
      "         222       0.12      0.55      0.20        64\n",
      "         223       0.03      0.14      0.05        49\n",
      "         224       0.17      0.59      0.26        85\n",
      "         225       0.10      0.59      0.18        69\n",
      "         226       0.11      0.34      0.17        82\n",
      "         227       0.17      0.51      0.25        69\n",
      "         228       0.04      0.19      0.07        97\n",
      "         229       0.03      0.20      0.06        60\n",
      "         230       0.04      0.25      0.07        77\n",
      "         231       0.10      0.50      0.16        22\n",
      "         232       0.05      0.27      0.09        59\n",
      "         233       0.13      0.56      0.22        77\n",
      "         234       0.09      0.32      0.14        37\n",
      "         235       0.12      0.47      0.19        30\n",
      "         236       0.27      0.77      0.40        39\n",
      "         237       0.07      0.26      0.10       101\n",
      "         238       0.03      0.20      0.05        50\n",
      "         239       0.01      0.05      0.01        21\n",
      "         240       0.08      0.30      0.13        33\n",
      "         241       0.05      0.25      0.08        88\n",
      "         242       0.07      0.42      0.12        36\n",
      "         243       0.03      0.31      0.06        29\n",
      "         244       0.22      0.49      0.30       112\n",
      "         245       0.06      0.30      0.10        44\n",
      "         246       0.37      0.79      0.50        62\n",
      "         247       0.04      0.31      0.07        26\n",
      "         248       0.02      0.12      0.04        51\n",
      "         249       0.02      0.13      0.03        31\n",
      "         250       0.04      0.35      0.08        23\n",
      "         251       0.12      0.47      0.19        73\n",
      "         252       0.03      0.13      0.05        53\n",
      "         253       0.10      0.43      0.16        44\n",
      "         254       0.05      0.26      0.09        50\n",
      "         255       0.07      0.25      0.11       158\n",
      "         256       0.06      0.24      0.09        83\n",
      "         257       0.07      0.45      0.12        40\n",
      "         258       0.29      0.54      0.38        98\n",
      "         259       0.10      0.40      0.16        63\n",
      "         260       0.14      0.41      0.21        95\n",
      "         261       0.10      0.35      0.16        85\n",
      "         262       0.07      0.38      0.11        42\n",
      "         263       0.22      0.41      0.29       162\n",
      "         264       0.05      0.39      0.09        41\n",
      "         265       0.07      0.41      0.12        34\n",
      "         266       0.04      0.33      0.07        12\n",
      "         267       0.03      0.29      0.06        21\n",
      "         268       0.19      0.59      0.28        92\n",
      "         269       0.05      0.38      0.08        21\n",
      "         270       0.15      0.63      0.24        62\n",
      "         271       0.16      0.61      0.26        77\n",
      "         272       0.10      0.37      0.16        89\n",
      "         273       0.45      0.85      0.59       119\n",
      "         274       0.23      0.84      0.36        37\n",
      "         275       0.04      0.23      0.07        40\n",
      "         276       0.02      0.12      0.03        51\n",
      "         277       0.21      0.63      0.31        54\n",
      "         278       0.16      0.68      0.26        57\n",
      "         279       0.37      0.80      0.50        89\n",
      "         280       0.08      0.38      0.13        29\n",
      "         281       0.27      0.64      0.38        81\n",
      "         282       0.22      0.66      0.32        56\n",
      "         283       0.10      0.33      0.16       137\n",
      "         284       0.13      0.52      0.20        29\n",
      "         285       0.18      0.65      0.28       103\n",
      "         286       0.30      0.77      0.43       100\n",
      "         287       0.13      0.57      0.22        51\n",
      "         288       0.02      0.14      0.04        43\n",
      "         289       0.03      0.28      0.05        32\n",
      "         290       0.03      0.24      0.06        50\n",
      "         291       0.10      0.43      0.16        58\n",
      "         292       0.06      0.36      0.11        33\n",
      "         293       0.13      0.62      0.21        50\n",
      "         294       0.08      0.50      0.14        68\n",
      "         295       0.05      0.24      0.08        37\n",
      "         296       0.01      0.17      0.03        12\n",
      "         297       0.04      0.26      0.07        47\n",
      "         298       0.08      0.53      0.14        17\n",
      "         299       0.17      0.73      0.27        22\n",
      "         300       0.04      0.25      0.08        60\n",
      "         301       0.01      0.08      0.02        52\n",
      "         302       0.00      0.05      0.01        21\n",
      "         303       0.21      0.61      0.31        90\n",
      "         304       0.32      0.83      0.46        88\n",
      "         305       0.37      0.81      0.51        64\n",
      "         306       0.24      0.51      0.33       101\n",
      "         307       0.04      0.18      0.07        45\n",
      "         308       0.00      0.03      0.01        40\n",
      "         309       0.04      0.22      0.06        41\n",
      "         310       0.13      0.50      0.21        68\n",
      "         311       0.08      0.45      0.13        20\n",
      "         312       0.03      0.20      0.06        20\n",
      "         313       0.03      0.27      0.05        30\n",
      "         314       0.14      0.48      0.21        81\n",
      "         315       0.19      0.64      0.29        59\n",
      "         316       0.06      0.33      0.10        24\n",
      "         317       0.06      0.24      0.09        78\n",
      "         318       0.16      0.48      0.24        60\n",
      "         319       0.17      0.57      0.26        46\n",
      "         320       0.07      0.41      0.12        61\n",
      "         321       0.17      0.60      0.26        48\n",
      "         322       0.14      0.55      0.22        58\n",
      "         323       0.25      0.55      0.35       155\n",
      "         324       0.04      0.30      0.08        37\n",
      "         325       0.03      0.24      0.06        33\n",
      "         326       0.04      0.29      0.07        52\n",
      "         327       0.03      0.14      0.04        59\n",
      "         328       0.28      0.79      0.41        71\n",
      "         329       0.13      0.48      0.20        66\n",
      "         330       0.08      0.30      0.13        50\n",
      "         331       0.03      0.28      0.06        32\n",
      "         332       0.01      0.20      0.02        10\n",
      "         333       0.04      0.14      0.06        81\n",
      "         334       0.04      0.23      0.07        30\n",
      "         335       0.03      0.14      0.05        36\n",
      "         336       0.11      0.62      0.19        21\n",
      "         337       0.02      0.23      0.03        26\n",
      "         338       0.17      0.55      0.26        53\n",
      "         339       0.01      0.10      0.02        29\n",
      "         340       0.23      0.62      0.33        55\n",
      "         341       0.06      0.33      0.10        12\n",
      "         342       0.06      0.37      0.10        19\n",
      "         343       0.08      0.50      0.14        20\n",
      "         344       0.03      0.14      0.05        21\n",
      "         345       0.12      0.71      0.21        21\n",
      "         346       0.01      0.20      0.03        25\n",
      "         347       0.12      0.45      0.19        47\n",
      "         348       0.01      0.08      0.01        25\n",
      "         349       0.18      0.79      0.30        34\n",
      "         350       0.04      0.50      0.08         8\n",
      "         351       0.05      0.50      0.09        10\n",
      "         352       0.22      0.56      0.31        61\n",
      "         353       0.03      0.30      0.06        10\n",
      "         354       0.04      0.22      0.07        58\n",
      "         355       0.09      0.54      0.16        59\n",
      "         356       0.23      0.50      0.32       145\n",
      "         357       0.24      0.57      0.33       135\n",
      "         358       0.09      0.36      0.14        36\n",
      "         359       0.26      0.67      0.38        67\n",
      "         360       0.07      0.34      0.11        62\n",
      "         361       0.42      0.86      0.57        76\n",
      "         362       0.01      0.12      0.02        24\n",
      "         363       0.33      0.81      0.46        64\n",
      "         364       0.02      0.17      0.04        18\n",
      "         365       0.04      0.30      0.07        27\n",
      "         366       0.28      0.85      0.42        27\n",
      "         367       0.06      0.26      0.10        68\n",
      "         368       0.01      0.08      0.01        13\n",
      "         369       0.02      0.25      0.04        24\n",
      "         370       0.01      0.09      0.01        22\n",
      "         371       0.03      0.19      0.05        36\n",
      "         372       0.22      0.43      0.29        30\n",
      "         373       0.19      0.64      0.29        74\n",
      "         374       0.06      0.40      0.10        30\n",
      "         375       0.02      0.15      0.03        20\n",
      "         376       0.00      0.00      0.00        15\n",
      "         377       0.19      0.42      0.26        79\n",
      "         378       0.01      0.09      0.02        35\n",
      "         379       0.00      0.00      0.00         7\n",
      "         380       0.12      0.51      0.19        57\n",
      "         381       0.04      0.20      0.06        59\n",
      "         382       0.03      0.20      0.05        41\n",
      "         383       0.13      0.48      0.21        69\n",
      "         384       0.03      0.21      0.06        42\n",
      "         385       0.08      0.33      0.12        21\n",
      "         386       0.02      0.12      0.03        24\n",
      "         387       0.05      0.29      0.09        34\n",
      "         388       0.06      0.38      0.10        39\n",
      "         389       0.02      0.13      0.03        45\n",
      "         390       0.35      0.68      0.46        75\n",
      "         391       0.06      0.34      0.10        32\n",
      "         392       0.07      0.62      0.12         8\n",
      "         393       0.01      0.07      0.02        59\n",
      "         394       0.04      0.21      0.07        38\n",
      "         395       0.09      0.39      0.15        61\n",
      "         396       0.07      0.34      0.12        65\n",
      "         397       0.27      0.47      0.34       190\n",
      "         398       0.16      0.83      0.27        36\n",
      "         399       0.02      0.22      0.04        23\n",
      "         400       0.06      0.33      0.10        27\n",
      "         401       0.07      0.44      0.11        36\n",
      "         402       0.11      0.42      0.17        52\n",
      "         403       0.03      0.45      0.06        11\n",
      "         404       0.15      0.83      0.26         6\n",
      "         405       0.07      0.33      0.11        49\n",
      "         406       0.06      0.68      0.11        19\n",
      "         407       0.06      0.27      0.10        11\n",
      "         408       0.11      0.52      0.18        46\n",
      "         409       0.05      0.25      0.08        16\n",
      "         410       0.00      0.00      0.00         5\n",
      "         411       0.08      0.29      0.13        68\n",
      "         412       0.07      0.31      0.12        32\n",
      "         413       0.05      0.18      0.08        61\n",
      "         414       0.02      0.19      0.03        21\n",
      "         415       0.07      0.44      0.12        18\n",
      "         416       0.08      0.64      0.15        25\n",
      "         417       0.01      0.12      0.02        16\n",
      "         418       0.04      0.27      0.06        26\n",
      "         419       0.12      0.50      0.19        50\n",
      "         420       0.01      0.14      0.02         7\n",
      "         421       0.05      0.32      0.08        28\n",
      "         422       0.07      0.41      0.12        32\n",
      "         423       0.05      0.36      0.08        28\n",
      "         424       0.22      0.89      0.35         9\n",
      "         425       0.00      0.00      0.00         6\n",
      "         426       0.02      0.18      0.04        33\n",
      "         427       0.37      0.80      0.50        64\n",
      "         428       0.21      0.86      0.34        14\n",
      "         429       0.42      0.73      0.53        11\n",
      "         430       0.12      0.43      0.19        61\n",
      "         431       0.05      0.24      0.08        49\n",
      "         432       0.04      0.43      0.08         7\n",
      "         433       0.04      0.36      0.08        11\n",
      "         434       0.24      0.67      0.36        15\n",
      "         435       0.07      0.50      0.13        16\n",
      "         436       0.13      0.45      0.20        42\n",
      "         437       0.02      0.10      0.04        49\n",
      "         438       0.35      0.78      0.48        49\n",
      "         439       0.08      0.27      0.12        55\n",
      "         440       0.05      0.22      0.08        45\n",
      "         441       0.03      0.21      0.05        24\n",
      "         442       0.04      0.32      0.07        31\n",
      "         443       0.06      0.24      0.10        25\n",
      "         444       0.04      0.28      0.07        25\n",
      "         445       0.07      0.41      0.13        22\n",
      "         446       0.05      0.31      0.08        16\n",
      "         447       0.01      0.33      0.03         9\n",
      "         448       0.26      0.53      0.35        43\n",
      "         449       0.12      0.43      0.19        44\n",
      "         450       0.09      0.46      0.16        37\n",
      "         451       0.08      0.37      0.13        30\n",
      "         452       0.02      0.23      0.04        13\n",
      "         453       0.01      0.12      0.03        25\n",
      "         454       0.17      0.67      0.27        39\n",
      "         455       0.13      0.47      0.21        98\n",
      "         456       0.02      1.00      0.05         2\n",
      "         457       0.16      0.54      0.24        54\n",
      "         458       0.02      0.30      0.04        10\n",
      "         459       0.03      0.24      0.05        21\n",
      "         460       0.02      0.14      0.03        29\n",
      "         461       0.12      0.50      0.20        28\n",
      "         462       0.02      0.11      0.04        27\n",
      "         463       0.06      0.40      0.11        40\n",
      "         464       0.14      0.37      0.21        49\n",
      "         465       0.06      0.44      0.10        32\n",
      "         466       0.16      0.38      0.22        90\n",
      "         467       0.17      0.67      0.27        49\n",
      "         468       0.10      0.41      0.16        59\n",
      "         469       0.03      0.14      0.05        35\n",
      "         470       0.27      0.83      0.41        42\n",
      "         471       0.10      0.45      0.17        42\n",
      "         472       0.17      0.36      0.23        64\n",
      "         473       0.08      0.41      0.14        22\n",
      "         474       0.05      0.24      0.08        37\n",
      "         475       0.02      0.13      0.03        30\n",
      "         476       0.20      0.66      0.31        29\n",
      "         477       0.30      0.66      0.41        67\n",
      "         478       0.20      0.94      0.33        16\n",
      "         479       0.03      0.16      0.05        37\n",
      "         480       0.19      0.70      0.29        46\n",
      "         481       0.17      0.43      0.24        56\n",
      "         482       0.04      0.25      0.07        28\n",
      "         483       0.34      0.76      0.47        51\n",
      "         484       0.07      0.28      0.11        32\n",
      "         485       0.02      0.11      0.03        37\n",
      "         486       0.07      0.32      0.12        41\n",
      "         487       0.09      0.46      0.14        24\n",
      "         488       0.01      0.08      0.02        26\n",
      "         489       0.10      0.39      0.16        36\n",
      "         490       0.40      0.83      0.54        29\n",
      "         491       0.03      0.24      0.06        37\n",
      "         492       0.00      0.00      0.00        16\n",
      "         493       0.02      0.12      0.04        41\n",
      "         494       0.14      0.35      0.20        26\n",
      "         495       0.15      0.67      0.25        21\n",
      "         496       0.03      0.17      0.05        12\n",
      "         497       0.12      0.46      0.18        39\n",
      "         498       0.05      0.29      0.08        31\n",
      "         499       0.05      0.31      0.09        45\n",
      "\n",
      "   micro avg       0.19      0.49      0.27     67448\n",
      "   macro avg       0.13      0.42      0.20     67448\n",
      "weighted avg       0.27      0.49      0.33     67448\n",
      " samples avg       0.29      0.45      0.30     67448\n",
      "\n",
      "Time taken to run this cell : 1:10:54.946941\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "classifier = OneVsRestClassifier(SGDClassifier(loss='log', alpha=0.00001, penalty='l1'))\n",
    "classifier.fit(x_train_multilabel, y_train)\n",
    "predictions = classifier.predict (x_test_multilabel)\n",
    "\n",
    "\n",
    "print(\"Accuracy :\",metrics.accuracy_score(y_test, predictions))\n",
    "print(\"Hamming loss \",metrics.hamming_loss(y_test,predictions))\n",
    "\n",
    "\n",
    "precision = precision_score(y_test, predictions, average='micro')\n",
    "recall = recall_score(y_test, predictions, average='micro')\n",
    "f1 = f1_score(y_test, predictions, average='micro')\n",
    " \n",
    "print(\"Micro-average quality numbers\")\n",
    "print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
    "\n",
    "precision = precision_score(y_test, predictions, average='macro')\n",
    "recall = recall_score(y_test, predictions, average='macro')\n",
    "f1 = f1_score(y_test, predictions, average='macro')\n",
    " \n",
    "print(\"Macro-average quality numbers\")\n",
    "print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
    "\n",
    "print (metrics.classification_report(y_test, predictions))\n",
    "print(\"Time taken to run this cell :\", datetime.now() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>SGD with hinge loss</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kWQKiSJxU994",
    "outputId": "7728dac3-cda2-495c-8a6f-dd08cdc1f5fc",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.069\n",
      "Hamming loss  0.00882585\n",
      "Micro-average quality numbers\n",
      "Precision: 0.1871, Recall: 0.4836, F1-measure: 0.2699\n",
      "Macro-average quality numbers\n",
      "Precision: 0.1325, Recall: 0.4194, F1-measure: 0.1928\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.79      0.71      1936\n",
      "           1       0.36      0.39      0.37      3047\n",
      "           2       0.47      0.51      0.49      2805\n",
      "           3       0.45      0.66      0.53      1644\n",
      "           4       0.51      0.55      0.53      2441\n",
      "           5       0.34      0.47      0.39      1171\n",
      "           6       0.50      0.62      0.56      1942\n",
      "           7       0.49      0.66      0.56      1343\n",
      "           8       0.22      0.25      0.23      1427\n",
      "           9       0.43      0.64      0.51       587\n",
      "          10       0.23      0.30      0.26       888\n",
      "          11       0.40      0.53      0.46      1197\n",
      "          12       0.29      0.42      0.34       964\n",
      "          13       0.27      0.46      0.34       696\n",
      "          14       0.53      0.69      0.60      1102\n",
      "          15       0.30      0.46      0.37       832\n",
      "          16       0.19      0.37      0.25       393\n",
      "          17       0.44      0.70      0.54       790\n",
      "          18       0.23      0.40      0.29       836\n",
      "          19       0.16      0.43      0.23       337\n",
      "          20       0.33      0.57      0.41       522\n",
      "          21       0.16      0.28      0.20       865\n",
      "          22       0.22      0.39      0.28       614\n",
      "          23       0.37      0.66      0.47       313\n",
      "          24       0.16      0.35      0.22       194\n",
      "          25       0.13      0.50      0.21       195\n",
      "          26       0.25      0.50      0.33       344\n",
      "          27       0.28      0.54      0.37       525\n",
      "          28       0.31      0.55      0.40       432\n",
      "          29       0.08      0.28      0.13       151\n",
      "          30       0.12      0.29      0.17       201\n",
      "          31       0.09      0.22      0.13       307\n",
      "          32       0.12      0.38      0.18       130\n",
      "          33       0.25      0.51      0.34       634\n",
      "          34       0.17      0.39      0.24       320\n",
      "          35       0.33      0.67      0.44       297\n",
      "          36       0.17      0.46      0.24       195\n",
      "          37       0.33      0.78      0.47        69\n",
      "          38       0.20      0.49      0.28       332\n",
      "          39       0.18      0.46      0.26       276\n",
      "          40       0.22      0.50      0.30       421\n",
      "          41       0.04      0.29      0.07        77\n",
      "          42       0.19      0.67      0.30        73\n",
      "          43       0.26      0.58      0.36       178\n",
      "          44       0.26      0.57      0.36       367\n",
      "          45       0.21      0.48      0.29       319\n",
      "          46       0.26      0.61      0.37       261\n",
      "          47       0.08      0.30      0.13       121\n",
      "          48       0.16      0.52      0.24       172\n",
      "          49       0.40      0.71      0.51       315\n",
      "          50       0.07      0.27      0.11       122\n",
      "          51       0.07      0.32      0.12       187\n",
      "          52       0.06      0.21      0.09        63\n",
      "          53       0.22      0.47      0.30       271\n",
      "          54       0.17      0.36      0.23       334\n",
      "          55       0.03      0.16      0.05        58\n",
      "          56       0.16      0.41      0.22       313\n",
      "          57       0.13      0.30      0.18       325\n",
      "          58       0.23      0.46      0.31       270\n",
      "          59       0.45      0.80      0.58       267\n",
      "          60       0.09      0.26      0.13       158\n",
      "          61       0.13      0.46      0.20        67\n",
      "          62       0.49      0.86      0.63       396\n",
      "          63       0.36      0.75      0.48       158\n",
      "          64       0.09      0.41      0.15        46\n",
      "          65       0.08      0.25      0.12       121\n",
      "          66       0.11      0.33      0.17       221\n",
      "          67       0.22      0.66      0.33       105\n",
      "          68       0.39      0.68      0.50       222\n",
      "          69       0.21      0.60      0.31       129\n",
      "          70       0.07      0.30      0.12       135\n",
      "          71       0.10      0.34      0.15       125\n",
      "          72       0.26      0.61      0.36       172\n",
      "          73       0.15      0.85      0.25        26\n",
      "          74       0.12      0.50      0.19        42\n",
      "          75       0.25      0.80      0.38        50\n",
      "          76       0.05      0.15      0.07       158\n",
      "          77       0.07      0.41      0.12        74\n",
      "          78       0.17      0.56      0.27       147\n",
      "          79       0.15      0.38      0.22       187\n",
      "          80       0.14      0.46      0.22       128\n",
      "          81       0.09      0.25      0.13       173\n",
      "          82       0.05      0.22      0.08       103\n",
      "          83       0.20      0.65      0.30       168\n",
      "          84       0.13      0.39      0.20        76\n",
      "          85       0.14      0.40      0.21       212\n",
      "          86       0.08      0.26      0.12       184\n",
      "          87       0.08      0.41      0.13        74\n",
      "          88       0.08      0.42      0.14        38\n",
      "          89       0.06      0.25      0.09        59\n",
      "          90       0.18      0.44      0.25       194\n",
      "          91       0.12      0.38      0.18       164\n",
      "          92       0.24      0.62      0.34       194\n",
      "          93       0.19      0.63      0.29       120\n",
      "          94       0.04      0.18      0.06        49\n",
      "          95       0.06      0.26      0.10        86\n",
      "          96       0.13      0.29      0.18       277\n",
      "          97       0.18      0.58      0.28       116\n",
      "          98       0.21      0.76      0.32        59\n",
      "          99       0.14      0.46      0.22        95\n",
      "         100       0.10      0.27      0.15       178\n",
      "         101       0.41      0.73      0.52       136\n",
      "         102       0.56      0.81      0.66       295\n",
      "         103       0.07      0.29      0.11        70\n",
      "         104       0.06      0.19      0.09       113\n",
      "         105       0.18      0.59      0.28       145\n",
      "         106       0.10      0.26      0.15       118\n",
      "         107       0.28      0.62      0.39       146\n",
      "         108       0.18      0.55      0.27       152\n",
      "         109       0.20      0.61      0.30       114\n",
      "         110       0.04      0.18      0.06        91\n",
      "         111       0.03      0.20      0.05        54\n",
      "         112       0.11      0.53      0.19        60\n",
      "         113       0.10      0.36      0.16        72\n",
      "         114       0.06      0.34      0.10        82\n",
      "         115       0.14      0.45      0.21       159\n",
      "         116       0.23      0.56      0.33       154\n",
      "         117       0.11      0.30      0.16       142\n",
      "         118       0.07      0.38      0.12        50\n",
      "         119       0.14      0.43      0.21       168\n",
      "         120       0.04      0.18      0.07        92\n",
      "         121       0.32      0.66      0.43       229\n",
      "         122       0.06      0.32      0.10        60\n",
      "         123       0.02      0.08      0.03        95\n",
      "         124       0.39      0.77      0.52        96\n",
      "         125       0.06      0.29      0.09        56\n",
      "         126       0.28      0.73      0.40       157\n",
      "         127       0.02      0.22      0.04        50\n",
      "         128       0.05      0.23      0.08        95\n",
      "         129       0.08      0.36      0.13        42\n",
      "         130       0.07      0.43      0.12        44\n",
      "         131       0.03      0.14      0.05        76\n",
      "         132       0.17      0.46      0.25       137\n",
      "         133       0.47      0.72      0.57       232\n",
      "         134       0.26      0.90      0.41        49\n",
      "         135       0.03      0.20      0.05        40\n",
      "         136       0.06      0.47      0.11        59\n",
      "         137       0.15      0.40      0.22       126\n",
      "         138       0.08      0.31      0.12       116\n",
      "         139       0.12      0.45      0.19        73\n",
      "         140       0.21      0.57      0.31       114\n",
      "         141       0.06      0.20      0.09        84\n",
      "         142       0.21      0.56      0.31       119\n",
      "         143       0.59      0.87      0.70       194\n",
      "         144       0.09      0.45      0.15        66\n",
      "         145       0.13      0.51      0.20       100\n",
      "         146       0.19      0.51      0.27       167\n",
      "         147       0.08      0.56      0.15        36\n",
      "         148       0.07      0.31      0.11        78\n",
      "         149       0.27      0.63      0.37       187\n",
      "         150       0.08      0.29      0.12       156\n",
      "         151       0.07      0.72      0.13        18\n",
      "         152       0.26      0.66      0.38       150\n",
      "         153       0.11      0.45      0.18        38\n",
      "         154       0.08      0.22      0.11       118\n",
      "         155       0.04      0.18      0.07        95\n",
      "         156       0.19      0.80      0.31        66\n",
      "         157       0.06      0.19      0.09        86\n",
      "         158       0.15      0.64      0.24        33\n",
      "         159       0.05      0.33      0.09        49\n",
      "         160       0.08      0.36      0.13        83\n",
      "         161       0.14      0.61      0.22        76\n",
      "         162       0.06      0.20      0.09        91\n",
      "         163       0.21      0.51      0.30       121\n",
      "         164       0.16      0.57      0.25        95\n",
      "         165       0.16      0.43      0.23       122\n",
      "         166       0.13      0.58      0.22        53\n",
      "         167       0.08      0.35      0.13        97\n",
      "         168       0.11      0.37      0.17       135\n",
      "         169       0.10      0.34      0.16       105\n",
      "         170       0.02      0.24      0.04        29\n",
      "         171       0.11      0.47      0.17        43\n",
      "         172       0.09      0.48      0.16        83\n",
      "         173       0.16      0.51      0.24       130\n",
      "         174       0.08      0.41      0.13        66\n",
      "         175       0.37      0.93      0.53        56\n",
      "         176       0.38      0.75      0.50       159\n",
      "         177       0.28      0.69      0.40       130\n",
      "         178       0.22      0.66      0.33       115\n",
      "         179       0.04      0.25      0.07        60\n",
      "         180       0.10      0.57      0.16        51\n",
      "         181       0.02      0.22      0.04        46\n",
      "         182       0.02      0.12      0.03        51\n",
      "         183       0.21      0.52      0.30       165\n",
      "         184       0.04      0.23      0.07        97\n",
      "         185       0.03      0.16      0.05        32\n",
      "         186       0.08      0.33      0.13        79\n",
      "         187       0.24      0.71      0.36        87\n",
      "         188       0.05      0.24      0.09        80\n",
      "         189       0.22      0.63      0.33        51\n",
      "         190       0.23      0.66      0.34        80\n",
      "         191       0.05      0.25      0.09        83\n",
      "         192       0.17      0.56      0.27       103\n",
      "         193       0.06      0.50      0.11        20\n",
      "         194       0.20      0.64      0.30        89\n",
      "         195       0.04      0.19      0.07        77\n",
      "         196       0.03      0.16      0.05        91\n",
      "         197       0.10      0.33      0.16       112\n",
      "         198       0.09      0.26      0.14       118\n",
      "         199       0.06      0.26      0.10        53\n",
      "         200       0.33      0.80      0.46        86\n",
      "         201       0.14      0.31      0.19       129\n",
      "         202       0.28      0.66      0.40       158\n",
      "         203       0.10      0.31      0.15       111\n",
      "         204       0.11      0.34      0.16        93\n",
      "         205       0.05      0.26      0.08        34\n",
      "         206       0.02      0.08      0.03        50\n",
      "         207       0.07      0.39      0.12        31\n",
      "         208       0.25      0.62      0.35       138\n",
      "         209       0.11      0.29      0.16       106\n",
      "         210       0.14      0.44      0.21       126\n",
      "         211       0.07      0.45      0.12        51\n",
      "         212       0.03      0.19      0.05        32\n",
      "         213       0.05      0.50      0.08        14\n",
      "         214       0.11      0.57      0.18        40\n",
      "         215       0.07      0.49      0.12        57\n",
      "         216       0.17      0.39      0.24       186\n",
      "         217       0.02      0.30      0.04        20\n",
      "         218       0.38      0.86      0.53       100\n",
      "         219       0.18      0.52      0.26        96\n",
      "         220       0.30      0.75      0.43       109\n",
      "         221       0.37      0.78      0.50       129\n",
      "         222       0.15      0.59      0.24        64\n",
      "         223       0.04      0.20      0.06        49\n",
      "         224       0.18      0.58      0.27        85\n",
      "         225       0.08      0.41      0.13        69\n",
      "         226       0.10      0.35      0.16        82\n",
      "         227       0.13      0.48      0.21        69\n",
      "         228       0.02      0.09      0.04        97\n",
      "         229       0.03      0.17      0.05        60\n",
      "         230       0.04      0.27      0.08        77\n",
      "         231       0.06      0.41      0.10        22\n",
      "         232       0.05      0.20      0.08        59\n",
      "         233       0.11      0.45      0.18        77\n",
      "         234       0.10      0.43      0.16        37\n",
      "         235       0.10      0.47      0.16        30\n",
      "         236       0.34      0.74      0.46        39\n",
      "         237       0.07      0.24      0.10       101\n",
      "         238       0.03      0.26      0.06        50\n",
      "         239       0.01      0.05      0.01        21\n",
      "         240       0.11      0.39      0.17        33\n",
      "         241       0.05      0.26      0.09        88\n",
      "         242       0.10      0.53      0.17        36\n",
      "         243       0.03      0.24      0.05        29\n",
      "         244       0.24      0.54      0.33       112\n",
      "         245       0.05      0.20      0.08        44\n",
      "         246       0.29      0.79      0.43        62\n",
      "         247       0.04      0.38      0.07        26\n",
      "         248       0.02      0.14      0.04        51\n",
      "         249       0.01      0.03      0.01        31\n",
      "         250       0.06      0.48      0.11        23\n",
      "         251       0.10      0.41      0.16        73\n",
      "         252       0.04      0.15      0.06        53\n",
      "         253       0.08      0.39      0.13        44\n",
      "         254       0.04      0.24      0.07        50\n",
      "         255       0.08      0.27      0.12       158\n",
      "         256       0.06      0.23      0.09        83\n",
      "         257       0.06      0.38      0.10        40\n",
      "         258       0.23      0.55      0.33        98\n",
      "         259       0.09      0.32      0.14        63\n",
      "         260       0.11      0.35      0.17        95\n",
      "         261       0.09      0.32      0.14        85\n",
      "         262       0.10      0.48      0.17        42\n",
      "         263       0.23      0.43      0.30       162\n",
      "         264       0.05      0.34      0.08        41\n",
      "         265       0.08      0.47      0.14        34\n",
      "         266       0.01      0.08      0.02        12\n",
      "         267       0.04      0.29      0.07        21\n",
      "         268       0.15      0.50      0.24        92\n",
      "         269       0.03      0.29      0.05        21\n",
      "         270       0.15      0.61      0.24        62\n",
      "         271       0.10      0.58      0.17        77\n",
      "         272       0.09      0.33      0.14        89\n",
      "         273       0.43      0.85      0.57       119\n",
      "         274       0.16      0.68      0.25        37\n",
      "         275       0.05      0.23      0.08        40\n",
      "         276       0.03      0.18      0.06        51\n",
      "         277       0.17      0.69      0.27        54\n",
      "         278       0.14      0.65      0.23        57\n",
      "         279       0.25      0.83      0.38        89\n",
      "         280       0.07      0.41      0.12        29\n",
      "         281       0.31      0.64      0.42        81\n",
      "         282       0.20      0.64      0.30        56\n",
      "         283       0.13      0.42      0.20       137\n",
      "         284       0.14      0.59      0.22        29\n",
      "         285       0.21      0.63      0.31       103\n",
      "         286       0.32      0.83      0.47       100\n",
      "         287       0.19      0.61      0.29        51\n",
      "         288       0.03      0.23      0.05        43\n",
      "         289       0.02      0.16      0.04        32\n",
      "         290       0.03      0.22      0.05        50\n",
      "         291       0.11      0.45      0.17        58\n",
      "         292       0.05      0.30      0.08        33\n",
      "         293       0.16      0.74      0.26        50\n",
      "         294       0.09      0.43      0.14        68\n",
      "         295       0.10      0.38      0.16        37\n",
      "         296       0.02      0.33      0.03        12\n",
      "         297       0.04      0.21      0.07        47\n",
      "         298       0.10      0.65      0.17        17\n",
      "         299       0.17      0.73      0.28        22\n",
      "         300       0.05      0.20      0.08        60\n",
      "         301       0.01      0.06      0.01        52\n",
      "         302       0.02      0.14      0.03        21\n",
      "         303       0.21      0.68      0.32        90\n",
      "         304       0.34      0.81      0.48        88\n",
      "         305       0.36      0.77      0.49        64\n",
      "         306       0.22      0.49      0.30       101\n",
      "         307       0.04      0.20      0.07        45\n",
      "         308       0.00      0.03      0.01        40\n",
      "         309       0.04      0.27      0.06        41\n",
      "         310       0.18      0.56      0.27        68\n",
      "         311       0.08      0.50      0.14        20\n",
      "         312       0.04      0.25      0.07        20\n",
      "         313       0.01      0.10      0.02        30\n",
      "         314       0.16      0.49      0.24        81\n",
      "         315       0.12      0.47      0.20        59\n",
      "         316       0.06      0.29      0.10        24\n",
      "         317       0.06      0.26      0.09        78\n",
      "         318       0.17      0.48      0.25        60\n",
      "         319       0.20      0.59      0.30        46\n",
      "         320       0.09      0.48      0.15        61\n",
      "         321       0.16      0.60      0.26        48\n",
      "         322       0.14      0.60      0.23        58\n",
      "         323       0.27      0.55      0.36       155\n",
      "         324       0.05      0.30      0.08        37\n",
      "         325       0.04      0.27      0.07        33\n",
      "         326       0.04      0.27      0.06        52\n",
      "         327       0.01      0.07      0.02        59\n",
      "         328       0.34      0.83      0.48        71\n",
      "         329       0.12      0.45      0.19        66\n",
      "         330       0.09      0.32      0.14        50\n",
      "         331       0.05      0.28      0.08        32\n",
      "         332       0.01      0.20      0.03        10\n",
      "         333       0.05      0.20      0.08        81\n",
      "         334       0.06      0.30      0.10        30\n",
      "         335       0.07      0.33      0.12        36\n",
      "         336       0.12      0.57      0.19        21\n",
      "         337       0.02      0.19      0.03        26\n",
      "         338       0.16      0.47      0.23        53\n",
      "         339       0.00      0.07      0.01        29\n",
      "         340       0.18      0.56      0.27        55\n",
      "         341       0.05      0.42      0.10        12\n",
      "         342       0.10      0.58      0.17        19\n",
      "         343       0.07      0.45      0.12        20\n",
      "         344       0.10      0.48      0.17        21\n",
      "         345       0.12      0.57      0.20        21\n",
      "         346       0.01      0.12      0.02        25\n",
      "         347       0.14      0.55      0.22        47\n",
      "         348       0.02      0.28      0.04        25\n",
      "         349       0.20      0.76      0.32        34\n",
      "         350       0.07      0.75      0.12         8\n",
      "         351       0.05      0.40      0.08        10\n",
      "         352       0.21      0.61      0.31        61\n",
      "         353       0.04      0.50      0.07        10\n",
      "         354       0.04      0.26      0.08        58\n",
      "         355       0.11      0.49      0.19        59\n",
      "         356       0.23      0.57      0.33       145\n",
      "         357       0.24      0.51      0.33       135\n",
      "         358       0.11      0.53      0.18        36\n",
      "         359       0.22      0.66      0.33        67\n",
      "         360       0.07      0.29      0.11        62\n",
      "         361       0.38      0.87      0.53        76\n",
      "         362       0.03      0.25      0.05        24\n",
      "         363       0.34      0.81      0.47        64\n",
      "         364       0.06      0.33      0.10        18\n",
      "         365       0.05      0.33      0.08        27\n",
      "         366       0.15      0.78      0.26        27\n",
      "         367       0.07      0.28      0.12        68\n",
      "         368       0.02      0.23      0.03        13\n",
      "         369       0.01      0.12      0.02        24\n",
      "         370       0.01      0.09      0.02        22\n",
      "         371       0.03      0.22      0.05        36\n",
      "         372       0.20      0.37      0.26        30\n",
      "         373       0.21      0.62      0.32        74\n",
      "         374       0.05      0.33      0.09        30\n",
      "         375       0.01      0.15      0.03        20\n",
      "         376       0.00      0.00      0.00        15\n",
      "         377       0.15      0.46      0.22        79\n",
      "         378       0.01      0.06      0.01        35\n",
      "         379       0.00      0.00      0.00         7\n",
      "         380       0.15      0.42      0.22        57\n",
      "         381       0.03      0.15      0.04        59\n",
      "         382       0.04      0.24      0.07        41\n",
      "         383       0.15      0.52      0.23        69\n",
      "         384       0.01      0.05      0.01        42\n",
      "         385       0.05      0.19      0.08        21\n",
      "         386       0.02      0.12      0.04        24\n",
      "         387       0.05      0.29      0.09        34\n",
      "         388       0.07      0.38      0.11        39\n",
      "         389       0.02      0.18      0.04        45\n",
      "         390       0.35      0.69      0.47        75\n",
      "         391       0.06      0.31      0.10        32\n",
      "         392       0.08      0.62      0.14         8\n",
      "         393       0.01      0.03      0.01        59\n",
      "         394       0.03      0.16      0.05        38\n",
      "         395       0.08      0.41      0.13        61\n",
      "         396       0.06      0.26      0.09        65\n",
      "         397       0.26      0.58      0.36       190\n",
      "         398       0.15      0.75      0.26        36\n",
      "         399       0.01      0.13      0.02        23\n",
      "         400       0.05      0.33      0.09        27\n",
      "         401       0.05      0.33      0.09        36\n",
      "         402       0.08      0.33      0.12        52\n",
      "         403       0.02      0.36      0.04        11\n",
      "         404       0.12      0.83      0.22         6\n",
      "         405       0.07      0.27      0.10        49\n",
      "         406       0.06      0.58      0.11        19\n",
      "         407       0.04      0.27      0.07        11\n",
      "         408       0.13      0.63      0.21        46\n",
      "         409       0.01      0.06      0.02        16\n",
      "         410       0.00      0.00      0.00         5\n",
      "         411       0.08      0.28      0.12        68\n",
      "         412       0.05      0.28      0.09        32\n",
      "         413       0.05      0.16      0.07        61\n",
      "         414       0.02      0.14      0.03        21\n",
      "         415       0.04      0.33      0.07        18\n",
      "         416       0.08      0.56      0.14        25\n",
      "         417       0.01      0.06      0.01        16\n",
      "         418       0.03      0.31      0.06        26\n",
      "         419       0.12      0.48      0.19        50\n",
      "         420       0.03      0.29      0.05         7\n",
      "         421       0.04      0.29      0.07        28\n",
      "         422       0.06      0.44      0.11        32\n",
      "         423       0.06      0.43      0.11        28\n",
      "         424       0.25      0.89      0.39         9\n",
      "         425       0.00      0.00      0.00         6\n",
      "         426       0.03      0.21      0.05        33\n",
      "         427       0.34      0.81      0.48        64\n",
      "         428       0.16      0.79      0.27        14\n",
      "         429       0.36      0.73      0.48        11\n",
      "         430       0.12      0.46      0.19        61\n",
      "         431       0.08      0.39      0.13        49\n",
      "         432       0.00      0.00      0.00         7\n",
      "         433       0.06      0.55      0.10        11\n",
      "         434       0.15      0.60      0.24        15\n",
      "         435       0.05      0.38      0.09        16\n",
      "         436       0.10      0.43      0.16        42\n",
      "         437       0.05      0.18      0.08        49\n",
      "         438       0.35      0.76      0.47        49\n",
      "         439       0.07      0.31      0.11        55\n",
      "         440       0.04      0.16      0.06        45\n",
      "         441       0.03      0.21      0.06        24\n",
      "         442       0.03      0.19      0.05        31\n",
      "         443       0.06      0.28      0.11        25\n",
      "         444       0.04      0.36      0.08        25\n",
      "         445       0.07      0.45      0.12        22\n",
      "         446       0.05      0.38      0.10        16\n",
      "         447       0.02      0.44      0.04         9\n",
      "         448       0.33      0.60      0.43        43\n",
      "         449       0.12      0.39      0.18        44\n",
      "         450       0.09      0.38      0.15        37\n",
      "         451       0.07      0.33      0.11        30\n",
      "         452       0.03      0.23      0.05        13\n",
      "         453       0.02      0.16      0.03        25\n",
      "         454       0.14      0.62      0.22        39\n",
      "         455       0.10      0.38      0.16        98\n",
      "         456       0.02      0.50      0.03         2\n",
      "         457       0.10      0.39      0.16        54\n",
      "         458       0.01      0.20      0.02        10\n",
      "         459       0.04      0.24      0.06        21\n",
      "         460       0.01      0.07      0.02        29\n",
      "         461       0.14      0.54      0.23        28\n",
      "         462       0.03      0.15      0.05        27\n",
      "         463       0.06      0.42      0.10        40\n",
      "         464       0.16      0.43      0.23        49\n",
      "         465       0.09      0.47      0.15        32\n",
      "         466       0.18      0.47      0.26        90\n",
      "         467       0.18      0.59      0.28        49\n",
      "         468       0.10      0.41      0.16        59\n",
      "         469       0.01      0.03      0.01        35\n",
      "         470       0.28      0.81      0.42        42\n",
      "         471       0.11      0.38      0.17        42\n",
      "         472       0.17      0.45      0.25        64\n",
      "         473       0.07      0.41      0.12        22\n",
      "         474       0.04      0.19      0.06        37\n",
      "         475       0.03      0.17      0.05        30\n",
      "         476       0.20      0.69      0.31        29\n",
      "         477       0.27      0.70      0.39        67\n",
      "         478       0.17      0.88      0.29        16\n",
      "         479       0.04      0.19      0.07        37\n",
      "         480       0.23      0.67      0.34        46\n",
      "         481       0.13      0.34      0.19        56\n",
      "         482       0.05      0.39      0.09        28\n",
      "         483       0.32      0.76      0.45        51\n",
      "         484       0.04      0.22      0.06        32\n",
      "         485       0.02      0.11      0.03        37\n",
      "         486       0.05      0.29      0.09        41\n",
      "         487       0.08      0.46      0.14        24\n",
      "         488       0.01      0.12      0.03        26\n",
      "         489       0.10      0.42      0.16        36\n",
      "         490       0.27      0.83      0.40        29\n",
      "         491       0.04      0.24      0.07        37\n",
      "         492       0.00      0.06      0.01        16\n",
      "         493       0.01      0.07      0.02        41\n",
      "         494       0.08      0.35      0.13        26\n",
      "         495       0.13      0.67      0.22        21\n",
      "         496       0.08      0.33      0.13        12\n",
      "         497       0.10      0.41      0.16        39\n",
      "         498       0.04      0.23      0.07        31\n",
      "         499       0.06      0.36      0.10        45\n",
      "\n",
      "   micro avg       0.19      0.48      0.27     67448\n",
      "   macro avg       0.13      0.42      0.19     67448\n",
      "weighted avg       0.27      0.48      0.33     67448\n",
      " samples avg       0.29      0.45      0.30     67448\n",
      "\n",
      "Time taken to run this cell : 1:07:51.576493\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "classifier = OneVsRestClassifier(SGDClassifier(loss='hinge', alpha=0.00001, penalty='l1'))\n",
    "classifier.fit(x_train_multilabel, y_train)\n",
    "predictions = classifier.predict (x_test_multilabel)\n",
    "\n",
    "\n",
    "print(\"Accuracy :\",metrics.accuracy_score(y_test, predictions))\n",
    "print(\"Hamming loss \",metrics.hamming_loss(y_test,predictions))\n",
    "\n",
    "\n",
    "precision = precision_score(y_test, predictions, average='micro')\n",
    "recall = recall_score(y_test, predictions, average='micro')\n",
    "f1 = f1_score(y_test, predictions, average='micro')\n",
    " \n",
    "print(\"Micro-average quality numbers\")\n",
    "print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
    "\n",
    "precision = precision_score(y_test, predictions, average='macro')\n",
    "recall = recall_score(y_test, predictions, average='macro')\n",
    "f1 = f1_score(y_test, predictions, average='macro')\n",
    " \n",
    "print(\"Macro-average quality numbers\")\n",
    "print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
    "\n",
    "print (metrics.classification_report(y_test, predictions))\n",
    "print(\"Time taken to run this cell :\", datetime.now() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Logistic regresssion hyperparameter tuning</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of train data X: (40000, 20000) Y : (40000, 500)\n",
      "Dimensions of test data X: (10000, 20000) Y: (10000, 500)\n"
     ]
    }
   ],
   "source": [
    "print(\"Dimensions of train data X:\",x_train_multilabel.shape, \"Y :\",y_train.shape)\n",
    "print(\"Dimensions of test data X:\",x_test_multilabel.shape,\"Y:\",y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_multilabel=x_test_multilabel[0:10000]\n",
    "y_test=y_test[0:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=2, error_score=nan,\n",
       "             estimator=OneVsRestClassifier(estimator=SGDClassifier(alpha=0.0001,\n",
       "                                                                   average=False,\n",
       "                                                                   class_weight=None,\n",
       "                                                                   early_stopping=False,\n",
       "                                                                   epsilon=0.1,\n",
       "                                                                   eta0=0.0,\n",
       "                                                                   fit_intercept=True,\n",
       "                                                                   l1_ratio=0.15,\n",
       "                                                                   learning_rate='optimal',\n",
       "                                                                   loss='log',\n",
       "                                                                   max_iter=1000,\n",
       "                                                                   n_iter_no_change=5,\n",
       "                                                                   n_jobs=None,\n",
       "                                                                   penalty='l1',\n",
       "                                                                   power_t=0.5,\n",
       "                                                                   random_state=None,\n",
       "                                                                   shuffle=True,\n",
       "                                                                   tol=0.001,\n",
       "                                                                   validation_fraction=0.1,\n",
       "                                                                   verbose=0,\n",
       "                                                                   warm_start=False),\n",
       "                                           n_jobs=None),\n",
       "             iid='deprecated', n_jobs=-1,\n",
       "             param_grid={'estimator__alpha': [1e-05, 0.001, 0.1, 10, 100]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='f1_micro', verbose=0)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#https://stackoverflow.com/questions/12632992/gridsearch-for-an-estimator-inside-a-onevsrestclassifier/12637528\n",
    "param_grid  = {\"estimator__alpha\": [10**-5, 10**-3, 10**-1, 10**1, 10**2]}\n",
    "\n",
    "clf = OneVsRestClassifier(SGDClassifier(loss='log',penalty='l1'))\n",
    "\n",
    "model = GridSearchCV(clf,param_grid, scoring = 'f1_micro', cv=2,n_jobs=-1)\n",
    "\n",
    "model.fit(x_train_multilabel, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.0693\n",
      "Hamming loss  0.0070414\n",
      "Micro-average quality numbers\n",
      "Precision: 0.2586, Recall: 0.4730, F1-measure: 0.3344\n",
      "Macro-average quality numbers\n",
      "Precision: 0.1965, Recall: 0.3564, F1-measure: 0.2354\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.80      0.70       659\n",
      "           1       0.36      0.34      0.35      1017\n",
      "           2       0.53      0.55      0.54       895\n",
      "           3       0.50      0.66      0.57       579\n",
      "           4       0.52      0.69      0.59       903\n",
      "           5       0.46      0.53      0.49       453\n",
      "           6       0.50      0.59      0.54       533\n",
      "           7       0.43      0.72      0.54       498\n",
      "           8       0.19      0.26      0.22       384\n",
      "           9       0.50      0.69      0.58       120\n",
      "          10       0.18      0.28      0.22       219\n",
      "          11       0.43      0.47      0.45       333\n",
      "          12       0.21      0.36      0.27       257\n",
      "          13       0.27      0.41      0.32       206\n",
      "          14       0.55      0.74      0.63       313\n",
      "          15       0.32      0.48      0.38       310\n",
      "          16       0.25      0.45      0.33       120\n",
      "          17       0.31      0.76      0.45       127\n",
      "          18       0.13      0.29      0.18       142\n",
      "          19       0.27      0.50      0.35       151\n",
      "          20       0.30      0.56      0.39       142\n",
      "          21       0.14      0.24      0.17       138\n",
      "          22       0.16      0.38      0.22       122\n",
      "          23       0.33      0.57      0.42        72\n",
      "          24       0.10      0.29      0.15        48\n",
      "          25       0.38      0.54      0.45        76\n",
      "          26       0.10      0.46      0.17        59\n",
      "          27       0.23      0.48      0.31       105\n",
      "          28       0.42      0.60      0.50       139\n",
      "          29       0.12      0.35      0.18        26\n",
      "          30       0.16      0.40      0.23        62\n",
      "          31       0.04      0.19      0.07        48\n",
      "          32       0.21      0.56      0.31        32\n",
      "          33       0.25      0.40      0.31        88\n",
      "          34       0.13      0.17      0.15        75\n",
      "          35       0.31      0.62      0.42        69\n",
      "          36       0.15      0.29      0.20        51\n",
      "          37       0.36      0.76      0.48        21\n",
      "          38       0.16      0.50      0.24       107\n",
      "          39       0.25      0.49      0.33        92\n",
      "          40       0.41      0.45      0.43        91\n",
      "          41       0.03      0.22      0.06         9\n",
      "          42       0.05      0.62      0.10         8\n",
      "          43       0.36      0.55      0.43        60\n",
      "          44       0.43      0.67      0.52       221\n",
      "          45       0.22      0.48      0.30        44\n",
      "          46       0.47      0.69      0.56        88\n",
      "          47       0.08      0.12      0.10        34\n",
      "          48       0.16      0.45      0.24        42\n",
      "          49       0.31      0.79      0.45        57\n",
      "          50       0.04      0.14      0.07        21\n",
      "          51       0.10      0.25      0.14        40\n",
      "          52       0.07      0.14      0.09        21\n",
      "          53       0.17      0.39      0.24        31\n",
      "          54       0.15      0.43      0.22        60\n",
      "          55       0.02      0.08      0.03        13\n",
      "          56       0.10      0.33      0.15        33\n",
      "          57       0.13      0.26      0.17        53\n",
      "          58       0.34      0.40      0.37        58\n",
      "          59       0.41      0.75      0.53        64\n",
      "          60       0.09      0.23      0.13        31\n",
      "          61       0.16      0.53      0.24        17\n",
      "          62       0.58      0.89      0.70       156\n",
      "          63       0.10      0.58      0.16        12\n",
      "          64       0.25      0.64      0.36        11\n",
      "          65       0.06      0.12      0.08        26\n",
      "          66       0.12      0.18      0.14       103\n",
      "          67       0.42      0.48      0.44        21\n",
      "          68       0.33      0.69      0.44        58\n",
      "          69       0.22      0.38      0.28        34\n",
      "          70       0.11      0.20      0.14        41\n",
      "          71       0.16      0.40      0.22        25\n",
      "          72       0.19      0.60      0.28        48\n",
      "          73       0.40      0.73      0.52        11\n",
      "          74       0.03      0.25      0.05         8\n",
      "          75       0.81      0.93      0.87        14\n",
      "          76       0.04      0.09      0.06        46\n",
      "          77       0.15      0.29      0.20        17\n",
      "          78       0.47      0.55      0.51        49\n",
      "          79       0.06      0.16      0.08        31\n",
      "          80       0.26      0.51      0.34        41\n",
      "          81       0.09      0.34      0.14        56\n",
      "          82       0.04      0.12      0.07        24\n",
      "          83       0.37      0.66      0.47        59\n",
      "          84       0.17      0.38      0.24        26\n",
      "          85       0.09      0.12      0.10        43\n",
      "          86       0.07      0.30      0.11        73\n",
      "          87       0.08      0.75      0.15        12\n",
      "          88       0.11      0.29      0.16        14\n",
      "          89       0.03      0.06      0.04        18\n",
      "          90       0.16      0.33      0.22        30\n",
      "          91       0.12      0.37      0.18        30\n",
      "          92       0.48      0.51      0.50        55\n",
      "          93       0.19      0.58      0.29        19\n",
      "          94       0.02      0.12      0.03         8\n",
      "          95       0.08      0.24      0.11        17\n",
      "          96       0.13      0.21      0.16       189\n",
      "          97       0.16      0.39      0.23        31\n",
      "          98       0.11      0.38      0.17        13\n",
      "          99       0.28      0.56      0.38        16\n",
      "         100       0.06      0.19      0.09        43\n",
      "         101       0.42      0.70      0.53        30\n",
      "         102       0.50      0.72      0.59        53\n",
      "         103       0.26      0.32      0.28        38\n",
      "         104       0.06      0.25      0.10        36\n",
      "         105       0.23      0.62      0.33        26\n",
      "         106       0.06      0.09      0.07        46\n",
      "         107       0.50      0.65      0.57        40\n",
      "         108       0.07      0.22      0.10        23\n",
      "         109       0.34      0.71      0.46        31\n",
      "         110       0.12      0.28      0.17        32\n",
      "         111       0.09      0.26      0.14        19\n",
      "         112       0.04      0.11      0.05        18\n",
      "         113       0.22      0.24      0.23        17\n",
      "         114       0.04      0.24      0.07        17\n",
      "         115       0.07      0.17      0.10        36\n",
      "         116       0.13      0.45      0.20        40\n",
      "         117       0.22      0.42      0.29        48\n",
      "         118       0.21      0.62      0.31         8\n",
      "         119       0.24      0.39      0.30        84\n",
      "         120       0.05      0.17      0.08        18\n",
      "         121       0.52      0.74      0.61        77\n",
      "         122       0.18      0.16      0.17        32\n",
      "         123       0.00      0.00      0.00        28\n",
      "         124       0.42      0.82      0.56        17\n",
      "         125       0.01      0.07      0.02        14\n",
      "         126       0.51      0.57      0.54        53\n",
      "         127       0.03      0.19      0.05        16\n",
      "         128       0.07      0.22      0.10        18\n",
      "         129       0.07      0.33      0.12         9\n",
      "         130       0.04      0.40      0.07         5\n",
      "         131       0.05      0.18      0.08        45\n",
      "         132       0.07      0.26      0.11        27\n",
      "         133       0.45      0.74      0.56        50\n",
      "         134       0.62      0.88      0.73        17\n",
      "         135       0.05      0.25      0.09        12\n",
      "         136       0.04      0.20      0.06        15\n",
      "         137       0.14      0.20      0.16        25\n",
      "         138       0.09      0.46      0.16        28\n",
      "         139       0.19      0.43      0.26        21\n",
      "         140       0.18      0.55      0.27        29\n",
      "         141       0.08      0.15      0.10        41\n",
      "         142       0.37      0.52      0.43        29\n",
      "         143       0.48      0.77      0.59        26\n",
      "         144       0.30      0.40      0.35        35\n",
      "         145       0.30      0.53      0.38        19\n",
      "         146       0.20      0.50      0.29        24\n",
      "         147       0.18      0.40      0.25        15\n",
      "         148       0.11      0.42      0.18        12\n",
      "         149       0.75      0.78      0.76        58\n",
      "         150       0.33      0.32      0.32        50\n",
      "         151       0.17      0.71      0.27         7\n",
      "         152       0.30      0.89      0.44        18\n",
      "         153       0.10      0.33      0.15         6\n",
      "         154       0.10      0.28      0.14        18\n",
      "         155       0.02      0.06      0.03        18\n",
      "         156       0.57      0.57      0.57        23\n",
      "         157       0.05      0.11      0.06        18\n",
      "         158       0.41      0.78      0.54         9\n",
      "         159       0.50      0.56      0.53        16\n",
      "         160       0.11      0.36      0.17        22\n",
      "         161       0.23      0.60      0.33        15\n",
      "         162       0.02      0.04      0.03        25\n",
      "         163       0.37      0.46      0.41        28\n",
      "         164       0.12      0.26      0.17        27\n",
      "         165       0.29      0.51      0.37        43\n",
      "         166       0.08      0.28      0.12        25\n",
      "         167       0.20      0.21      0.21        28\n",
      "         168       0.04      0.33      0.07        12\n",
      "         169       0.21      0.35      0.26        23\n",
      "         170       0.07      0.22      0.11         9\n",
      "         171       0.19      0.50      0.28        12\n",
      "         172       0.14      0.50      0.22        12\n",
      "         173       0.21      0.37      0.27        27\n",
      "         174       0.10      0.28      0.15        18\n",
      "         175       0.77      0.91      0.83        11\n",
      "         176       0.51      0.81      0.63        57\n",
      "         177       0.29      0.41      0.34        32\n",
      "         178       0.27      0.70      0.39        20\n",
      "         179       0.24      0.30      0.27        23\n",
      "         180       0.41      0.43      0.42        21\n",
      "         181       0.06      0.31      0.09        13\n",
      "         182       0.04      0.20      0.07        10\n",
      "         183       0.45      0.54      0.49        96\n",
      "         184       0.15      0.10      0.12        40\n",
      "         185       0.00      0.00      0.00         7\n",
      "         186       0.12      0.27      0.17        11\n",
      "         187       0.17      0.50      0.25        12\n",
      "         188       0.08      0.19      0.11        16\n",
      "         189       0.06      0.14      0.08         7\n",
      "         190       0.52      0.59      0.55        27\n",
      "         191       0.06      0.31      0.10        13\n",
      "         192       0.45      0.57      0.50        37\n",
      "         193       0.08      0.22      0.12         9\n",
      "         194       0.30      0.82      0.44        11\n",
      "         195       0.05      0.11      0.07        27\n",
      "         196       0.06      0.25      0.10        16\n",
      "         197       0.03      0.12      0.05        26\n",
      "         198       0.11      0.21      0.14        29\n",
      "         199       0.02      0.12      0.03         8\n",
      "         200       0.75      0.73      0.74        37\n",
      "         201       0.14      0.26      0.18        31\n",
      "         202       0.25      0.62      0.36        21\n",
      "         203       0.11      0.25      0.15        28\n",
      "         204       0.06      0.36      0.11        11\n",
      "         205       0.16      0.29      0.20        17\n",
      "         206       0.02      0.06      0.03        17\n",
      "         207       0.00      0.00      0.00         4\n",
      "         208       0.60      0.77      0.67        47\n",
      "         209       0.25      0.16      0.19        19\n",
      "         210       0.27      0.50      0.35        54\n",
      "         211       0.10      0.29      0.15        17\n",
      "         212       0.00      0.00      0.00         3\n",
      "         213       0.07      0.25      0.11         4\n",
      "         214       0.03      0.17      0.06         6\n",
      "         215       0.26      0.39      0.31        18\n",
      "         216       0.08      0.12      0.10        25\n",
      "         217       0.00      0.00      0.00         6\n",
      "         218       0.68      0.93      0.78        43\n",
      "         219       0.34      0.39      0.37        33\n",
      "         220       0.40      0.54      0.46        26\n",
      "         221       0.62      0.54      0.58        24\n",
      "         222       0.28      0.55      0.37        20\n",
      "         223       0.03      0.11      0.05         9\n",
      "         224       0.21      0.57      0.31        30\n",
      "         225       0.05      0.67      0.09         6\n",
      "         226       0.19      0.55      0.28        11\n",
      "         227       0.38      0.53      0.44        15\n",
      "         228       0.12      0.19      0.15        16\n",
      "         229       0.03      0.14      0.05        14\n",
      "         230       0.05      0.18      0.07        17\n",
      "         231       0.67      0.38      0.48        16\n",
      "         232       0.03      0.07      0.04        14\n",
      "         233       0.59      0.50      0.54        34\n",
      "         234       0.05      0.14      0.07         7\n",
      "         235       0.43      0.30      0.35        10\n",
      "         236       0.54      0.65      0.59        20\n",
      "         237       0.11      0.13      0.12        31\n",
      "         238       0.05      0.18      0.08        17\n",
      "         239       0.03      0.14      0.05         7\n",
      "         240       0.38      0.53      0.44        15\n",
      "         241       0.04      0.18      0.07        17\n",
      "         242       0.33      0.14      0.20        14\n",
      "         243       0.00      0.00      0.00         1\n",
      "         244       0.09      0.41      0.15        17\n",
      "         245       0.04      0.15      0.06        13\n",
      "         246       0.62      0.80      0.70        20\n",
      "         247       0.04      0.38      0.07         8\n",
      "         248       0.02      0.10      0.03        10\n",
      "         249       0.00      0.00      0.00         5\n",
      "         250       0.25      0.33      0.29         9\n",
      "         251       0.02      0.50      0.04         6\n",
      "         252       0.00      0.00      0.00         9\n",
      "         253       0.11      0.55      0.18        11\n",
      "         254       0.05      0.12      0.07         8\n",
      "         255       0.13      0.17      0.15       112\n",
      "         256       0.05      0.11      0.06        19\n",
      "         257       0.14      0.25      0.18         8\n",
      "         258       0.52      0.79      0.62        19\n",
      "         259       0.03      0.07      0.04        15\n",
      "         260       0.09      0.18      0.12        22\n",
      "         261       0.08      0.26      0.13        19\n",
      "         262       0.12      0.55      0.20        11\n",
      "         263       0.36      0.31      0.34       147\n",
      "         264       0.03      0.33      0.06         3\n",
      "         265       0.08      0.22      0.11         9\n",
      "         266       0.50      0.20      0.29         5\n",
      "         267       0.05      0.17      0.07         6\n",
      "         268       0.24      0.49      0.32        35\n",
      "         269       0.40      0.29      0.33         7\n",
      "         270       0.21      0.50      0.30         6\n",
      "         271       0.14      0.47      0.21        19\n",
      "         272       0.04      0.25      0.07        12\n",
      "         273       0.77      0.79      0.78        42\n",
      "         274       0.50      0.50      0.50         6\n",
      "         275       0.09      0.20      0.12        10\n",
      "         276       0.07      0.11      0.09        18\n",
      "         277       0.46      0.71      0.56        17\n",
      "         278       0.17      0.40      0.24        15\n",
      "         279       0.54      0.88      0.67        17\n",
      "         280       0.05      0.22      0.09         9\n",
      "         281       0.69      0.62      0.65        29\n",
      "         282       0.42      0.81      0.55        16\n",
      "         283       0.11      0.35      0.17        17\n",
      "         284       0.20      0.33      0.25         3\n",
      "         285       0.58      0.79      0.67        47\n",
      "         286       0.11      0.64      0.19        11\n",
      "         287       0.32      0.35      0.33        17\n",
      "         288       0.11      0.12      0.12         8\n",
      "         289       0.00      0.00      0.00         5\n",
      "         290       0.05      0.13      0.07        15\n",
      "         291       0.19      0.50      0.28        10\n",
      "         292       0.08      0.12      0.10         8\n",
      "         293       0.33      0.72      0.45        18\n",
      "         294       0.14      0.13      0.14        23\n",
      "         295       0.06      0.17      0.09         6\n",
      "         296       0.12      0.33      0.18         3\n",
      "         297       0.07      0.38      0.12         8\n",
      "         298       0.03      1.00      0.06         1\n",
      "         299       0.07      0.50      0.12         4\n",
      "         300       0.07      0.27      0.11        15\n",
      "         301       0.03      0.07      0.04        14\n",
      "         302       0.00      0.00      0.00         5\n",
      "         303       0.48      0.93      0.64        15\n",
      "         304       0.23      0.58      0.33        12\n",
      "         305       0.73      0.73      0.73        30\n",
      "         306       0.38      0.53      0.44        49\n",
      "         307       0.06      0.43      0.11         7\n",
      "         308       0.00      0.00      0.00         8\n",
      "         309       0.03      0.08      0.04        12\n",
      "         310       0.31      0.45      0.37        20\n",
      "         311       0.14      1.00      0.24         4\n",
      "         312       0.38      0.33      0.35         9\n",
      "         313       0.04      0.07      0.05        14\n",
      "         314       0.35      0.29      0.32        21\n",
      "         315       0.38      0.73      0.50        11\n",
      "         316       0.33      0.67      0.44         3\n",
      "         317       0.11      0.21      0.15        14\n",
      "         318       0.00      0.00      0.00         7\n",
      "         319       0.21      0.90      0.35        10\n",
      "         320       0.16      0.21      0.18        28\n",
      "         321       0.37      0.78      0.50         9\n",
      "         322       0.16      0.75      0.26         8\n",
      "         323       0.52      0.48      0.49       101\n",
      "         324       0.02      0.12      0.03         8\n",
      "         325       0.02      0.17      0.04         6\n",
      "         326       0.06      0.15      0.09        13\n",
      "         327       0.00      0.00      0.00        13\n",
      "         328       0.43      0.70      0.53        37\n",
      "         329       0.19      0.62      0.29         8\n",
      "         330       0.00      0.00      0.00         6\n",
      "         331       0.00      0.00      0.00         4\n",
      "         332       0.00      0.00      0.00         3\n",
      "         333       0.05      0.33      0.09         6\n",
      "         334       0.50      0.33      0.40         9\n",
      "         335       0.17      0.25      0.20         8\n",
      "         336       0.12      0.25      0.17         4\n",
      "         337       0.12      0.30      0.18        10\n",
      "         338       0.22      0.80      0.35        10\n",
      "         339       0.00      0.00      0.00         4\n",
      "         340       0.40      0.50      0.44        16\n",
      "         341       0.00      0.00      0.00         2\n",
      "         342       0.00      0.00      0.00         1\n",
      "         343       0.00      0.00      0.00         7\n",
      "         344       0.00      0.00      0.00         3\n",
      "         345       0.73      0.80      0.76        10\n",
      "         346       0.00      0.00      0.00         5\n",
      "         347       0.42      0.57      0.48        14\n",
      "         348       0.00      0.00      0.00         1\n",
      "         349       0.27      0.78      0.40         9\n",
      "         350       0.00      0.00      0.00         1\n",
      "         351       0.05      0.50      0.08         2\n",
      "         352       0.73      0.73      0.73        26\n",
      "         353       0.50      0.33      0.40         3\n",
      "         354       0.02      0.25      0.04         4\n",
      "         355       0.04      0.20      0.06        10\n",
      "         356       0.04      0.07      0.05        14\n",
      "         357       0.31      0.75      0.44        16\n",
      "         358       0.17      0.22      0.19         9\n",
      "         359       0.24      0.78      0.37        18\n",
      "         360       0.04      0.12      0.06        16\n",
      "         361       0.30      0.79      0.43        14\n",
      "         362       0.04      0.20      0.06         5\n",
      "         363       0.56      1.00      0.71         5\n",
      "         364       0.00      0.00      0.00         3\n",
      "         365       0.00      0.00      0.00         9\n",
      "         366       0.69      0.75      0.72        12\n",
      "         367       0.20      0.56      0.29         9\n",
      "         368       0.00      0.00      0.00         0\n",
      "         369       0.00      0.00      0.00         7\n",
      "         370       0.00      0.00      0.00         6\n",
      "         371       0.04      0.38      0.07        13\n",
      "         372       0.75      0.43      0.55         7\n",
      "         373       0.30      0.35      0.32        17\n",
      "         374       0.00      0.00      0.00         4\n",
      "         375       0.04      0.33      0.07         3\n",
      "         376       0.00      0.00      0.00         6\n",
      "         377       0.12      0.15      0.14        13\n",
      "         378       0.00      0.00      0.00         8\n",
      "         379       0.00      0.00      0.00         3\n",
      "         380       0.53      0.45      0.49        20\n",
      "         381       0.00      0.00      0.00        16\n",
      "         382       0.00      0.00      0.00        24\n",
      "         383       0.52      0.42      0.47        33\n",
      "         384       0.00      0.00      0.00        17\n",
      "         385       0.00      0.00      0.00         6\n",
      "         386       0.00      0.00      0.00         4\n",
      "         387       0.05      0.11      0.07         9\n",
      "         388       0.25      0.33      0.29         9\n",
      "         389       0.00      0.00      0.00         4\n",
      "         390       0.59      0.72      0.65        18\n",
      "         391       0.15      0.30      0.20        10\n",
      "         392       0.00      0.00      0.00         3\n",
      "         393       0.00      0.00      0.00        17\n",
      "         394       0.00      0.00      0.00        10\n",
      "         395       0.23      0.42      0.29        12\n",
      "         396       0.03      0.14      0.04         7\n",
      "         397       0.28      0.37      0.32        19\n",
      "         398       0.43      0.72      0.54        18\n",
      "         399       0.10      0.11      0.11         9\n",
      "         400       0.19      0.60      0.29         5\n",
      "         401       0.24      0.46      0.32        13\n",
      "         402       0.05      0.17      0.08         6\n",
      "         403       0.10      0.50      0.17         2\n",
      "         404       0.00      0.00      0.00         1\n",
      "         405       0.14      0.38      0.20         8\n",
      "         406       0.13      0.40      0.20         5\n",
      "         407       0.07      0.33      0.12         3\n",
      "         408       0.06      0.25      0.10         4\n",
      "         409       0.00      0.00      0.00         3\n",
      "         410       0.00      0.00      0.00         2\n",
      "         411       0.21      0.45      0.29        11\n",
      "         412       0.17      0.50      0.25         6\n",
      "         413       0.10      0.18      0.13        34\n",
      "         414       0.00      0.00      0.00         8\n",
      "         415       0.08      1.00      0.14         1\n",
      "         416       0.25      0.60      0.35         5\n",
      "         417       0.08      0.17      0.11         6\n",
      "         418       0.00      0.00      0.00         9\n",
      "         419       0.15      0.36      0.21        14\n",
      "         420       0.00      0.00      0.00         2\n",
      "         421       0.18      0.40      0.25         5\n",
      "         422       0.12      0.83      0.21         6\n",
      "         423       0.00      0.00      0.00         4\n",
      "         424       1.00      1.00      1.00         1\n",
      "         425       0.00      0.00      0.00         2\n",
      "         426       0.14      0.38      0.20        13\n",
      "         427       0.86      0.86      0.86        21\n",
      "         428       0.78      0.78      0.78         9\n",
      "         429       0.00      0.00      0.00         1\n",
      "         430       0.05      0.25      0.08         4\n",
      "         431       0.21      0.30      0.25        23\n",
      "         432       0.00      0.00      0.00         1\n",
      "         433       0.00      0.00      0.00         5\n",
      "         434       0.20      0.50      0.29         2\n",
      "         435       0.14      0.50      0.22         2\n",
      "         436       0.08      0.23      0.12        13\n",
      "         437       0.00      0.00      0.00         7\n",
      "         438       0.29      0.67      0.40         9\n",
      "         439       0.02      0.14      0.03         7\n",
      "         440       0.06      0.20      0.10        10\n",
      "         441       0.17      0.33      0.22         6\n",
      "         442       0.08      0.18      0.11        11\n",
      "         443       0.10      0.09      0.10        11\n",
      "         444       0.05      0.07      0.06        14\n",
      "         445       0.00      0.00      0.00         3\n",
      "         446       0.27      1.00      0.43         3\n",
      "         447       0.02      0.25      0.03         4\n",
      "         448       0.50      0.55      0.52        11\n",
      "         449       0.29      0.56      0.38         9\n",
      "         450       0.12      0.33      0.17         6\n",
      "         451       0.03      0.20      0.06         5\n",
      "         452       0.00      0.00      0.00         3\n",
      "         453       0.00      0.00      0.00        17\n",
      "         454       0.38      0.41      0.39        22\n",
      "         455       0.07      0.33      0.12        12\n",
      "         456       0.00      0.00      0.00         0\n",
      "         457       0.07      0.43      0.12         7\n",
      "         458       0.03      0.33      0.06         3\n",
      "         459       0.05      0.33      0.08         3\n",
      "         460       0.00      0.00      0.00         1\n",
      "         461       0.19      0.60      0.29         5\n",
      "         462       0.00      0.00      0.00         2\n",
      "         463       0.05      0.29      0.09        14\n",
      "         464       0.19      0.33      0.24         9\n",
      "         465       0.18      0.33      0.24         6\n",
      "         466       0.05      0.20      0.08         5\n",
      "         467       0.33      0.71      0.45         7\n",
      "         468       0.33      0.10      0.15        20\n",
      "         469       0.00      0.00      0.00         8\n",
      "         470       0.57      1.00      0.73         8\n",
      "         471       0.36      0.56      0.43         9\n",
      "         472       0.15      0.43      0.22        14\n",
      "         473       0.43      0.60      0.50         5\n",
      "         474       0.00      0.00      0.00         2\n",
      "         475       0.00      0.00      0.00         8\n",
      "         476       0.21      0.50      0.30         6\n",
      "         477       0.33      0.67      0.44        12\n",
      "         478       1.00      0.25      0.40         4\n",
      "         479       0.17      0.11      0.13         9\n",
      "         480       0.50      0.80      0.62         5\n",
      "         481       0.15      0.44      0.23         9\n",
      "         482       0.00      0.00      0.00         4\n",
      "         483       0.31      0.77      0.44        13\n",
      "         484       0.00      0.00      0.00         6\n",
      "         485       0.06      0.11      0.08         9\n",
      "         486       0.05      0.20      0.08         5\n",
      "         487       0.20      0.22      0.21         9\n",
      "         488       0.07      0.11      0.09         9\n",
      "         489       0.12      0.50      0.19         6\n",
      "         490       0.00      0.00      0.00        14\n",
      "         491       0.06      0.14      0.09        14\n",
      "         492       0.00      0.00      0.00         3\n",
      "         493       0.00      0.00      0.00         8\n",
      "         494       0.19      0.21      0.20        14\n",
      "         495       0.75      0.50      0.60         6\n",
      "         496       0.00      0.00      0.00         2\n",
      "         497       0.07      0.20      0.10         5\n",
      "         498       0.33      0.20      0.25        10\n",
      "         499       0.09      0.20      0.13        15\n",
      "\n",
      "   micro avg       0.26      0.47      0.33     18699\n",
      "   macro avg       0.20      0.36      0.24     18699\n",
      "weighted avg       0.33      0.47      0.38     18699\n",
      " samples avg       0.34      0.45      0.34     18699\n",
      "\n",
      "Time taken to run this cell : 0:08:48.564091\n"
     ]
    }
   ],
   "source": [
    "#applying the best alpha\n",
    "start = datetime.now()\n",
    "classifier = OneVsRestClassifier(SGDClassifier(loss='log', alpha=0.0001, penalty='l1'))\n",
    "classifier.fit(x_train_multilabel, y_train)\n",
    "predictions = classifier.predict (x_test_multilabel)\n",
    "\n",
    "\n",
    "print(\"Accuracy :\",metrics.accuracy_score(y_test, predictions))\n",
    "print(\"Hamming loss \",metrics.hamming_loss(y_test,predictions))\n",
    "\n",
    "\n",
    "precision = precision_score(y_test, predictions, average='micro')\n",
    "recall = recall_score(y_test, predictions, average='micro')\n",
    "f1 = f1_score(y_test, predictions, average='micro')\n",
    " \n",
    "print(\"Micro-average quality numbers\")\n",
    "print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
    "\n",
    "precision = precision_score(y_test, predictions, average='macro')\n",
    "recall = recall_score(y_test, predictions, average='macro')\n",
    "f1 = f1_score(y_test, predictions, average='macro')\n",
    " \n",
    "print(\"Macro-average quality numbers\")\n",
    "print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
    "\n",
    "print (metrics.classification_report(y_test, predictions))\n",
    "print(\"Time taken to run this cell :\", datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "SO_Tag_Predictor.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
