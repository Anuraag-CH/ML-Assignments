{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN_MNIST.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4J_lZxw7EI5",
        "colab_type": "text"
      },
      "source": [
        "#3-Convolution Layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUUMVd8G7JnR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2030376c-6d2f-4dd4-bb29-9ae25c36fd7d"
      },
      "source": [
        "# Credits: https://github.com/keras-team/keras/blob/master/examples/mnist_cnn.py\n",
        "\n",
        "\n",
        "from __future__ import print_function\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten,BatchNormalization\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras import backend as K\n",
        "\n",
        "batch_size = 128\n",
        "num_classes = 10\n",
        "epochs = 12\n",
        "\n",
        "# input image dimensions\n",
        "img_rows, img_cols = 28, 28\n",
        "\n",
        "# the data, split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "print(x_train.shape)\n",
        "\n",
        "if K.image_data_format() == 'channels_first':\n",
        "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
        "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
        "    input_shape = (1, img_rows, img_cols)\n",
        "else:\n",
        "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "    input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(128, kernel_size=(2, 2),activation='relu',input_shape=input_shape,kernel_initializer='glorot_normal'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "\n",
        "model.add(Conv2D(32, (5, 5), activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss=keras.losses.categorical_crossentropy,optimizer=keras.optimizers.Adam(),metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train,batch_size=batch_size,epochs=epochs,verbose=1,validation_data=(x_test, y_test))\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 2s 0us/step\n",
            "(60000, 28, 28)\n",
            "x_train shape: (60000, 28, 28, 1)\n",
            "60000 train samples\n",
            "10000 test samples\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4479: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 27, 27, 128)       640       \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 27, 27, 128)       512       \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 25, 25, 64)        73792     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 12, 12, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 8, 8, 32)          51232     \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 8, 8, 32)          128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 4, 4, 32)          0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 4, 4, 32)          0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 128)               65664     \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 193,258\n",
            "Trainable params: 192,938\n",
            "Non-trainable params: 320\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/12\n",
            "60000/60000 [==============================] - 22s 374us/step - loss: 0.2054 - acc: 0.9366 - val_loss: 0.0603 - val_acc: 0.9810\n",
            "Epoch 2/12\n",
            "60000/60000 [==============================] - 19s 310us/step - loss: 0.0615 - acc: 0.9814 - val_loss: 0.0320 - val_acc: 0.9896\n",
            "Epoch 3/12\n",
            "60000/60000 [==============================] - 19s 310us/step - loss: 0.0451 - acc: 0.9860 - val_loss: 0.0296 - val_acc: 0.9907\n",
            "Epoch 4/12\n",
            "60000/60000 [==============================] - 19s 311us/step - loss: 0.0395 - acc: 0.9873 - val_loss: 0.0203 - val_acc: 0.9936\n",
            "Epoch 5/12\n",
            "60000/60000 [==============================] - 19s 311us/step - loss: 0.0314 - acc: 0.9900 - val_loss: 0.0443 - val_acc: 0.9869\n",
            "Epoch 6/12\n",
            "60000/60000 [==============================] - 19s 312us/step - loss: 0.0287 - acc: 0.9912 - val_loss: 0.0398 - val_acc: 0.9877\n",
            "Epoch 7/12\n",
            "60000/60000 [==============================] - 19s 311us/step - loss: 0.0268 - acc: 0.9914 - val_loss: 0.0283 - val_acc: 0.9912\n",
            "Epoch 8/12\n",
            "60000/60000 [==============================] - 19s 310us/step - loss: 0.0243 - acc: 0.9924 - val_loss: 0.0235 - val_acc: 0.9922\n",
            "Epoch 9/12\n",
            "60000/60000 [==============================] - 19s 310us/step - loss: 0.0218 - acc: 0.9928 - val_loss: 0.0302 - val_acc: 0.9917\n",
            "Epoch 10/12\n",
            "60000/60000 [==============================] - 19s 310us/step - loss: 0.0207 - acc: 0.9932 - val_loss: 0.0278 - val_acc: 0.9914\n",
            "Epoch 11/12\n",
            "60000/60000 [==============================] - 19s 311us/step - loss: 0.0199 - acc: 0.9934 - val_loss: 0.0219 - val_acc: 0.9940\n",
            "Epoch 12/12\n",
            "60000/60000 [==============================] - 19s 311us/step - loss: 0.0176 - acc: 0.9940 - val_loss: 0.0282 - val_acc: 0.9919\n",
            "Test loss: 0.028177190259779036\n",
            "Test accuracy: 0.9919\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Arn1pkv8mtA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "aa581023-53e0-43ad-e128-7f29fb7d2d87"
      },
      "source": [
        "# Credits: https://github.com/keras-team/keras/blob/master/examples/mnist_cnn.py\n",
        "\n",
        "\n",
        "from __future__ import print_function\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten,BatchNormalization\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras import backend as K\n",
        "\n",
        "batch_size = 128\n",
        "num_classes = 10\n",
        "epochs = 12\n",
        "\n",
        "# input image dimensions\n",
        "img_rows, img_cols = 28, 28\n",
        "\n",
        "# the data, split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "print(x_train.shape)\n",
        "\n",
        "if K.image_data_format() == 'channels_first':\n",
        "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
        "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
        "    input_shape = (1, img_rows, img_cols)\n",
        "else:\n",
        "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "    input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(128, kernel_size=(2, 2),activation='relu',input_shape=input_shape,kernel_initializer='he_normal'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), activation='sigmoid'))\n",
        "model.add(Conv2D(32, (5, 5), activation='sigmoid'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='sigmoid'))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss=keras.losses.categorical_crossentropy,optimizer=keras.optimizers.Adagrad(),metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train,batch_size=batch_size,epochs=epochs,verbose=1,validation_data=(x_test, y_test))\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n",
            "x_train shape: (60000, 28, 28, 1)\n",
            "60000 train samples\n",
            "10000 test samples\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_4 (Conv2D)            (None, 27, 27, 128)       640       \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 27, 27, 128)       512       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 13, 13, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 11, 11, 64)        73792     \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 7, 7, 32)          51232     \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 7, 7, 32)          128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 3, 3, 32)          0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 288)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 128)               36992     \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 164,586\n",
            "Trainable params: 164,266\n",
            "Non-trainable params: 320\n",
            "_________________________________________________________________\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/12\n",
            "60000/60000 [==============================] - 14s 233us/step - loss: 0.1210 - acc: 0.9708 - val_loss: 0.0464 - val_acc: 0.9880\n",
            "Epoch 2/12\n",
            "60000/60000 [==============================] - 13s 220us/step - loss: 0.0365 - acc: 0.9906 - val_loss: 0.0337 - val_acc: 0.9912\n",
            "Epoch 3/12\n",
            "60000/60000 [==============================] - 13s 220us/step - loss: 0.0247 - acc: 0.9939 - val_loss: 0.0308 - val_acc: 0.9917\n",
            "Epoch 4/12\n",
            "60000/60000 [==============================] - 13s 220us/step - loss: 0.0185 - acc: 0.9958 - val_loss: 0.0268 - val_acc: 0.9926\n",
            "Epoch 5/12\n",
            "60000/60000 [==============================] - 13s 220us/step - loss: 0.0141 - acc: 0.9970 - val_loss: 0.0282 - val_acc: 0.9921\n",
            "Epoch 6/12\n",
            "60000/60000 [==============================] - 13s 220us/step - loss: 0.0107 - acc: 0.9983 - val_loss: 0.0307 - val_acc: 0.9915\n",
            "Epoch 7/12\n",
            "60000/60000 [==============================] - 13s 220us/step - loss: 0.0086 - acc: 0.9989 - val_loss: 0.0258 - val_acc: 0.9928\n",
            "Epoch 8/12\n",
            "60000/60000 [==============================] - 13s 220us/step - loss: 0.0070 - acc: 0.9992 - val_loss: 0.0226 - val_acc: 0.9939\n",
            "Epoch 9/12\n",
            "60000/60000 [==============================] - 13s 220us/step - loss: 0.0058 - acc: 0.9995 - val_loss: 0.0234 - val_acc: 0.9932\n",
            "Epoch 10/12\n",
            "60000/60000 [==============================] - 13s 220us/step - loss: 0.0049 - acc: 0.9996 - val_loss: 0.0220 - val_acc: 0.9943\n",
            "Epoch 11/12\n",
            "60000/60000 [==============================] - 13s 220us/step - loss: 0.0041 - acc: 0.9998 - val_loss: 0.0230 - val_acc: 0.9932\n",
            "Epoch 12/12\n",
            "60000/60000 [==============================] - 13s 220us/step - loss: 0.0036 - acc: 0.9998 - val_loss: 0.0240 - val_acc: 0.9930\n",
            "Test loss: 0.023953616692498327\n",
            "Test accuracy: 0.993\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKHwhLRV9JNM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 986
        },
        "outputId": "f19b7cda-753a-4fd7-8daa-370b0c0f43e5"
      },
      "source": [
        "# Credits: https://github.com/keras-team/keras/blob/master/examples/mnist_cnn.py\n",
        "\n",
        "\n",
        "from __future__ import print_function\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten,BatchNormalization\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras import backend as K\n",
        "\n",
        "batch_size = 128\n",
        "num_classes = 10\n",
        "epochs = 12\n",
        "\n",
        "# input image dimensions\n",
        "img_rows, img_cols = 28, 28\n",
        "\n",
        "# the data, split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "print(x_train.shape)\n",
        "\n",
        "if K.image_data_format() == 'channels_first':\n",
        "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
        "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
        "    input_shape = (1, img_rows, img_cols)\n",
        "else:\n",
        "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "    input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(128, kernel_size=(2, 2),activation='tanh',input_shape=input_shape,kernel_initializer='RandomNormal'))\n",
        "model.add(Conv2D(64, (3, 3), activation='tanh'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Conv2D(32, (5, 5), activation='tanh'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss=keras.losses.categorical_crossentropy,optimizer=keras.optimizers.Adadelta(),metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train,batch_size=batch_size,epochs=epochs,verbose=1,validation_data=(x_test, y_test))\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n",
            "x_train shape: (60000, 28, 28, 1)\n",
            "60000 train samples\n",
            "10000 test samples\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_7 (Conv2D)            (None, 27, 27, 128)       640       \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 25, 25, 64)        73792     \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 25, 25, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 21, 21, 32)        51232     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 10, 10, 32)        0         \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 3200)              0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 128)               409728    \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 536,682\n",
            "Trainable params: 536,682\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/12\n",
            "60000/60000 [==============================] - 19s 316us/step - loss: 0.2839 - acc: 0.9129 - val_loss: 0.0724 - val_acc: 0.9775\n",
            "Epoch 2/12\n",
            "60000/60000 [==============================] - 18s 303us/step - loss: 0.1130 - acc: 0.9661 - val_loss: 0.0507 - val_acc: 0.9844\n",
            "Epoch 3/12\n",
            "60000/60000 [==============================] - 18s 302us/step - loss: 0.0850 - acc: 0.9748 - val_loss: 0.0498 - val_acc: 0.9845\n",
            "Epoch 4/12\n",
            "60000/60000 [==============================] - 18s 303us/step - loss: 0.0677 - acc: 0.9795 - val_loss: 0.0390 - val_acc: 0.9886\n",
            "Epoch 5/12\n",
            "60000/60000 [==============================] - 18s 303us/step - loss: 0.0577 - acc: 0.9817 - val_loss: 0.0348 - val_acc: 0.9889\n",
            "Epoch 6/12\n",
            "60000/60000 [==============================] - 18s 303us/step - loss: 0.0522 - acc: 0.9840 - val_loss: 0.0348 - val_acc: 0.9894\n",
            "Epoch 7/12\n",
            "60000/60000 [==============================] - 18s 302us/step - loss: 0.0453 - acc: 0.9857 - val_loss: 0.0398 - val_acc: 0.9895\n",
            "Epoch 8/12\n",
            "60000/60000 [==============================] - 18s 303us/step - loss: 0.0417 - acc: 0.9871 - val_loss: 0.0349 - val_acc: 0.9895\n",
            "Epoch 9/12\n",
            "60000/60000 [==============================] - 18s 305us/step - loss: 0.0383 - acc: 0.9879 - val_loss: 0.0371 - val_acc: 0.9892\n",
            "Epoch 10/12\n",
            "60000/60000 [==============================] - 18s 303us/step - loss: 0.0331 - acc: 0.9896 - val_loss: 0.0329 - val_acc: 0.9910\n",
            "Epoch 11/12\n",
            "60000/60000 [==============================] - 18s 302us/step - loss: 0.0297 - acc: 0.9907 - val_loss: 0.0347 - val_acc: 0.9914\n",
            "Epoch 12/12\n",
            "60000/60000 [==============================] - 18s 302us/step - loss: 0.0289 - acc: 0.9905 - val_loss: 0.0299 - val_acc: 0.9904\n",
            "Test loss: 0.02994774283710726\n",
            "Test accuracy: 0.9904\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWqEdk9x9znK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 884
        },
        "outputId": "948d43a3-ca23-4cae-be97-af1859e78059"
      },
      "source": [
        "# Credits: https://github.com/keras-team/keras/blob/master/examples/mnist_cnn.py\n",
        "\n",
        "\n",
        "from __future__ import print_function\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten,BatchNormalization\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras import backend as K\n",
        "\n",
        "batch_size = 128\n",
        "num_classes = 10\n",
        "epochs = 12\n",
        "\n",
        "# input image dimensions\n",
        "img_rows, img_cols = 28, 28\n",
        "\n",
        "# the data, split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "print(x_train.shape)\n",
        "\n",
        "if K.image_data_format() == 'channels_first':\n",
        "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
        "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
        "    input_shape = (1, img_rows, img_cols)\n",
        "else:\n",
        "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "    input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(128, kernel_size=(2, 2),activation='relu',input_shape=input_shape,kernel_initializer='glorot_normal'))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(Conv2D(32, (5, 5), activation='relu'))\n",
        "\n",
        "\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss=keras.losses.categorical_crossentropy,optimizer=keras.optimizers.sgd(),metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train,batch_size=batch_size,epochs=epochs,verbose=1,validation_data=(x_test, y_test))\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n",
            "x_train shape: (60000, 28, 28, 1)\n",
            "60000 train samples\n",
            "10000 test samples\n",
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_10 (Conv2D)           (None, 27, 27, 128)       640       \n",
            "_________________________________________________________________\n",
            "conv2d_11 (Conv2D)           (None, 25, 25, 64)        73792     \n",
            "_________________________________________________________________\n",
            "conv2d_12 (Conv2D)           (None, 21, 21, 32)        51232     \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 14112)             0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 128)               1806464   \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 1,933,418\n",
            "Trainable params: 1,933,418\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/12\n",
            "60000/60000 [==============================] - 17s 276us/step - loss: 0.6677 - acc: 0.8096 - val_loss: 0.2495 - val_acc: 0.9236\n",
            "Epoch 2/12\n",
            "60000/60000 [==============================] - 16s 266us/step - loss: 0.2070 - acc: 0.9392 - val_loss: 0.1576 - val_acc: 0.9545\n",
            "Epoch 3/12\n",
            "60000/60000 [==============================] - 16s 267us/step - loss: 0.1461 - acc: 0.9555 - val_loss: 0.1358 - val_acc: 0.9613\n",
            "Epoch 4/12\n",
            "60000/60000 [==============================] - 16s 266us/step - loss: 0.1135 - acc: 0.9669 - val_loss: 0.1020 - val_acc: 0.9663\n",
            "Epoch 5/12\n",
            "60000/60000 [==============================] - 16s 266us/step - loss: 0.0935 - acc: 0.9720 - val_loss: 0.0806 - val_acc: 0.9746\n",
            "Epoch 6/12\n",
            "60000/60000 [==============================] - 16s 267us/step - loss: 0.0785 - acc: 0.9767 - val_loss: 0.0763 - val_acc: 0.9760\n",
            "Epoch 7/12\n",
            "60000/60000 [==============================] - 16s 267us/step - loss: 0.0676 - acc: 0.9800 - val_loss: 0.0819 - val_acc: 0.9749\n",
            "Epoch 8/12\n",
            "60000/60000 [==============================] - 16s 266us/step - loss: 0.0576 - acc: 0.9829 - val_loss: 0.0617 - val_acc: 0.9813\n",
            "Epoch 9/12\n",
            "60000/60000 [==============================] - 16s 266us/step - loss: 0.0502 - acc: 0.9848 - val_loss: 0.0691 - val_acc: 0.9780\n",
            "Epoch 10/12\n",
            "60000/60000 [==============================] - 16s 267us/step - loss: 0.0447 - acc: 0.9869 - val_loss: 0.0636 - val_acc: 0.9799\n",
            "Epoch 11/12\n",
            "60000/60000 [==============================] - 16s 266us/step - loss: 0.0402 - acc: 0.9878 - val_loss: 0.0588 - val_acc: 0.9828\n",
            "Epoch 12/12\n",
            "60000/60000 [==============================] - 16s 266us/step - loss: 0.0360 - acc: 0.9894 - val_loss: 0.0660 - val_acc: 0.9790\n",
            "Test loss: 0.06601626948665362\n",
            "Test accuracy: 0.979\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCIkxFA-_eYC",
        "colab_type": "text"
      },
      "source": [
        "#5-Layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5Mxgg1H_gxE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ce384ef6-fec8-4a3e-f7d5-d8cc5b13ea18"
      },
      "source": [
        "# Credits: https://github.com/keras-team/keras/blob/master/examples/mnist_cnn.py\n",
        "\n",
        "\n",
        "from __future__ import print_function\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras import backend as K\n",
        "\n",
        "batch_size = 512\n",
        "num_classes = 10\n",
        "epochs = 10\n",
        "\n",
        "# input image dimensions\n",
        "img_rows, img_cols = 28, 28\n",
        "\n",
        "# the data, split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "print(x_train.shape)\n",
        "\n",
        "if K.image_data_format() == 'channels_first':\n",
        "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
        "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
        "    input_shape = (1, img_rows, img_cols)\n",
        "else:\n",
        "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "    input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(128, kernel_size=(2, 2),activation='relu',input_shape=input_shape,kernel_initializer='RandomNormal'))\n",
        "model.add(Conv2D(96, (3, 3), activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv2D(84, (5, 5),padding='same', activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(64, (3, 3),padding='same', activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "\n",
        "\n",
        "model.add(Conv2D(32, (7, 7),padding='same', activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss=keras.losses.categorical_crossentropy,optimizer=keras.optimizers.Adam(),metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train,batch_size=batch_size,epochs=epochs,verbose=1,validation_data=(x_test, y_test))\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n",
            "x_train shape: (60000, 28, 28, 1)\n",
            "60000 train samples\n",
            "10000 test samples\n",
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_13 (Conv2D)           (None, 27, 27, 128)       640       \n",
            "_________________________________________________________________\n",
            "conv2d_14 (Conv2D)           (None, 25, 25, 96)        110688    \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 25, 25, 96)        384       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2 (None, 12, 12, 96)        0         \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 12, 12, 96)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_15 (Conv2D)           (None, 12, 12, 84)        201684    \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 12, 12, 84)        336       \n",
            "_________________________________________________________________\n",
            "conv2d_16 (Conv2D)           (None, 12, 12, 64)        48448     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2 (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_17 (Conv2D)           (None, 6, 6, 32)          100384    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_8 (MaxPooling2 (None, 3, 3, 32)          0         \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 3, 3, 32)          0         \n",
            "_________________________________________________________________\n",
            "flatten_5 (Flatten)          (None, 288)               0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 128)               36992     \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 500,846\n",
            "Trainable params: 500,486\n",
            "Non-trainable params: 360\n",
            "_________________________________________________________________\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 26s 430us/step - loss: 0.7600 - acc: 0.7493 - val_loss: 0.1371 - val_acc: 0.9561\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 23s 377us/step - loss: 0.1271 - acc: 0.9645 - val_loss: 0.0475 - val_acc: 0.9857\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 23s 376us/step - loss: 0.0804 - acc: 0.9772 - val_loss: 0.0374 - val_acc: 0.9903\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 23s 376us/step - loss: 0.0699 - acc: 0.9806 - val_loss: 0.0260 - val_acc: 0.9927\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 23s 376us/step - loss: 0.0552 - acc: 0.9845 - val_loss: 0.0252 - val_acc: 0.9925\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 23s 377us/step - loss: 0.0461 - acc: 0.9870 - val_loss: 0.0355 - val_acc: 0.9906\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 23s 375us/step - loss: 0.0419 - acc: 0.9886 - val_loss: 0.0245 - val_acc: 0.9922\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 23s 376us/step - loss: 0.0383 - acc: 0.9897 - val_loss: 0.0228 - val_acc: 0.9941\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 23s 376us/step - loss: 0.0346 - acc: 0.9902 - val_loss: 0.0227 - val_acc: 0.9936\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 23s 376us/step - loss: 0.0296 - acc: 0.9915 - val_loss: 0.0272 - val_acc: 0.9921\n",
            "Test loss: 0.02720410974002425\n",
            "Test accuracy: 0.9921\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdPVDJUwE7jU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8cf2647f-3dfb-41dd-8940-2713aa1cb6b3"
      },
      "source": [
        "# Credits: https://github.com/keras-team/keras/blob/master/examples/mnist_cnn.py\n",
        "\n",
        "\n",
        "from __future__ import print_function\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras import backend as K\n",
        "\n",
        "batch_size = 512\n",
        "num_classes = 10\n",
        "epochs = 10\n",
        "\n",
        "# input image dimensions\n",
        "img_rows, img_cols = 28, 28\n",
        "\n",
        "# the data, split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "print(x_train.shape)\n",
        "\n",
        "if K.image_data_format() == 'channels_first':\n",
        "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
        "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
        "    input_shape = (1, img_rows, img_cols)\n",
        "else:\n",
        "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "    input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(128, kernel_size=(2, 2),activation='relu',input_shape=input_shape,kernel_initializer='RandomNormal'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Conv2D(96, (3, 3), activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "\n",
        "model.add(Conv2D(84, (5, 5),padding='same', activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(64, (3, 3),padding='same', activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Conv2D(32, (7, 7),padding='same', activation='relu'))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss=keras.losses.categorical_crossentropy,optimizer=keras.optimizers.Adam(),metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train,batch_size=batch_size,epochs=epochs,verbose=1,validation_data=(x_test, y_test))\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n",
            "x_train shape: (60000, 28, 28, 1)\n",
            "60000 train samples\n",
            "10000 test samples\n",
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_18 (Conv2D)           (None, 27, 27, 128)       640       \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 27, 27, 128)       512       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_9 (MaxPooling2 (None, 13, 13, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_19 (Conv2D)           (None, 11, 11, 96)        110688    \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 11, 11, 96)        384       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_10 (MaxPooling (None, 5, 5, 96)          0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_9 (Batch (None, 5, 5, 96)          384       \n",
            "_________________________________________________________________\n",
            "conv2d_20 (Conv2D)           (None, 5, 5, 84)          201684    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_11 (MaxPooling (None, 2, 2, 84)          0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_10 (Batc (None, 2, 2, 84)          336       \n",
            "_________________________________________________________________\n",
            "conv2d_21 (Conv2D)           (None, 2, 2, 64)          48448     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_12 (MaxPooling (None, 1, 1, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_22 (Conv2D)           (None, 1, 1, 32)          100384    \n",
            "_________________________________________________________________\n",
            "flatten_6 (Flatten)          (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 128)               4224      \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 468,974\n",
            "Trainable params: 468,166\n",
            "Non-trainable params: 808\n",
            "_________________________________________________________________\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 17s 278us/step - loss: 0.4008 - acc: 0.8914 - val_loss: 0.0798 - val_acc: 0.9753\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 15s 244us/step - loss: 0.0344 - acc: 0.9900 - val_loss: 0.0834 - val_acc: 0.9755\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 15s 244us/step - loss: 0.0218 - acc: 0.9932 - val_loss: 0.0293 - val_acc: 0.9920\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 15s 244us/step - loss: 0.0127 - acc: 0.9963 - val_loss: 0.0267 - val_acc: 0.9922\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 15s 244us/step - loss: 0.0077 - acc: 0.9978 - val_loss: 0.0290 - val_acc: 0.9920\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 15s 243us/step - loss: 0.0070 - acc: 0.9980 - val_loss: 0.0393 - val_acc: 0.9885\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 15s 243us/step - loss: 0.0045 - acc: 0.9987 - val_loss: 0.0391 - val_acc: 0.9915\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 15s 244us/step - loss: 0.0050 - acc: 0.9982 - val_loss: 0.0405 - val_acc: 0.9899\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 15s 245us/step - loss: 0.0061 - acc: 0.9980 - val_loss: 0.0461 - val_acc: 0.9886\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 15s 243us/step - loss: 0.0056 - acc: 0.9984 - val_loss: 0.0405 - val_acc: 0.9901\n",
            "Test loss: 0.04050913316059923\n",
            "Test accuracy: 0.9901\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tb2SYQGcFg8j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7d8cf9b1-a743-4079-92fa-112bbe2e5eaa"
      },
      "source": [
        "# Credits: https://github.com/keras-team/keras/blob/master/examples/mnist_cnn.py\n",
        "\n",
        "\n",
        "from __future__ import print_function\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras import backend as K\n",
        "\n",
        "batch_size = 512\n",
        "num_classes = 10\n",
        "epochs = 10\n",
        "\n",
        "# input image dimensions\n",
        "img_rows, img_cols = 28, 28\n",
        "\n",
        "# the data, split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "print(x_train.shape)\n",
        "\n",
        "if K.image_data_format() == 'channels_first':\n",
        "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
        "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
        "    input_shape = (1, img_rows, img_cols)\n",
        "else:\n",
        "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "    input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(512, kernel_size=(2, 2),activation='sigmoid',input_shape=input_shape,kernel_initializer='he_normal'))\n",
        "model.add(Conv2D(256, (3, 3),padding='same', activation='sigmoid'))\n",
        "model.add(Conv2D(128, (5, 5),padding='same', activation='sigmoid'))\n",
        "\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "\n",
        "model.add(Conv2D(64, (3, 3),padding='same', activation='sigmoid'))\n",
        "model.add(Conv2D(32, (7, 7),padding='same', activation='sigmoid'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "\n",
        "\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='sigmoid'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss=keras.losses.categorical_crossentropy,optimizer=keras.optimizers.Adagrad(),metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train,batch_size=batch_size,epochs=epochs,verbose=1,validation_data=(x_test, y_test))\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n",
            "x_train shape: (60000, 28, 28, 1)\n",
            "60000 train samples\n",
            "10000 test samples\n",
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_23 (Conv2D)           (None, 27, 27, 512)       2560      \n",
            "_________________________________________________________________\n",
            "conv2d_24 (Conv2D)           (None, 27, 27, 256)       1179904   \n",
            "_________________________________________________________________\n",
            "conv2d_25 (Conv2D)           (None, 27, 27, 128)       819328    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_13 (MaxPooling (None, 13, 13, 128)       0         \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 13, 13, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_26 (Conv2D)           (None, 13, 13, 64)        73792     \n",
            "_________________________________________________________________\n",
            "conv2d_27 (Conv2D)           (None, 13, 13, 32)        100384    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_14 (MaxPooling (None, 6, 6, 32)          0         \n",
            "_________________________________________________________________\n",
            "dropout_10 (Dropout)         (None, 6, 6, 32)          0         \n",
            "_________________________________________________________________\n",
            "dropout_11 (Dropout)         (None, 6, 6, 32)          0         \n",
            "_________________________________________________________________\n",
            "flatten_7 (Flatten)          (None, 1152)              0         \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 128)               147584    \n",
            "_________________________________________________________________\n",
            "dropout_12 (Dropout)         (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 2,324,842\n",
            "Trainable params: 2,324,842\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 108s 2ms/step - loss: 2.3278 - acc: 0.1031 - val_loss: 2.3017 - val_acc: 0.1135\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 94s 2ms/step - loss: 2.3027 - acc: 0.1092 - val_loss: 2.3014 - val_acc: 0.1135\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 94s 2ms/step - loss: 2.3020 - acc: 0.1107 - val_loss: 2.3014 - val_acc: 0.1135\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 94s 2ms/step - loss: 2.3016 - acc: 0.1112 - val_loss: 2.3013 - val_acc: 0.1135\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 94s 2ms/step - loss: 2.3015 - acc: 0.1122 - val_loss: 2.3012 - val_acc: 0.1135\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 94s 2ms/step - loss: 2.3017 - acc: 0.1124 - val_loss: 2.3011 - val_acc: 0.1135\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 94s 2ms/step - loss: 2.3014 - acc: 0.1123 - val_loss: 2.3011 - val_acc: 0.1135\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 94s 2ms/step - loss: 2.3015 - acc: 0.1123 - val_loss: 2.3011 - val_acc: 0.1135\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 94s 2ms/step - loss: 2.3012 - acc: 0.1125 - val_loss: 2.3011 - val_acc: 0.1135\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 94s 2ms/step - loss: 2.3012 - acc: 0.1124 - val_loss: 2.3011 - val_acc: 0.1135\n",
            "Test loss: 2.301075766372681\n",
            "Test accuracy: 0.1135\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dlk4J1aaGOBi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 986
        },
        "outputId": "9ef7f3f4-40af-4000-d0da-1897977f8509"
      },
      "source": [
        "# Credits: https://github.com/keras-team/keras/blob/master/examples/mnist_cnn.py\n",
        "\n",
        "\n",
        "from __future__ import print_function\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras import backend as K\n",
        "\n",
        "batch_size = 512\n",
        "num_classes = 10\n",
        "epochs = 10\n",
        "\n",
        "# input image dimensions\n",
        "img_rows, img_cols = 28, 28\n",
        "\n",
        "# the data, split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "print(x_train.shape)\n",
        "\n",
        "if K.image_data_format() == 'channels_first':\n",
        "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
        "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
        "    input_shape = (1, img_rows, img_cols)\n",
        "else:\n",
        "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "    input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(256, kernel_size=(2, 2),padding='same',activation='tanh',input_shape=input_shape,kernel_initializer='glorot_normal'))\n",
        "model.add(Conv2D(128, (3, 3),padding='same', activation='tanh'))\n",
        "model.add(Conv2D(128, (3, 3),padding='same', activation='tanh'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "\n",
        "model.add(Conv2D(64, (5, 5),padding='same', activation='tanh'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(32, (7, 7),padding='same', activation='tanh'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='tanh'))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss=keras.losses.categorical_crossentropy,optimizer=keras.optimizers.Adadelta(),metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train,batch_size=batch_size,epochs=epochs,verbose=1,validation_data=(x_test, y_test))\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n",
            "x_train shape: (60000, 28, 28, 1)\n",
            "60000 train samples\n",
            "10000 test samples\n",
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_28 (Conv2D)           (None, 28, 28, 256)       1280      \n",
            "_________________________________________________________________\n",
            "conv2d_29 (Conv2D)           (None, 28, 28, 128)       295040    \n",
            "_________________________________________________________________\n",
            "conv2d_30 (Conv2D)           (None, 28, 28, 128)       147584    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_15 (MaxPooling (None, 14, 14, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_31 (Conv2D)           (None, 14, 14, 64)        204864    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_16 (MaxPooling (None, 7, 7, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_32 (Conv2D)           (None, 7, 7, 32)          100384    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_17 (MaxPooling (None, 3, 3, 32)          0         \n",
            "_________________________________________________________________\n",
            "flatten_8 (Flatten)          (None, 288)               0         \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 128)               36992     \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 787,434\n",
            "Trainable params: 787,434\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 50s 830us/step - loss: 0.3525 - acc: 0.8938 - val_loss: 0.1133 - val_acc: 0.9653\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 44s 728us/step - loss: 0.0633 - acc: 0.9821 - val_loss: 0.0732 - val_acc: 0.9748\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 44s 728us/step - loss: 0.0406 - acc: 0.9885 - val_loss: 0.0333 - val_acc: 0.9885\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 44s 729us/step - loss: 0.0224 - acc: 0.9936 - val_loss: 0.0376 - val_acc: 0.9883\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 44s 729us/step - loss: 0.0141 - acc: 0.9964 - val_loss: 0.0305 - val_acc: 0.9895\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 44s 729us/step - loss: 0.0083 - acc: 0.9983 - val_loss: 0.0236 - val_acc: 0.9917\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 44s 729us/step - loss: 0.0044 - acc: 0.9994 - val_loss: 0.0237 - val_acc: 0.9916\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 44s 729us/step - loss: 0.0023 - acc: 0.9998 - val_loss: 0.0219 - val_acc: 0.9922\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 44s 729us/step - loss: 0.0015 - acc: 0.9999 - val_loss: 0.0220 - val_acc: 0.9926\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 44s 729us/step - loss: 0.0010 - acc: 1.0000 - val_loss: 0.0219 - val_acc: 0.9925\n",
            "Test loss: 0.021898437683159137\n",
            "Test accuracy: 0.9925\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "deSsvm93Jwi6",
        "colab_type": "text"
      },
      "source": [
        "#7-layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9DyNBhvJygb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "af179195-b8f7-44d8-f0ff-b9ec26c9e3e8"
      },
      "source": [
        "# Credits: https://github.com/keras-team/keras/blob/master/examples/mnist_cnn.py\n",
        "\n",
        "\n",
        "from __future__ import print_function\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten,BatchNormalization\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras import backend as K\n",
        "\n",
        "batch_size = 128\n",
        "num_classes = 10\n",
        "epochs = 12\n",
        "\n",
        "# input image dimensions\n",
        "img_rows, img_cols = 28, 28\n",
        "\n",
        "# the data, split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "print(x_train.shape)\n",
        "\n",
        "if K.image_data_format() == 'channels_first':\n",
        "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
        "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
        "    input_shape = (1, img_rows, img_cols)\n",
        "else:\n",
        "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "    input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(128, kernel_size=(2, 2),padding='same',activation='relu',input_shape=input_shape,kernel_initializer='RandomNormal'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(128, (3, 3),padding='same', activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3),padding='same', activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Conv2D(64, (5, 5),padding='same', activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.5))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(32, (5, 5),padding='same', activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Conv2D(32, (7, 7),padding='same', activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.5))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(16, (7, 7),padding='same', activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(400, activation='relu'))\n",
        "model.add(Dense(200, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss=keras.losses.categorical_crossentropy,optimizer=keras.optimizers.Adam(),metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train,batch_size=batch_size,epochs=epochs,verbose=1,validation_data=(x_test, y_test))\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n",
            "x_train shape: (60000, 28, 28, 1)\n",
            "60000 train samples\n",
            "10000 test samples\n",
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_33 (Conv2D)           (None, 28, 28, 128)       640       \n",
            "_________________________________________________________________\n",
            "batch_normalization_11 (Batc (None, 28, 28, 128)       512       \n",
            "_________________________________________________________________\n",
            "conv2d_34 (Conv2D)           (None, 28, 28, 128)       147584    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_18 (MaxPooling (None, 14, 14, 128)       0         \n",
            "_________________________________________________________________\n",
            "dropout_13 (Dropout)         (None, 14, 14, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_35 (Conv2D)           (None, 14, 14, 64)        73792     \n",
            "_________________________________________________________________\n",
            "batch_normalization_12 (Batc (None, 14, 14, 64)        256       \n",
            "_________________________________________________________________\n",
            "dropout_14 (Dropout)         (None, 14, 14, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_36 (Conv2D)           (None, 14, 14, 64)        102464    \n",
            "_________________________________________________________________\n",
            "batch_normalization_13 (Batc (None, 14, 14, 64)        256       \n",
            "_________________________________________________________________\n",
            "dropout_15 (Dropout)         (None, 14, 14, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_19 (MaxPooling (None, 7, 7, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_37 (Conv2D)           (None, 7, 7, 32)          51232     \n",
            "_________________________________________________________________\n",
            "batch_normalization_14 (Batc (None, 7, 7, 32)          128       \n",
            "_________________________________________________________________\n",
            "dropout_16 (Dropout)         (None, 7, 7, 32)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_38 (Conv2D)           (None, 7, 7, 32)          50208     \n",
            "_________________________________________________________________\n",
            "batch_normalization_15 (Batc (None, 7, 7, 32)          128       \n",
            "_________________________________________________________________\n",
            "dropout_17 (Dropout)         (None, 7, 7, 32)          0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_20 (MaxPooling (None, 3, 3, 32)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_39 (Conv2D)           (None, 3, 3, 16)          25104     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_21 (MaxPooling (None, 1, 1, 16)          0         \n",
            "_________________________________________________________________\n",
            "flatten_9 (Flatten)          (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 400)               6800      \n",
            "_________________________________________________________________\n",
            "dense_18 (Dense)             (None, 200)               80200     \n",
            "_________________________________________________________________\n",
            "dropout_18 (Dropout)         (None, 200)               0         \n",
            "_________________________________________________________________\n",
            "dense_19 (Dense)             (None, 10)                2010      \n",
            "=================================================================\n",
            "Total params: 541,314\n",
            "Trainable params: 540,674\n",
            "Non-trainable params: 640\n",
            "_________________________________________________________________\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/12\n",
            "60000/60000 [==============================] - 44s 732us/step - loss: 0.5045 - acc: 0.8300 - val_loss: 0.0647 - val_acc: 0.9821\n",
            "Epoch 2/12\n",
            "60000/60000 [==============================] - 41s 680us/step - loss: 0.1028 - acc: 0.9711 - val_loss: 0.0629 - val_acc: 0.9803\n",
            "Epoch 3/12\n",
            "60000/60000 [==============================] - 41s 684us/step - loss: 0.0731 - acc: 0.9796 - val_loss: 0.0671 - val_acc: 0.9833\n",
            "Epoch 4/12\n",
            "60000/60000 [==============================] - 41s 683us/step - loss: 0.0622 - acc: 0.9827 - val_loss: 0.0423 - val_acc: 0.9877\n",
            "Epoch 5/12\n",
            "60000/60000 [==============================] - 41s 681us/step - loss: 0.0551 - acc: 0.9846 - val_loss: 0.0375 - val_acc: 0.9889\n",
            "Epoch 6/12\n",
            "60000/60000 [==============================] - 41s 681us/step - loss: 0.0519 - acc: 0.9857 - val_loss: 0.0898 - val_acc: 0.9733\n",
            "Epoch 7/12\n",
            "60000/60000 [==============================] - 41s 680us/step - loss: 0.0466 - acc: 0.9868 - val_loss: 0.0525 - val_acc: 0.9831\n",
            "Epoch 8/12\n",
            "60000/60000 [==============================] - 41s 682us/step - loss: 0.0440 - acc: 0.9882 - val_loss: 0.0276 - val_acc: 0.9923\n",
            "Epoch 9/12\n",
            "60000/60000 [==============================] - 41s 680us/step - loss: 0.0423 - acc: 0.9882 - val_loss: 0.0452 - val_acc: 0.9869\n",
            "Epoch 10/12\n",
            "60000/60000 [==============================] - 41s 681us/step - loss: 0.0387 - acc: 0.9890 - val_loss: 0.0433 - val_acc: 0.9879\n",
            "Epoch 11/12\n",
            "60000/60000 [==============================] - 41s 682us/step - loss: 0.0391 - acc: 0.9894 - val_loss: 0.0321 - val_acc: 0.9911\n",
            "Epoch 12/12\n",
            "60000/60000 [==============================] - 41s 680us/step - loss: 0.0372 - acc: 0.9892 - val_loss: 0.0389 - val_acc: 0.9900\n",
            "Test loss: 0.038856790525408\n",
            "Test accuracy: 0.99\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1mNZNsUvMedo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "93d53219-e6d8-44b4-a536-6cce8781589e"
      },
      "source": [
        "# Credits: https://github.com/keras-team/keras/blob/master/examples/mnist_cnn.py\n",
        "\n",
        "\n",
        "from __future__ import print_function\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten,BatchNormalization\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras import backend as K\n",
        "\n",
        "batch_size = 128\n",
        "num_classes = 10\n",
        "epochs = 12\n",
        "\n",
        "# input image dimensions\n",
        "img_rows, img_cols = 28, 28\n",
        "\n",
        "# the data, split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "print(x_train.shape)\n",
        "\n",
        "if K.image_data_format() == 'channels_first':\n",
        "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
        "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
        "    input_shape = (1, img_rows, img_cols)\n",
        "else:\n",
        "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "    input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(256, kernel_size=(2, 2),padding='same',activation='relu',input_shape=input_shape,kernel_initializer='RandomNormal'))\n",
        "model.add(Conv2D(128, (3, 3),padding='same', activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(128, (3, 3),padding='same', activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3),padding='same', activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(32, (5, 5),padding='same', activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(32, (5, 5),padding='same', activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(26, (7, 7),padding='same', activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(400, activation='relu'))\n",
        "model.add(Dense(200, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss=keras.losses.categorical_crossentropy,optimizer=keras.optimizers.Adam(),metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train,batch_size=batch_size,epochs=epochs,verbose=1,validation_data=(x_test, y_test))\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n",
            "x_train shape: (60000, 28, 28, 1)\n",
            "60000 train samples\n",
            "10000 test samples\n",
            "Model: \"sequential_10\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_40 (Conv2D)           (None, 28, 28, 256)       1280      \n",
            "_________________________________________________________________\n",
            "conv2d_41 (Conv2D)           (None, 28, 28, 128)       295040    \n",
            "_________________________________________________________________\n",
            "batch_normalization_16 (Batc (None, 28, 28, 128)       512       \n",
            "_________________________________________________________________\n",
            "conv2d_42 (Conv2D)           (None, 28, 28, 128)       147584    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_22 (MaxPooling (None, 14, 14, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_43 (Conv2D)           (None, 14, 14, 64)        73792     \n",
            "_________________________________________________________________\n",
            "batch_normalization_17 (Batc (None, 14, 14, 64)        256       \n",
            "_________________________________________________________________\n",
            "conv2d_44 (Conv2D)           (None, 14, 14, 32)        51232     \n",
            "_________________________________________________________________\n",
            "batch_normalization_18 (Batc (None, 14, 14, 32)        128       \n",
            "_________________________________________________________________\n",
            "conv2d_45 (Conv2D)           (None, 14, 14, 32)        25632     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_23 (MaxPooling (None, 7, 7, 32)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_46 (Conv2D)           (None, 7, 7, 26)          40794     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_24 (MaxPooling (None, 3, 3, 26)          0         \n",
            "_________________________________________________________________\n",
            "flatten_10 (Flatten)         (None, 234)               0         \n",
            "_________________________________________________________________\n",
            "dense_20 (Dense)             (None, 400)               94000     \n",
            "_________________________________________________________________\n",
            "dense_21 (Dense)             (None, 200)               80200     \n",
            "_________________________________________________________________\n",
            "dropout_19 (Dropout)         (None, 200)               0         \n",
            "_________________________________________________________________\n",
            "dense_22 (Dense)             (None, 10)                2010      \n",
            "=================================================================\n",
            "Total params: 812,460\n",
            "Trainable params: 812,012\n",
            "Non-trainable params: 448\n",
            "_________________________________________________________________\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/12\n",
            "60000/60000 [==============================] - 63s 1ms/step - loss: 0.1832 - acc: 0.9432 - val_loss: 0.0452 - val_acc: 0.9854\n",
            "Epoch 2/12\n",
            "60000/60000 [==============================] - 60s 997us/step - loss: 0.0513 - acc: 0.9863 - val_loss: 0.2087 - val_acc: 0.9414\n",
            "Epoch 3/12\n",
            "60000/60000 [==============================] - 60s 996us/step - loss: 0.0415 - acc: 0.9886 - val_loss: 0.0277 - val_acc: 0.9911\n",
            "Epoch 4/12\n",
            "60000/60000 [==============================] - 60s 997us/step - loss: 0.0297 - acc: 0.9918 - val_loss: 0.0414 - val_acc: 0.9888\n",
            "Epoch 5/12\n",
            "60000/60000 [==============================] - 60s 998us/step - loss: 0.0284 - acc: 0.9921 - val_loss: 0.0285 - val_acc: 0.9923\n",
            "Epoch 6/12\n",
            "60000/60000 [==============================] - 60s 996us/step - loss: 0.0219 - acc: 0.9938 - val_loss: 0.0422 - val_acc: 0.9893\n",
            "Epoch 7/12\n",
            "60000/60000 [==============================] - 60s 997us/step - loss: 0.0241 - acc: 0.9932 - val_loss: 0.0336 - val_acc: 0.9893\n",
            "Epoch 8/12\n",
            "60000/60000 [==============================] - 60s 995us/step - loss: 0.0188 - acc: 0.9947 - val_loss: 0.0537 - val_acc: 0.9859\n",
            "Epoch 9/12\n",
            "60000/60000 [==============================] - 60s 995us/step - loss: 0.0188 - acc: 0.9952 - val_loss: 0.0286 - val_acc: 0.9920\n",
            "Epoch 10/12\n",
            "60000/60000 [==============================] - 60s 996us/step - loss: 0.0168 - acc: 0.9952 - val_loss: 0.1287 - val_acc: 0.9732\n",
            "Epoch 11/12\n",
            "60000/60000 [==============================] - 60s 997us/step - loss: 0.0152 - acc: 0.9958 - val_loss: 0.0347 - val_acc: 0.9922\n",
            "Epoch 12/12\n",
            "60000/60000 [==============================] - 60s 994us/step - loss: 0.0144 - acc: 0.9962 - val_loss: 0.0326 - val_acc: 0.9923\n",
            "Test loss: 0.03264233518390265\n",
            "Test accuracy: 0.9923\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hp9CHCQ-PHhe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4c2c66c3-24ca-4fa1-aa16-56ade68a231a"
      },
      "source": [
        "# Credits: https://github.com/keras-team/keras/blob/master/examples/mnist_cnn.py\n",
        "\n",
        "\n",
        "from __future__ import print_function\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten,BatchNormalization\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras import backend as K\n",
        "\n",
        "batch_size = 128\n",
        "num_classes = 10\n",
        "epochs = 12\n",
        "\n",
        "# input image dimensions\n",
        "img_rows, img_cols = 28, 28\n",
        "\n",
        "# the data, split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "print(x_train.shape)\n",
        "\n",
        "if K.image_data_format() == 'channels_first':\n",
        "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
        "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
        "    input_shape = (1, img_rows, img_cols)\n",
        "else:\n",
        "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "    input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(256, kernel_size=(2, 2),padding='same',activation='sigmoid',input_shape=input_shape,kernel_initializer='he_normal'))\n",
        "model.add(Conv2D(128, (3, 3),padding='same', activation='sigmoid'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Conv2D(128, (3, 3),padding='same', activation='sigmoid'))\n",
        "model.add(Conv2D(128, (5, 5),padding='same', activation='sigmoid'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(128, (5, 5),padding='same', activation='sigmoid'))\n",
        "model.add(Conv2D(128, (7, 7),padding='same', activation='sigmoid'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Conv2D(128, (7, 7),padding='same', activation='sigmoid'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(400, activation='sigmoid'))\n",
        "model.add(Dense(200, activation='sigmoid'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss=keras.losses.categorical_crossentropy,optimizer=keras.optimizers.Adagrad(),metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train,batch_size=batch_size,epochs=epochs,verbose=1,validation_data=(x_test, y_test))\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n",
            "x_train shape: (60000, 28, 28, 1)\n",
            "60000 train samples\n",
            "10000 test samples\n",
            "Model: \"sequential_11\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_47 (Conv2D)           (None, 28, 28, 256)       1280      \n",
            "_________________________________________________________________\n",
            "conv2d_48 (Conv2D)           (None, 28, 28, 128)       295040    \n",
            "_________________________________________________________________\n",
            "dropout_20 (Dropout)         (None, 28, 28, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_49 (Conv2D)           (None, 28, 28, 128)       147584    \n",
            "_________________________________________________________________\n",
            "conv2d_50 (Conv2D)           (None, 28, 28, 128)       409728    \n",
            "_________________________________________________________________\n",
            "dropout_21 (Dropout)         (None, 28, 28, 128)       0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_25 (MaxPooling (None, 14, 14, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_51 (Conv2D)           (None, 14, 14, 128)       409728    \n",
            "_________________________________________________________________\n",
            "conv2d_52 (Conv2D)           (None, 14, 14, 128)       802944    \n",
            "_________________________________________________________________\n",
            "dropout_22 (Dropout)         (None, 14, 14, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_53 (Conv2D)           (None, 14, 14, 128)       802944    \n",
            "_________________________________________________________________\n",
            "dropout_23 (Dropout)         (None, 14, 14, 128)       0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_26 (MaxPooling (None, 7, 7, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten_11 (Flatten)         (None, 6272)              0         \n",
            "_________________________________________________________________\n",
            "dense_23 (Dense)             (None, 400)               2509200   \n",
            "_________________________________________________________________\n",
            "dense_24 (Dense)             (None, 200)               80200     \n",
            "_________________________________________________________________\n",
            "dropout_24 (Dropout)         (None, 200)               0         \n",
            "_________________________________________________________________\n",
            "dense_25 (Dense)             (None, 10)                2010      \n",
            "=================================================================\n",
            "Total params: 5,460,658\n",
            "Trainable params: 5,460,658\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/12\n",
            "60000/60000 [==============================] - 103s 2ms/step - loss: 2.3165 - acc: 0.1057 - val_loss: 2.3014 - val_acc: 0.1135\n",
            "Epoch 2/12\n",
            "60000/60000 [==============================] - 99s 2ms/step - loss: 2.3020 - acc: 0.1085 - val_loss: 2.3010 - val_acc: 0.1135\n",
            "Epoch 3/12\n",
            "60000/60000 [==============================] - 99s 2ms/step - loss: 2.3017 - acc: 0.1108 - val_loss: 2.3011 - val_acc: 0.1135\n",
            "Epoch 4/12\n",
            "60000/60000 [==============================] - 99s 2ms/step - loss: 2.3015 - acc: 0.1117 - val_loss: 2.3011 - val_acc: 0.1135\n",
            "Epoch 5/12\n",
            "60000/60000 [==============================] - 99s 2ms/step - loss: 2.3017 - acc: 0.1123 - val_loss: 2.3011 - val_acc: 0.1135\n",
            "Epoch 6/12\n",
            "60000/60000 [==============================] - 99s 2ms/step - loss: 2.3014 - acc: 0.1122 - val_loss: 2.3011 - val_acc: 0.1135\n",
            "Epoch 7/12\n",
            "60000/60000 [==============================] - 99s 2ms/step - loss: 2.3015 - acc: 0.1122 - val_loss: 2.3010 - val_acc: 0.1135\n",
            "Epoch 8/12\n",
            "60000/60000 [==============================] - 99s 2ms/step - loss: 2.3015 - acc: 0.1122 - val_loss: 2.3011 - val_acc: 0.1135\n",
            "Epoch 9/12\n",
            "60000/60000 [==============================] - 99s 2ms/step - loss: 2.3013 - acc: 0.1124 - val_loss: 2.3010 - val_acc: 0.1135\n",
            "Epoch 10/12\n",
            "60000/60000 [==============================] - 99s 2ms/step - loss: 2.3014 - acc: 0.1123 - val_loss: 2.3010 - val_acc: 0.1135\n",
            "Epoch 11/12\n",
            "60000/60000 [==============================] - 99s 2ms/step - loss: 2.3014 - acc: 0.1123 - val_loss: 2.3010 - val_acc: 0.1135\n",
            "Epoch 12/12\n",
            "60000/60000 [==============================] - 99s 2ms/step - loss: 2.3014 - acc: 0.1123 - val_loss: 2.3010 - val_acc: 0.1135\n",
            "Test loss: 2.3010334442138674\n",
            "Test accuracy: 0.1135\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eURlsTXDQzDH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "455e3c3d-eaec-4e23-88c8-fb51935338d3"
      },
      "source": [
        "# Credits: https://github.com/keras-team/keras/blob/master/examples/mnist_cnn.py\n",
        "\n",
        "\n",
        "from __future__ import print_function\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten,BatchNormalization\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras import backend as K\n",
        "\n",
        "batch_size = 128\n",
        "num_classes = 10\n",
        "epochs = 12\n",
        "\n",
        "# input image dimensions\n",
        "img_rows, img_cols = 28, 28\n",
        "\n",
        "# the data, split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "print(x_train.shape)\n",
        "\n",
        "if K.image_data_format() == 'channels_first':\n",
        "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
        "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
        "    input_shape = (1, img_rows, img_cols)\n",
        "else:\n",
        "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "    input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(512, kernel_size=(2, 2),padding='same',activation='tanh',input_shape=input_shape,kernel_initializer='glorot_normal'))\n",
        "model.add(Conv2D(256, (3, 3),padding='same', activation='tanh'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(128, (3, 3),padding='same', activation='tanh'))\n",
        "model.add(Conv2D(128, (5, 5),padding='same', activation='tanh'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(128, (5, 5),padding='same', activation='tanh'))\n",
        "model.add(Conv2D(64, (5, 5),padding='same', activation='tanh'))\n",
        "model.add(Conv2D(32, (7, 7),padding='same', activation='tanh'))\n",
        "model.add(Conv2D(16, (7, 7),padding='same', activation='tanh'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(400, activation='tanh'))\n",
        "model.add(Dense(200, activation='tanh'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss=keras.losses.categorical_crossentropy,optimizer=keras.optimizers.Adadelta(),metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train,batch_size=batch_size,epochs=epochs,verbose=1,validation_data=(x_test, y_test))\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n",
            "x_train shape: (60000, 28, 28, 1)\n",
            "60000 train samples\n",
            "10000 test samples\n",
            "Model: \"sequential_12\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_54 (Conv2D)           (None, 28, 28, 512)       2560      \n",
            "_________________________________________________________________\n",
            "conv2d_55 (Conv2D)           (None, 28, 28, 256)       1179904   \n",
            "_________________________________________________________________\n",
            "max_pooling2d_27 (MaxPooling (None, 14, 14, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_56 (Conv2D)           (None, 14, 14, 128)       295040    \n",
            "_________________________________________________________________\n",
            "conv2d_57 (Conv2D)           (None, 14, 14, 128)       409728    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_28 (MaxPooling (None, 7, 7, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_58 (Conv2D)           (None, 7, 7, 128)         409728    \n",
            "_________________________________________________________________\n",
            "conv2d_59 (Conv2D)           (None, 7, 7, 64)          204864    \n",
            "_________________________________________________________________\n",
            "conv2d_60 (Conv2D)           (None, 7, 7, 32)          100384    \n",
            "_________________________________________________________________\n",
            "conv2d_61 (Conv2D)           (None, 7, 7, 16)          25104     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_29 (MaxPooling (None, 3, 3, 16)          0         \n",
            "_________________________________________________________________\n",
            "flatten_12 (Flatten)         (None, 144)               0         \n",
            "_________________________________________________________________\n",
            "dense_26 (Dense)             (None, 400)               58000     \n",
            "_________________________________________________________________\n",
            "dense_27 (Dense)             (None, 200)               80200     \n",
            "_________________________________________________________________\n",
            "dropout_25 (Dropout)         (None, 200)               0         \n",
            "_________________________________________________________________\n",
            "dense_28 (Dense)             (None, 10)                2010      \n",
            "=================================================================\n",
            "Total params: 2,767,522\n",
            "Trainable params: 2,767,522\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/12\n",
            "60000/60000 [==============================] - 105s 2ms/step - loss: 2.4997 - acc: 0.1044 - val_loss: 2.3146 - val_acc: 0.1135\n",
            "Epoch 2/12\n",
            "60000/60000 [==============================] - 99s 2ms/step - loss: 2.3530 - acc: 0.0987 - val_loss: 2.3254 - val_acc: 0.0980\n",
            "Epoch 3/12\n",
            "60000/60000 [==============================] - 99s 2ms/step - loss: 2.3278 - acc: 0.1024 - val_loss: 2.3078 - val_acc: 0.0958\n",
            "Epoch 4/12\n",
            "60000/60000 [==============================] - 99s 2ms/step - loss: 2.3145 - acc: 0.1032 - val_loss: 2.3070 - val_acc: 0.1135\n",
            "Epoch 5/12\n",
            "60000/60000 [==============================] - 99s 2ms/step - loss: 2.3095 - acc: 0.1044 - val_loss: 2.3090 - val_acc: 0.0974\n",
            "Epoch 6/12\n",
            "60000/60000 [==============================] - 99s 2ms/step - loss: 2.3083 - acc: 0.1053 - val_loss: 2.3167 - val_acc: 0.1135\n",
            "Epoch 7/12\n",
            "60000/60000 [==============================] - 99s 2ms/step - loss: 2.3080 - acc: 0.1031 - val_loss: 2.3052 - val_acc: 0.1028\n",
            "Epoch 8/12\n",
            "60000/60000 [==============================] - 99s 2ms/step - loss: 2.3074 - acc: 0.1030 - val_loss: 2.3115 - val_acc: 0.1009\n",
            "Epoch 9/12\n",
            "60000/60000 [==============================] - 99s 2ms/step - loss: 2.3076 - acc: 0.1043 - val_loss: 2.3049 - val_acc: 0.1135\n",
            "Epoch 10/12\n",
            "60000/60000 [==============================] - 99s 2ms/step - loss: 2.3069 - acc: 0.1032 - val_loss: 2.3045 - val_acc: 0.0958\n",
            "Epoch 11/12\n",
            "60000/60000 [==============================] - 99s 2ms/step - loss: 2.3072 - acc: 0.1033 - val_loss: 2.3065 - val_acc: 0.1028\n",
            "Epoch 12/12\n",
            "60000/60000 [==============================] - 99s 2ms/step - loss: 2.3066 - acc: 0.1041 - val_loss: 2.3052 - val_acc: 0.1135\n",
            "Test loss: 2.305227696228027\n",
            "Test accuracy: 0.1135\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6dEztAuRlGT",
        "colab_type": "text"
      },
      "source": [
        "#Summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTYUmoMBb_fy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "e475f210-bd65-4ba4-d3c8-b1cc066c1562"
      },
      "source": [
        "#pretty table\n",
        "#c-Convoluted layer M-Maxpool layer\n",
        "from prettytable import PrettyTable\n",
        "x = PrettyTable()\n",
        "\n",
        "x.field_names = [\"No of concoluted layers\",\"Batch_Normalization\", \"Activation_function\", \"optimizers\", \"weight_initialization\",\"Architecture\",\"Dropout_rate\",\"Accuracy\"]\n",
        "\n",
        "x.add_row([3,\"Y\",\"Relu\",\"Adam\",\"Glorot Normal\",\"CCMCM\",\"Y\",0.9919])\n",
        "x.add_row([3,\"Y\",\"sigmoid\",\"Adagrad\",\"He_Normal\",\"CMCCM\",\"N\",0.9930])\n",
        "x.add_row([3,\"N\",\"tanh\",\"Adadelta\",\"Random Normal\",\"CCCM\",\"Y\",0.9904])\n",
        "x.add_row([3,\"N\",\"Relu\",\"sgd\",\"Glorot Normal\",\"CCC\",\"N\",0.9790])\n",
        "\n",
        "x.add_row([5,\"Y\",\"Relu\",\"Adam\",\"Random Normal\",\"CCMCCMCM\",\"Y\",0.9921])\n",
        "x.add_row([5,\"Y\",\"Relu\",\"Adam\",\"Random Normal\",\"CMCMCMCMC\",\"N\",0.9901])\n",
        "x.add_row([5,\"N\",\"sigmoid\",\"Adagrad\",\"He Normal\",\"CCCMCCM\",\"Y\",0.1135])\n",
        "x.add_row([5,\"N\",\"tanh\",\"adadelta\",\"Glorot Normal\",\"CCCMCMCM\",\"N\",0.9925])\n",
        "\n",
        "\n",
        "\n",
        "x.add_row([7,\"Y\",\"Relu\",\"Adam\",\"Random Normal\",\"CCMCCMCCMCM\",\"Y\",0.9900])\n",
        "x.add_row([7,\"Y\",\"Relu\",\"Adam\",\"Random Normal\",\"CCCMCCCMCM\",\"N\",0.9923])\n",
        "x.add_row([7,\"N\",\"sigmoid\",\"Adagrad\",\"He Normal\",\"CCCCMCCCM\",\"Y\",0.1135])\n",
        "x.add_row([7,\"N\",\"tanh\",\"adadelta\",\"Glorot Normal\",\"CMCCMCCCCM\",\"N\",0.1135])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(x)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------------------------+---------------------+---------------------+------------+-----------------------+--------------+--------------+----------+\n",
            "| No of concoluted layers | Batch_Normalization | Activation_function | optimizers | weight_initialization | Architecture | Dropout_rate | Accuracy |\n",
            "+-------------------------+---------------------+---------------------+------------+-----------------------+--------------+--------------+----------+\n",
            "|            3            |          Y          |         Relu        |    Adam    |     Glorot Normal     |    CCMCM     |      Y       |  0.9919  |\n",
            "|            3            |          Y          |       sigmoid       |  Adagrad   |       He_Normal       |    CMCCM     |      N       |  0.993   |\n",
            "|            3            |          N          |         tanh        |  Adadelta  |     Random Normal     |     CCCM     |      Y       |  0.9904  |\n",
            "|            3            |          N          |         Relu        |    sgd     |     Glorot Normal     |     CCC      |      N       |  0.979   |\n",
            "|            5            |          Y          |         Relu        |    Adam    |     Random Normal     |   CCMCCMCM   |      Y       |  0.9921  |\n",
            "|            5            |          Y          |         Relu        |    Adam    |     Random Normal     |  CMCMCMCMC   |      N       |  0.9901  |\n",
            "|            5            |          N          |       sigmoid       |  Adagrad   |       He Normal       |   CCCMCCM    |      Y       |  0.1135  |\n",
            "|            5            |          N          |         tanh        |  adadelta  |     Glorot Normal     |   CCCMCMCM   |      N       |  0.9925  |\n",
            "|            7            |          Y          |         Relu        |    Adam    |     Random Normal     | CCMCCMCCMCM  |      Y       |   0.99   |\n",
            "|            7            |          Y          |         Relu        |    Adam    |     Random Normal     |  CCCMCCCMCM  |      N       |  0.9923  |\n",
            "|            7            |          N          |       sigmoid       |  Adagrad   |       He Normal       |  CCCCMCCCM   |      Y       |  0.1135  |\n",
            "|            7            |          N          |         tanh        |  adadelta  |     Glorot Normal     |  CMCCMCCCCM  |      N       |  0.1135  |\n",
            "+-------------------------+---------------------+---------------------+------------+-----------------------+--------------+--------------+----------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUjaOFzuthkn",
        "colab_type": "text"
      },
      "source": [
        "1.3 convoluted layers with sigmoid activation unit gives best accuarcy score followed by 7 convoluted layers with relu activation unit.\n",
        "\n",
        "2.5 convoluted layers with sigmoid activation function and adagrad optimizer \n",
        "and 7 conoluted layers with sigmoid,tanh actiavtion units give worst accuracy scores"
      ]
    }
  ]
}